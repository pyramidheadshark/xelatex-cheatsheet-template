## Итоговый Конспект: Логистическая Регрессия (для Собеседования)

*(Используем согласованные обозначения: $m$ - число объектов, $n$ - число признаков, $x^{(i)}$ - вектор признаков $i$-го объекта ($[x_0^{(i)}, x_1^{(i)}, ..., x_n^{(i)}]^T$, где $x_0^{(i)}=1$), $y^{(i)}$ - истинный класс $i$-го объекта (0 или 1), $w$ - вектор весов ($[w_0, w_1, ..., w_n]^T$), $\hat{p}^{(i)}$ - предсказанная вероятность класса 1 для $i$-го объекта, $J(w)$ - функция потерь, $\alpha$ - скорость обучения).*

### 0. Введение и Назначение

* **Определение:** Логистическая регрессия – это **линейный классификатор**, используемый преимущественно для задач **бинарной классификации**. Несмотря на слово "регрессия" в названии, она моделирует **вероятность** принадлежности объекта к определенному классу.
* **Задача:** Предсказать вероятность $P(y=1 | x; w)$ – вероятность того, что объект с признаками $x$ принадлежит классу 1, при заданных параметрах модели $w$.
* **Ключевая идея:** Использовать линейную комбинацию признаков, но "пропустить" результат через специальную функцию (сигмоиду), чтобы получить значение в диапазоне (0, 1), интерпретируемое как вероятность.

### 1. Модель и Предсказание Вероятности

1. **Шаг 1: Линейная Комбинация (Логит)**
    * **Определение:** Вычисляется взвешенная сумма признаков объекта $i$, включая фиктивный признак $x_0^{(i)}=1$ для учета смещения $w_0$. Это значение называется **логит** (logit) и обозначается $z^{(i)}$.
    * **Формула (скалярная):**
        $z^{(i)} = w_0 x_0^{(i)} + w_1 x_1^{(i)} + \dots + w_n x_n^{(i)} = \sum_{j=0}^{n} w_j x_j^{(i)}$
    * **Формула (векторная):**
        $z^{(i)} = w^T x^{(i)}$
    * **Смысл:** Логит – это "сырой" выход линейной части модели. Он может быть любым действительным числом $(-\infty, +\infty)$.

2. **Шаг 2: Сигмоидная Функция (Логистическая Функция)**
    * **Определение:** Нелинейная функция $\sigma(z)$, которая преобразует любое действительное число $z$ (логит) в значение в интервале (0, 1).
    * **Формула:**
        $\sigma(z) = \frac{1}{1 + e^{-z}}$
        *(где $e \approx 2.718$ - основание натурального логарифма)*
    * **Свойства Сигмоиды (Важно для интервью):**
        * **Диапазон:** $0 < \sigma(z) < 1$ для всех $z$. Идеально для вероятностей.
        * **Монотонность:** Строго возрастающая функция. Большему логиту соответствует большая вероятность.
        * **Гладкость:** Бесконечно дифференцируема везде. Это важно для градиентного спуска.
        * **Симметрия:** $\sigma(-z) = 1 - \sigma(z)$. Центрирована вокруг точки (0, 0.5).
        * **Производная:** $\sigma'(z) = \frac{d\sigma}{dz} = \sigma(z) (1 - \sigma(z))$. Простая и удобная форма производной – ключевое преимущество для вывода градиента LogLoss.
        * **График:** S-образная кривая.
    * **Возможный вопрос:** *Можно ли заменить сигмоиду?*
        * **Ответ:** Да, можно использовать другие функции, отображающие $(-\infty, +\infty)$ в (0, 1) и обладающие похожими свойствами (монотонность, гладкость). Пример: **пробит-функция** (интегральная функция стандартного нормального распределения, CDF). Однако сигмоида приводит к более простой форме градиента для LogLoss и имеет вероятностную интерпретацию через ММП. Замена повлияет на функцию потерь и интерпретацию весов.

3. **Шаг 3: Предсказание Вероятности**
    * **Определение:** Предсказанная вероятность $\hat{p}^{(i)}$ класса 1 для объекта $i$ получается применением сигмоиды к логиту $z^{(i)}$.
    * **Формула (скалярная/векторная):**
        $\hat{p}^{(i)} = P(y^{(i)}=1 | x^{(i)}; w) = \sigma(z^{(i)}) = \sigma(w^T x^{(i)}) = \frac{1}{1 + e^{-\sum_{j=0}^{n} w_j x_j^{(i)}}}$

4. **Предсказание Класса (Решающее Правило)**
    * Обычно используется порог 0.5:
        * Если $\hat{p}^{(i)} \ge 0.5 \implies$ Предсказать класс 1.
        * Если $\hat{p}^{(i)} < 0.5 \implies$ Предсказать класс 0.
    * **Замечание:** Порог 0.5 соответствует решению $w^T x^{(i)} \ge 0$. То есть, **решающая граница** логистической регрессии является **линейной** в пространстве признаков (гиперплоскость $w^T x = 0$).

### 2. Функция Потерь (Log Loss / Бинарная Кросс-Энтропия)

1. **Цель:** Нужна функция $J(w)$, которая измеряет несоответствие между предсказанными вероятностями $\hat{p}^{(i)}$ и истинными классами $y^{(i)}$ (0 или 1), и которую мы будем минимизировать для нахождения оптимальных весов $w$.
2. **Почему не MSE? (Ключевой вопрос):**
    * **Главная причина:** Если использовать MSE $J = \frac{1}{m} \sum (\sigma(w^T x^{(i)}) - y^{(i)})^2$, то итоговая функция потерь $J(w)$ оказывается **невыпуклой**. Это значит, что у нее может быть много локальных минимумов, и градиентный спуск не гарантирует сходимость к глобальному минимуму.
    * **Дополнительно:** MSE штрафует ошибки квадратично, что может быть нелогично для вероятностей; LogLoss имеет более естественную вероятностную интерпретацию.
3. **Формула Log Loss (для всей выборки):**
    * **Определение:** Средняя по всем объектам отрицательная логарифмическая функция правдоподобия.
    * **Скалярная форма:**
        $J(w) = -\frac{1}{m} \sum_{i=1}^{m} [ y^{(i)} \log(\hat{p}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{p}^{(i)}) ]$
        *(где $\log$ - натуральный логарифм, $\hat{p}^{(i)} = \sigma(w^T x^{(i)})$)*
    * **Векторная форма (с использованием поэлементных операций):**
        $J(w) = -\frac{1}{m} [\mathbf{y}^T \log(\hat{\mathbf{p}}) + (1 - \mathbf{y})^T \log(1 - \hat{\mathbf{p}})]$
        *(где $\hat{\mathbf{p}} = \sigma(\mathbf{X}w)$)*
4. **Интерпретация Штрафа (Как работает формула):**
    * **Если $y^{(i)}=1$:** Штраф равен $-\log(\hat{p}^{(i)})$. Он стремится к 0 при $\hat{p}^{(i)} \to 1$ (верно и уверенно) и к $+\infty$ при $\hat{p}^{(i)} \to 0$ (неверно и уверенно).
    * **Если $y^{(i)}=0$:** Штраф равен $-\log(1 - \hat{p}^{(i)})$. Он стремится к 0 при $\hat{p}^{(i)} \to 0$ (верно и уверенно) и к $+\infty$ при $\hat{p}^{(i)} \to 1$ (неверно и уверенно).
    * **Вывод:** LogLoss сильно штрафует за уверенные ошибки.
5. **Происхождение Log Loss (Принцип Максимального Правдоподобия - MLE):**
    * **Предположение:** Считаем, что $y^{(i)}$ следует распределению Бернулли с параметром $p = \hat{p}^{(i)} = \sigma(w^T x^{(i)})$. То есть, $P(y^{(i)} | x^{(i)}; w) = (\hat{p}^{(i)})^{y^{(i)}} (1 - \hat{p}^{(i)})^{1 - y^{(i)}}$.
    * **Правдоподобие (Likelihood) выборки:** Так как объекты независимы, правдоподобие всей выборки $L(w) = P(\mathbf{y} | \mathbf{X}; w)$ равно произведению вероятностей для каждого объекта:
        $L(w) = \prod_{i=1}^{m} P(y^{(i)} | x^{(i)}; w) = \prod_{i=1}^{m} (\hat{p}^{(i)})^{y^{(i)}} (1 - \hat{p}^{(i)})^{1 - y^{(i)}}$
    * **Максимизация Правдоподобия:** Мы хотим найти такие $w$, которые максимизируют $L(w)$.
    * **Логарифмирование Правдоподобия (Log-Likelihood):** Максимизировать $L(w)$ эквивалентно максимизации $\log(L(w))$ (так как логарифм - монотонная функция). Это делают, чтобы:
        * Превратить **произведение** в **сумму** (логарифм произведения = сумма логарифмов), что гораздо проще дифференцировать.
        * Избежать **численной неустойчивости** при перемножении многих малых вероятностей (<1).
        $\log L(w) = \sum_{i=1}^{m} [ y^{(i)} \log(\hat{p}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{p}^{(i)}) ]$
    * **Переход к минимизации:** Максимизация $\log L(w)$ эквивалентна **минимизации** $-\log L(w)$. Если усреднить по выборке (разделить на $m$), получим:
        $\min_w J(w) = \min_w \left( -\frac{1}{m} \log L(w) \right)$
        Это и есть формула Log Loss!
6. **Свойства Log Loss (Важно):**
    * **Выпуклость:** Log Loss является **выпуклой функцией** относительно параметров $w$. Это гарантирует, что градиентный спуск найдет **единственный глобальный минимум**.

### 3. Обучение Модели: Градиентный Спуск

1. **Цель:** Найти вектор весов $w$, минимизирующий функцию потерь $J(w)$, итеративно обновляя веса в направлении анти-градиента.
2. **Аналитическое решение:** В отличие от линейной регрессии с MSE, для логистической регрессии **нет простого аналитического решения**. Используются итеративные методы оптимизации.
3. **Градиент Функции Потерь Log Loss:**
    * **Определение:** Вектор $\nabla J(w)$, состоящий из частных производных $\frac{\partial J}{\partial w_k}$ по каждому весу $w_k$ (включая $w_0$).
    * **Формула частной производной (скалярная):** (Результат после применения chain rule и использования $\sigma'(z) = \sigma(z)(1-\sigma(z))$)
        $\frac{\partial J}{\partial w_k} = \frac{1}{m} \sum_{i=1}^{m} (\hat{p}^{(i)} - y^{(i)}) x_k^{(i)}$
        *(Примечание: Формула идентична градиенту MSE, но $\hat{p}^{(i)}$ вычисляется через сигмоиду!)*
    * **Формула градиента (векторная):**
        $\nabla J(w) = \frac{1}{m} \mathbf{X}^T (\sigma(\mathbf{X}w) - \mathbf{y})$
4. **Алгоритм Градиентного Спуска (Mini-Batch GD):**
    * **Инициализация:** $w$ (нули или малые случайные), $\alpha$, $B$, критерий остановки.
    * **Цикл по Эпохам:**
        * Перемешать данные.
        * **Цикл по Мини-Батчам:**
            * **Forward Pass:** Посчитать логиты $z^{(i)} = w^T x^{(i)}$ и вероятности $\hat{p}^{(i)} = \sigma(z^{(i)})$ для объектов $i$ батча.
            * **Gradient Calculation (Backward Pass):** Посчитать градиент $g_k = \frac{1}{B} \sum_{i \in \text{Batch}} (\hat{p}^{(i)} - y^{(i)}) x_k^{(i)}$ для каждого $k=0..n$.
            * **Weight Update:** Обновить все веса: $w_k := w_k - \alpha \cdot g_k$.
5. **Варианты GD:** Batch (весь датасет, медленно, точно), Stochastic (1 объект, быстро, шумно), Mini-batch (компромисс, стандарт де-факто).
6. **Оптимизаторы:** Вместо простого GD часто используют более продвинутые:
    * **Momentum:** Добавляет "инерцию" к обновлению весов, помогая быстрее проходить плато и сглаживать колебания.
    * **RMSProp:** Адаптирует скорость обучения для каждого параметра на основе скользящего среднего квадратов градиентов.
    * **Adam:** Сочетает идеи Momentum и RMSProp. Часто является хорошим выбором по умолчанию.
    * **AdamW:** Adam с корректным применением L2-регуляризации (weight decay).

### 4. Практические Аспекты и "Фишки" для Интервью

1. **Масштабирование Признаков (Feature Scaling):**
    * **Необходимо ли?** Да, **настоятельно рекомендуется**.
    * **Почему?**
        * **Ускорение сходимости GD:** Признаки разного масштаба приводят к "вытянутой" форме функции потерь, замедляя GD. Масштабирование делает ее более "сферической".
        * **Корректная работа регуляризации:** L1/L2 штрафуют абсолютные значения весов. Без масштабирования штраф будет непропорционален для признаков разного масштаба.
    * **Методы:** StandardScaler (к N(0,1)), MinMaxScaler (к).
2. **Инициализация Весов:**
    * **Влияет ли?** Так как LogLoss выпуклая, **не влияет на конечный результат** (глобальный минимум един), но **влияет на скорость сходимости**. Инициализация нулями обычно работает нормально для LogReg (в отличие от глубоких нейросетей).
3. **Регуляризация (L1/L2):**
    * Может и должна применяться к логистической регрессии для борьбы с переобучением (особенно при большом числе признаков).
    * Добавляется к функции потерь LogLoss (штрафуются веса $w_1, ..., w_n$, но не $w_0$).
    * Механизмы и эффекты L1 (разреженность, отбор признаков) и L2 (сжатие весов) аналогичны линейной регрессии.
4. **Интерпретируемость:**
    * Веса $w_j$ в логистической регрессии интерпретируются сложнее, чем в линейной. Они показывают, как изменяется **логит** (логарифм отношения шансов $log(\frac{p}{1-p})$) при изменении $x_j$ на единицу. Часто смотрят на $exp(w_j)$ - во сколько раз изменятся **шансы** ($p/(1-p)$).
5. **Сравнение с Другими Моделями:**
    * **Простота и Эффективность:** LogReg – простая, быстрая, часто дает хороший baseline.
    * **Линейная Граница:** Основное ограничение – может разделять классы только линейно. Если граница сложная, нужны другие модели (деревья, SVM с ядром, нейросети) или полиномиальные признаки.
6. **Мультиклассовая Логистическая Регрессия:**
    * Использует **Softmax** функцию на выходе вместо сигмоиды.
    * Функция потерь – **Cross-Entropy Loss**.
    * Может рассматриваться как однослойная нейронная сеть с Softmax активацией.

### 5. Краткое Резюме (Ключевые Точки для Запоминания)

* **Задача:** Бинарная классификация (предсказание вероятности $P(y=1|x)$).
* **Модель:** $\hat{p} = \sigma(w^T x)$, где $\sigma(z) = 1 / (1 + e^{-z})$ (Сигмоида).
* **Loss:** Log Loss (Бинарная Кросс-Энтропия), выпуклая, выводится из ММП.
* **Обучение:** Градиентный спуск (или его варианты), нет аналитического решения.
* **Градиент:** $\frac{\partial J}{\partial w_k} = \frac{1}{m} \sum (\hat{p}^{(i)} - y^{(i)}) x_k^{(i)}$.
* **Практика:** Нуждается в масштабировании признаков, можно применять L1/L2 регуляризацию.
* **Ограничение:** Линейная решающая граница.
