# Детальный Вывод Формул: Нейронные Сети

*(Используем обозначения: $m$ - число объектов, $n^{[l]}$ - число нейронов в слое $l$ (не считая bias-нейрона), $L$ - число слоев в сети, $x = a^{[0]}$ - входной вектор ($n^{[0]} \times 1$), $W^{[l]}$ - матрица весов слоя $l$ (размер $n^{[l]} \times n^{[l-1]}$), $b^{[l]}$ - вектор смещений слоя $l$ ($n^{[l]} \times 1$), $z^{[l]}$ - вектор преактиваций (логитов) слоя $l$ ($n^{[l]} \times 1$), $a^{[l]}$ - вектор активаций слоя $l$ ($n^{[l]} \times 1$), $a^{[l]} = f^{[l]}(z^{[l]})$ где $f^{[l]}$ - функция активации слоя $l$, $y$ - вектор истинных значений, $\hat{y} = a^{[L]}$ - вектор предсказанных значений, $J$ - функция потерь. Индекс $j$ нумерует нейроны в текущем слое $l$, индекс $k$ - в предыдущем слое $l-1$.)*

*(Примечание: Мы не будем явно включать bias $b$ в матрицу весов $W$ для большей ясности при выводе градиентов по $W$ и $b$ отдельно).*

## 1. Прямой Проход (Forward Pass)

Вычисление предсказания сети $\hat{y} = a^{[L]}$ для одного входного примера $x = a^{[0]}$.

**Для одного слоя $l$ (от $l=1$ до $L$):**

**Шаг 1: Вычисление Преактивации (Логита) $z^{[l]}$**
Вектор $z^{[l]}$ получается путем применения линейного преобразования к активациям предыдущего слоя $a^{[l-1]}$ с использованием весов $W^{[l]}$ и смещений $b^{[l]}$.

* **Векторная форма:**
    $$
    z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}
    $$
* **Скалярная форма (для $j$-го нейрона слоя $l$):**
    $$
    z_j^{[l]} = \sum_{k=1}^{n^{[l-1]}} W_{jk}^{[l]} a_k^{[l-1]} + b_j^{[l]}
    $$
  * **Объяснение:** Логит $j$-го нейрона слоя $l$ равен взвешенной сумме активаций $a_k^{[l-1]}$ всех нейронов $k$ предыдущего слоя $l-1$ (веса $W_{jk}^{[l]}$) плюс смещение $b_j^{[l]}$ этого нейрона.

**Шаг 2: Вычисление Активации $a^{[l]}$**
Вектор $a^{[l]}$ получается применением функции активации $f^{[l]}$ поэлементно к вектору преактиваций $z^{[l]}$.

* **Векторная форма:**
    $$
    a^{[l]} = f^{[l]}(z^{[l]})
    $$
* **Скалярная форма (для $j$-го нейрона слоя $l$):**
    $$
    a_j^{[l]} = f^{[l]}(z_j^{[l]})
    $$
  * **Объяснение:** Активация $j$-го нейрона слоя $l$ получается пропусканием его логита $z_j^{[l]}$ через функцию активации $f^{[l]}$ этого слоя (например, ReLU, Sigmoid).

Эти два шага повторяются для всех слоев от $l=1$ до $l=L$. Результат $a^{[L]}$ является выходом сети $\hat{y}$.

## 2. Производные Функций Активации

Производные функций активации $\frac{da_j^{[l]}}{dz_j^{[l]}} = f^{[l]'}(z_j^{[l]})$ необходимы для обратного распространения ошибки.

**2.1 Производная Сигмоиды (как в LogReg):**
$$
\sigma'(z) = \sigma(z) (1 - \sigma(z))
$$
Или в терминах активации $a = \sigma(z)$:
$$
\frac{d\sigma}{dz} = a (1 - a)
$$

**2.2 Производная Tanh:**
Используя $\tanh(z) = 2\sigma(2z) - 1$ и цепное правило:
$$
\tanh'(z) = \frac{d}{dz}(2\sigma(2z) - 1) = 2 \cdot \sigma'(2z) \cdot 2 = 4 \sigma(2z)(1 - \sigma(2z))
$$
Можно показать, что это равно:
$$
\boxed{\tanh'(z) = 1 - \tanh^2(z)}
$$
Или в терминах активации $a = \tanh(z)$:
$$
\frac{d\tanh}{dz} = 1 - a^2
$$

**2.3 Производная ReLU:**
$$
ReLU(z) = \max(0, z)
$$
Производная определяется кусочно:
$$
\boxed{
ReLU'(z) = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z < 0 \\ \text{не определена} & \text{if } z = 0 \end{cases}
}
$$

* **Объяснение:** На практике, для $z=0$ производную обычно полагают равной 0 или 1. Это не создает проблем, так как вероятность попасть ровно в 0 мала. Важно, что при $z<0$ производная равна 0 (причина "умирающего ReLU"), а при $z>0$ она равна 1 (градиент проходит без изменений).

**2.4 Производная Leaky ReLU:**
$$
LeakyReLU(z) = \max(\alpha z, z) = \begin{cases} z & \text{if } z \ge 0 \\ \alpha z & \text{if } z < 0 \end{cases}
$$
$$
\boxed{
LeakyReLU'(z) = \begin{cases} 1 & \text{if } z > 0 \\ \alpha & \text{if } z < 0 \\ \text{не определена} & \text{if } z = 0 \end{cases}
}
$$

* **Объяснение:** При $z<0$ градиент не нулевой, а равен малому коэффициенту $\alpha$.

## 3. Обратное Распространение Ошибки (Backpropagation)

Цель: Вычислить градиенты функции потерь $J$ по всем параметрам сети ($W^{[l]}$ и $b^{[l]}$ для всех слоев $l$). Мы используем цепное правило, двигаясь от последнего слоя к первому.

Определим **"ошибку" нейрона $j$ в слое $l$** как частную производную итоговой функции потерь $J$ по преактивации $z_j^{[l]}$ этого нейрона:
$$
\delta_j^{[l]} = \frac{\partial J}{\partial z_j^{[l]}}
$$
Вектор ошибок для слоя $l$: $\delta^{[l]} = \frac{\partial J}{\partial z^{[l]}}$.

**Шаг 1: Ошибка Выходного Слоя $\delta^{[L]}$**
Ошибка $\delta^{[L]}$ зависит от функции потерь $J$ и функции активации последнего слоя $f^{[L]}$. По цепному правилу:
$$
\delta_j^{[L]} = \frac{\partial J}{\partial z_j^{[L]}} = \sum_{k=1}^{n^{[L]}} \frac{\partial J}{\partial a_k^{[L]}} \cdot \frac{\partial a_k^{[L]}}{\partial z_j^{[L]}}
$$
Поскольку $a_k^{[L]} = f^{[L]}(z_k^{[L]})$, то $\frac{\partial a_k^{[L]}}{\partial z_j^{[L]}} = 0$ при $k \ne j$, и равно $f^{[L]'}(z_j^{[L]})$ при $k=j$. Поэтому:
$$
\delta_j^{[L]} = \frac{\partial J}{\partial a_j^{[L]}} \cdot f^{[L]'}(z_j^{[L]})
$$

* **Векторная форма:**
    $$
    \boxed{\delta^{[L]} = \nabla_{a^{[L]}} J \odot f^{[L]'}(z^{[L]})}
    $$
  * $\nabla_{a^{[L]}} J$: Градиент функции потерь по активациям выходного слоя.
  * $\odot$: Поэлементное умножение (Адамара).
  * $f^{[L]'}(z^{[L]})$: Вектор производных функции активации, вычисленных в точках $z^{[L]}$.

* **Конкретные случаи для $\delta^{[L]}$:**
  * **MSE Loss и Линейная активация ($f^{[L]}(z)=z$):**
        $J = \frac{1}{m} \sum_{j=1}^{n^{[L]}} \frac{1}{2} (a_j^{[L]} - y_j)^2$.
        $\frac{\partial J}{\partial a_j^{[L]}} = \frac{1}{m} (a_j^{[L]} - y_j)$.
        $f^{[L]'}(z_j^{[L]}) = 1$.
        $\delta_j^{[L]} = \frac{1}{m} (a_j^{[L]} - y_j)$.
        **Векторно:** $\delta^{[L]} = \frac{1}{m} (a^{[L]} - y)$.
  * **Cross-Entropy Loss и Softmax активация:** (Вывод сложнее, но результат простой)
        $\delta_j^{[L]} = \frac{1}{m} (a_j^{[L]} - y_j)$ (где $y_j$ - компонента one-hot вектора $y$).
        **Векторно:** $\delta^{[L]} = \frac{1}{m} (a^{[L]} - y)$.
  * **Binary Cross-Entropy Loss и Sigmoid активация ($f^{[L]}=\sigma$):**
        $J = -\frac{1}{m} [ y \log(a^{[L]}) + (1-y) \log(1-a^{[L]}) ]$.
        $\frac{\partial J}{\partial a^{[L]}} = -\frac{1}{m} (\frac{y}{a^{[L]}} - \frac{1-y}{1-a^{[L]}}) = -\frac{1}{m} \frac{y(1-a^{[L]}) - (1-y)a^{[L]}}{a^{[L]}(1-a^{[L]})} = -\frac{1}{m} \frac{y-a^{[L]}}{a^{[L]}(1-a^{[L]})}$.
        $f^{[L]'}(z^{[L]}) = \sigma'(z^{[L]}) = a^{[L]}(1-a^{[L]})$.
        $\delta^{[L]} = \left( -\frac{1}{m} \frac{y-a^{[L]}}{a^{[L]}(1-a^{[L]})} \right) \odot (a^{[L]}(1-a^{[L]})) = \frac{1}{m} (a^{[L]} - y)$.
        **Векторно:** $\delta^{[L]} = \frac{1}{m} (a^{[L]} - y)$.

* **Замечание:** Во многих стандартных комбинациях (MSE+Linear, CE+Softmax, BCE+Sigmoid) формула для $\delta^{[L]}$ упрощается до $\frac{1}{m} (\text{Предсказание} - \text{Истина})$.

**Шаг 2: Ошибка для Скрытых Слоев $\delta^{[l]}$ (для $l = L-1, ..., 1$)**
Чтобы найти ошибку $\delta^{[l]}$ слоя $l$, нужно знать, как изменение преактивации $z^{[l]}$ этого слоя влияет на итоговую ошибку $J$. Это влияние происходит через *все* нейроны следующего слоя $l+1$. Используем цепное правило:
$$
\delta_j^{[l]} = \frac{\partial J}{\partial z_j^{[l]}} = \sum_{k=1}^{n^{[l+1]}} \frac{\partial J}{\partial z_k^{[l+1]}} \cdot \frac{\partial z_k^{[l+1]}}{\partial a_j^{[l]}} \cdot \frac{\partial a_j^{[l]}}{\partial z_j^{[l]}}
$$
Разберем компоненты:

* $\frac{\partial J}{\partial z_k^{[l+1]}} = \delta_k^{[l+1]}$ (ошибка $k$-го нейрона следующего слоя, которую мы уже знаем или посчитаем рекурсивно).
* $z_k^{[l+1]} = \sum_{p=1}^{n^{[l]}} W_{kp}^{[l+1]} a_p^{[l]} + b_k^{[l+1]}$. Поэтому $\frac{\partial z_k^{[l+1]}}{\partial a_j^{[l]}} = W_{kj}^{[l+1]}$ (вес, соединяющий нейрон $j$ слоя $l$ с нейроном $k$ слоя $l+1$).
* $\frac{\partial a_j^{[l]}}{\partial z_j^{[l]}} = f^{[l]'}(z_j^{[l]})$ (производная активации $j$-го нейрона слоя $l$).

Подставляем:
$$
\delta_j^{[l]} = \left( \sum_{k=1}^{n^{[l+1]}} \delta_k^{[l+1]} W_{kj}^{[l+1]} \right) \cdot f^{[l]'}(z_j^{[l]})
$$

* **Векторная форма:**
    Первая часть $\sum_{k=1}^{n^{[l+1]}} \delta_k^{[l+1]} W_{kj}^{[l+1]}$ соответствует $j$-й компоненте вектора $(W^{[l+1]})^T \delta^{[l+1]}$. Поэтому:
    $$
    \boxed{\delta^{[l]} = ((W^{[l+1]})^T \delta^{[l+1]}) \odot f^{[l]'}(z^{[l]})}
    $$
  * **Объяснение:** Ошибка слоя $l$ вычисляется как ошибка следующего слоя $\delta^{[l+1]}$, "распространенная назад" через транспонированные веса $(W^{[l+1]})^T$, и затем умноженная поэлементно на производную функции активации слоя $l$. Это ключевой шаг обратного распространения.

**Шаг 3: Градиенты по Параметрам ($W^{[l]}, b^{[l]}$)**
Зная ошибку $\delta^{[l]}$, мы можем найти градиенты по параметрам слоя $l$.

* **Градиент по Весам $W^{[l]}$:**
    Нам нужна $\frac{\partial J}{\partial W_{jk}^{[l]}}$. Снова цепное правило:
    $$
    \frac{\partial J}{\partial W_{jk}^{[l]}} = \frac{\partial J}{\partial z_j^{[l]}} \cdot \frac{\partial z_j^{[l]}}{\partial W_{jk}^{[l]}}
    $$
    Первый множитель - это $\delta_j^{[l]}$. Второй множитель:
    $$
    \frac{\partial z_j^{[l]}}{\partial W_{jk}^{[l]}} = \frac{\partial}{\partial W_{jk}^{[l]}} \left( \sum_{p=1}^{n^{[l-1]}} W_{jp}^{[l]} a_p^{[l-1]} + b_j^{[l]} \right) = a_k^{[l-1]}
    $$
    (Производная суммы по $W_{jk}^{[l]}$ равна коэффициенту при нем, то есть $a_k^{[l-1]}$).
    Получаем:
    $$
    \frac{\partial J}{\partial W_{jk}^{[l]}} = \delta_j^{[l]} a_k^{[l-1]}
    $$
  * **Матричная форма:** Собирая все производные $\frac{\partial J}{\partial W_{jk}^{[l]}}$ в матрицу $\frac{\partial J}{\partial W^{[l]}}$ того же размера, что и $W^{[l]}$, получаем:
        $$
        \boxed{\frac{\partial J}{\partial W^{[l]}} = \delta^{[l]} (a^{[l-1]})^T}
        $$
    * **Объяснение:** Градиент по матрице весов слоя $l$ равен внешнему произведению вектора ошибки этого слоя $\delta^{[l]}$ (размер $n^{[l]} \times 1$) на транспонированный вектор активаций предыдущего слоя $(a^{[l-1]})^T$ (размер $1 \times n^{[l-1]}$). Результат имеет размер $n^{[l]} \times n^{[l-1]}$, как и $W^{[l]}$. (Примечание: При усреднении по батчу размером $B$ здесь будет $\frac{1}{B} \sum_{i \in Batch} \delta^{[l](i)} (a^{[l-1](i)})^T$).

* **Градиент по Смещениям $b^{[l]}$:**
    $$
    \frac{\partial J}{\partial b_j^{[l]}} = \frac{\partial J}{\partial z_j^{[l]}} \cdot \frac{\partial z_j^{[l]}}{\partial b_j^{[l]}}
    $$
    Первый множитель - $\delta_j^{[l]}$. Второй множитель:
    $$
    \frac{\partial z_j^{[l]}}{\partial b_j^{[l]}} = \frac{\partial}{\partial b_j^{[l]}} \left( \sum_{p=1}^{n^{[l-1]}} W_{jp}^{[l]} a_p^{[l-1]} + b_j^{[l]} \right) = 1
    $$
    Получаем:
    $$
    \frac{\partial J}{\partial b_j^{[l]}} = \delta_j^{[l]}
    $$
  * **Векторная форма:**
        $$
        \boxed{\frac{\partial J}{\partial b^{[l]}} = \delta^{[l]}}
        $$
    * **Объяснение:** Градиент по вектору смещений слоя $l$ просто равен вектору ошибки этого слоя $\delta^{[l]}$. (Примечание: При усреднении по батчу будет $\frac{1}{B} \sum_{i \in Batch} \delta^{[l](i)}$).

**Итог Backpropagation:**
Мы получили формулы для вычисления ошибки $\delta^{[l]}$ для каждого слоя (двигаясь назад от $L$ до 1) и, используя эти ошибки, вычислили градиенты по параметрам $\frac{\partial J}{\partial W^{[l]}}$ и $\frac{\partial J}{\partial b^{[l]}}$ для каждого слоя.

## 4. Обновление Весов (Градиентный Спуск)

Используя вычисленные градиенты, обновляем параметры сети:

**Скалярная форма (для каждого веса и смещения):**
$$
\boxed{
\begin{aligned}
W_{jk}^{[l]} &:= W_{jk}^{[l]} - \alpha \frac{\partial J}{\partial W_{jk}^{[l]}} \\
b_j^{[l]} &:= b_j^{[l]} - \alpha \frac{\partial J}{\partial b_j^{[l]}}
\end{aligned}
}
$$

**Векторная/Матричная форма (для каждого слоя $l$):**
$$
\boxed{
\begin{aligned}
W^{[l]} &:= W^{[l]} - \alpha \frac{\partial J}{\partial W^{[l]}} \\
b^{[l]} &:= b^{[l]} - \alpha \frac{\partial J}{\partial b^{[l]}}
\end{aligned}
}
$$

* **Объяснение:** Параметры обновляются шагом в направлении анти-градиента. Вместо простого GD могут использоваться более сложные оптимизаторы (Adam и т.д.), которые модифицируют это правило обновления.

## 5. Batch Normalization (Формулы Прямого Прохода)

*(Для одного признака (канала) $k$ на входе слоя $z_k$, по мини-батчу $B$)*

**Шаг 1: Среднее по батчу $\mu_{B,k}$**
$$
\mu_{B,k} = \frac{1}{B} \sum_{i=1}^{B} z_k^{(i)}
$$

**Шаг 2: Дисперсия по батчу $\sigma^2_{B,k}$**
$$
\sigma^2_{B,k} = \frac{1}{B} \sum_{i=1}^{B} (z_k^{(i)} - \mu_{B,k})^2
$$

**Шаг 3: Нормализация $\hat{z}_k^{(i)}$**
$$
\hat{z}_k^{(i)} = \frac{z_k^{(i)} - \mu_{B,k}}{\sqrt{\sigma^2_{B,k} + \epsilon}}
$$

* $\epsilon$: Малая константа для численной стабильности (например, $10^{-8}$).

**Шаг 4: Масштабирование и Сдвиг $BN(z_k^{(i)})$**
$$
\boxed{
BN(z_k^{(i)}) = \gamma_k \hat{z}_k^{(i)} + \beta_k
}
$$

* $\gamma_k, \beta_k$: **Обучаемые** параметры слоя Batch Normalization для признака $k$.

*(Вывод градиентов для Batch Normalization сложнее и выходит за рамки базового обзора, но важно знать, что $\gamma_k$ и $\beta_k$ обновляются через backpropagation, как и другие параметры сети).*
