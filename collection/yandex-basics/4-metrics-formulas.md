# Детальный Вывод Формул: Метрики Качества в Машинном Обучении

Этот документ содержит подробные определения и формулы для основных метрик качества, используемых в задачах регрессии и классификации. Мы будем придерживаться скалярной нотации, где это возможно, и использовать унифицированные обозначения.

## 0. Основные Обозначения

* $m$ - количество примеров (объектов) в выборке.
* $n$ - количество признаков (не считая фиктивного $x_0$).
* $y^{(i)}$ - истинное значение целевой переменной для $i$-го примера.
* $\hat{y}^{(i)}$ - предсказанное моделью значение для $i$-го примера (для регрессии).
* $\hat{p}^{(i)}$ - предсказанная моделью вероятность принадлежности к классу 1 для $i$-го примера (для бинарной классификации).
* $\bar{y} = \frac{1}{m} \sum_{i=1}^{m} y^{(i)}$ - среднее истинных значений (для регрессии).
* **Для классификации (из Матрицы Ошибок):**
  * `TP` (True Positive): Истинно Положительные (класс 1 предсказан как 1).
  * `TN` (True Negative): Истинно Отрицательные (класс 0 предсказан как 0).
  * `FP` (False Positive): Ложно Положительные (класс 0 предсказан как 1) - Ошибка I рода.
  * `FN` (False Negative): Ложно Отрицательные (класс 1 предсказан как 0) - Ошибка II рода.

---

## 1. Метрики Регрессии

Оценивают близость предсказанных непрерывных значений $\hat{y}^{(i)}$ к истинным $y^{(i)}$.

### 1.1. MSE (Mean Squared Error / Среднеквадратичная Ошибка)

* **Определение:** Средний квадрат разности между истинным и предсказанным значениями.
* **Формула:**
    $$
    \text{MSE} = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2
    $$
* **Объяснение:** Возведение в квадрат делает ошибку всегда положительной и сильно штрафует за большие отклонения. Часто используется как функция потерь из-за хороших математических свойств (выпуклость, дифференцируемость). Измеряется в квадрате единиц целевой переменной. Чувствительна к выбросам.

### 1.2. RMSE (Root Mean Squared Error / Корень из Среднеквадратичной Ошибки)

* **Определение:** Квадратный корень из MSE.
* **Формула:**
    $$
    \text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2}
    $$
* **Объяснение:** Возвращает ошибку к исходной размерности целевой переменной, что делает ее более интерпретируемой, чем MSE. Сохраняет чувствительность к большим ошибкам и выбросам.

### 1.3. MAE (Mean Absolute Error / Средняя Абсолютная Ошибка)

* **Определение:** Средний модуль разности между истинным и предсказанным значениями.
* **Формула:**
    $$
    \text{MAE} = \frac{1}{m} \sum_{i=1}^{m} |y^{(i)} - \hat{y}^{(i)}|
    $$
* **Объяснение:** Измеряет среднюю величину ошибки без учета знака. Менее чувствительна к выбросам по сравнению с MSE/RMSE, так как не использует квадрат. Имеет ту же размерность, что и целевая переменная.

### 1.4. R² (Coefficient of Determination / Коэффициент Детерминации)

* **Определение:** Доля общей дисперсии истинных значений $y^{(i)}$, которая объясняется моделью. Сравнивает модель с простой базовой моделью, предсказывающей среднее $\bar{y}$.
* **Вспомогательные Величины:**
  * **RSS (Residual Sum of Squares):** Сумма квадратов остатков (ошибок) модели.
        $$
        RSS = \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2
        $$
  * **TSS (Total Sum of Squares):** Общая сумма квадратов отклонений истинных значений от их среднего. Пропорциональна дисперсии $y$.
        $$
        TSS = \sum_{i=1}^{m} (y^{(i)} - \bar{y})^2
        $$
* **Формула R²:**
    $$
    R^2 = 1 - \frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2}{\sum_{i=1}^{m} (y^{(i)} - \bar{y})^2}
    $$
* **Объяснение:** Показывает, насколько лучше модель предсказывает данные по сравнению с константной моделью (предсказание среднего). $R^2=1$ - идеальное объяснение дисперсии. $R^2=0$ - модель не лучше среднего. $R^2<0$ - модель хуже среднего. **Важно:** $R^2$ не уменьшается при добавлении новых признаков, даже бесполезных.

### 1.5. Adjusted R² (Скорректированный R²)

* **Определение:** Модификация R², которая вводит штраф за количество признаков в модели.
* **Формула:**
    $$
    R^2_{adj} = 1 - (1 - R^2) \frac{m-1}{m-n-1}
    $$
  * $m$ - количество примеров.
  * $n$ - количество признаков (не считая $w_0$).
* **Объяснение:** Позволяет сравнивать модели с разным количеством признаков более честно. $R^2_{adj}$ увеличивается только если добавленный признак вносит значимый вклад в объяснение дисперсии. Всегда $R^2_{adj} \le R^2$.

### 1.6. MAPE (Mean Absolute Percentage Error / Средняя Абсолютная Процентная Ошибка)

* **Определение:** Средняя абсолютная ошибка, выраженная в процентах от истинного значения.
* **Формула:**
    $$
    \text{MAPE} = \frac{100\%}{m} \sum_{i=1}^{m} \left| \frac{y^{(i)} - \hat{y}^{(i)}}{y^{(i)}} \right|
    $$
* **Объяснение:** Показывает относительную ошибку. Интуитивно понятна ("ошибка X%"). **Ограничения:** Не определена при $y^{(i)}=0$. Нестабильна при $y^{(i)}$, близких к нулю. Несимметрична (одинаковая абсолютная ошибка при завышении и занижении дает разный вклад в MAPE).

### 1.7. Huber Loss (Функция Потерь Хьюбера)

* **Определение:** Робастная функция потерь, которая ведет себя как MSE для маленьких ошибок и как MAE для больших. Используется чаще как функция потерь при обучении, но может служить и метрикой.
* **Формула:**
    $$
    L_\delta(a) = \begin{cases} \frac{1}{2}a^2 & \text{при } |a| \le \delta \\ \delta(|a| - \frac{1}{2}\delta) & \text{при } |a| > \delta \end{cases}
    $$
  * $a = y^{(i)} - \hat{y}^{(i)}$ (ошибка для одного примера).
  * $\delta > 0$ - гиперпараметр (порог), определяющий границу между квадратичным и линейным поведением.
* **Средний Huber Loss по выборке:**
    $$
    \text{Huber Loss} = \frac{1}{m} \sum_{i=1}^{m} L_\delta(y^{(i)} - \hat{y}^{(i)})
    $$
* **Объяснение:** Сочетает преимущества MSE (гладкость около нуля) и MAE (устойчивость к выбросам). При $\delta \to \infty$ стремится к MSE, при $\delta \to 0$ стремится к MAE.

---

## 2. Метрики Бинарной Классификации (Пороговые)

Зависят от выбора порога для перевода вероятностей $\hat{p}^{(i)}$ в классы. Используют значения `TP`, `TN`, `FP`, `FN` из матрицы ошибок.

### 2.1. Accuracy (Доля Правильных Ответов)

* **Определение:** Доля правильно классифицированных объектов.
* **Формула:**
    $$
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
    $$
* **Объяснение:** Простая и интуитивная метрика. **Сильно искажается при дисбалансе классов.** Не использовать как единственную метрику при дисбалансе.

### 2.2. Precision (Точность)

* **Определение:** Доля объектов, действительно принадлежащих классу 1, среди всех объектов, которые модель отнесла к классу 1.
* **Формула:**
    $$
    \text{Precision} = \frac{TP}{TP + FP}
    $$
* **Объяснение:** Отвечает на вопрос "Насколько можно доверять предсказаниям класса 1?". Высокая точность означает мало ложных срабатываний (FP).

### 2.3. Recall (Полнота / Sensitivity / TPR)

* **Определение:** Доля объектов класса 1, которые модель правильно обнаружила среди всех реально существующих объектов класса 1.
* **Формула:**
    $$
    \text{Recall} = \text{TPR} = \frac{TP}{TP + FN}
    $$
* **Объяснение:** Отвечает на вопрос "Насколько хорошо модель находит объекты класса 1?". Высокая полнота означает мало пропусков класса 1 (FN).

### 2.4. F1-Score (F1-Мера)

* **Определение:** Гармоническое среднее Precision и Recall.
* **Формула:**
    $$
    F1 = 2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}
    $$
* **Объяснение:** Балансирует Precision и Recall. Будет высокой только если обе метрики высоки. Гармоническое среднее сильнее штрафует за низкие значения одной из компонент по сравнению с арифметическим средним. Полезна при дисбалансе или когда важны и P, и R.

### 2.5. F-beta Score (Обобщенная F-Мера)

* **Определение:** Обобщение F1-меры, позволяющее придать разный вес Precision и Recall.
* **Формула:**
    $$
    F_\beta = (1 + \beta^2) \frac{\text{Precision} \times \text{Recall}}{(\beta^2 \times \text{Precision}) + \text{Recall}}
    $$
  * $\beta \ge 0$ - параметр, контролирующий баланс.
* **Объяснение:**
  * При $\beta = 1$, $F_\beta = F1$.
  * При $\beta > 1$, Recall считается важнее Precision (например, $F_2$ придает Recall вдвое больший вес).
  * При $0 \le \beta < 1$, Precision считается важнее Recall (например, $F_{0.5}$ придает Precision вдвое больший вес).

---

## 3. Метрики Бинарной Классификации (Ранжирующие / Порогонезависимые)

Оценивают качество разделения классов моделью по предсказанным вероятностям $\hat{p}^{(i)}$, не зависят от конкретного порога.

### 3.1. TPR (True Positive Rate) - Полнота / Recall / Чувствительность

* Повтор для ясности в контексте ROC.
* **Формула:**
    $$
    \text{TPR} = \frac{TP}{TP + FN}
    $$
* **Объяснение:** Доля истинных позитивных объектов, правильно идентифицированных моделью. Ось Y в ROC-кривой.

### 3.2. FPR (False Positive Rate) - Частота Ложных Срабатываний

* **Определение:** Доля истинных негативных объектов, которые модель ошибочно классифицировала как позитивные.
* **Формула:**
    $$
    \text{FPR} = \frac{FP}{FP + TN}
    $$
* **Объяснение:** Показывает, как часто модель "бьет ложную тревогу". Ось X в ROC-кривой.

### 3.3. ROC-Кривая и ROC AUC

* **ROC-Кривая:** График TPR (ось Y) против FPR (ось X) при изменении порога классификации.
* **ROC AUC (Площадь под ROC-кривой):** Число от 0 до 1, измеряющее общую способность модели ранжировать позитивные примеры выше негативных. AUC = 0.5 - случайное гадание, AUC = 1 - идеальное разделение.
* **Объяснение:** AUC показывает вероятность того, что случайно выбранный позитивный объект получит от модели оценку выше, чем случайно выбранный негативный объект. Относительно устойчив к дисбалансу классов, но может маскировать проблемы с Precision.

### 3.4. PR-Кривая и PR AUC

* **PR-Кривая:** График Precision (ось Y) против Recall (ось X) при изменении порога классификации.
* **PR AUC (Площадь под PR-кривой):** Число от (доля позитивного класса) до 1.
* **Объяснение:** Оценивает, насколько хорошо модель может достигать высокой точности (Precision) при нахождении позитивных объектов (Recall). **Более чувствительна к дисбалансу классов и производительности на минорном классе**, чем ROC AUC, так как Precision напрямую зависит от FP. Часто является более показательной метрикой при работе с несбалансированными данными, если важен минорный класс.
