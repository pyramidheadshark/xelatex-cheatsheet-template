# Детальный Вывод Формул: Регуляризация (L2 и L1)

*(Используем обозначения: $m$ - число объектов, $n$ - число *исходных* признаков, $x^{(i)}$ - вектор признаков $i$-го объекта ($[x_0^{(i)}, x_1^{(i)}, ..., x_n^{(i)}]^T$, где $x_0^{(i)}=1$), $y^{(i)}$ - истинное значение $i$-го объекта, $w$ - вектор весов ($[w_0, w_1, ..., w_n]^T$), $\hat{y}^{(i)}$ - предсказанное значение для $i$-го объекта (например, $\sum_{j=0}^{n} w_j x_j^{(i)}$ для лин. регрессии), $J_{original}(w)$ - исходная функция потерь (например, MSE), $J_{reg}(w)$ - регуляризованная функция потерь, $\lambda$ - коэффициент регуляризации, $\alpha$ - скорость обучения. Индекс $j$ для весов признаков пробегает от 1 до $n$, $w_0$ - свободный член, не регуляризуется).*

## 1. Общая Идея Регуляризации

Цель - предотвратить переобучение, добавив штраф за сложность модели (большие веса) к исходной функции потерь.

**Общая Формула Регуляризованной Функции Потерь:**
$$
J_{reg}(w) = J_{original}(w) + \text{Penalty}(w)
$$

* **Объяснение:** Мы минимизируем не только ошибку на обучающих данных ($J_{original}$), но и "штраф" за величину весов ($\text{Penalty}$). Это заставляет модель искать баланс между точностью на трейне и простотой (меньшими весами).

## 2. L2 Регуляризация (Гребневая / Ridge)

Штрафует модель пропорционально сумме квадратов весов (кроме $w_0$).

**Определение Штрафа L2 (Penalty):**
$$
\text{Penalty}_{L2}(w) = \frac{\lambda}{2m} \sum_{j=1}^{n} w_j^2
$$

* **$\lambda \ge 0$**: Гиперпараметр, контролирующий силу регуляризации.
* **$\sum_{j=1}^{n} w_j^2$**: Сумма квадратов весов *только* для признаков (от $w_1$ до $w_n$). $w_0$ не штрафуется.
* **$\frac{1}{2m}$**: Коэффициент для удобства и согласования с $J_{original}$ (часто берут $J_{original} = \frac{1}{2m}\sum(\dots)^2$). Ключевой элемент - $\lambda \sum w_j^2$.

**Регуляризованная Функция Потерь $J_{Ridge}(w)$ (Пример с MSE):**
$$
J_{Ridge}(w) = \underbrace{\frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2}_{J_{original} \text{ (MSE)}} + \underbrace{\frac{\lambda}{2m} \sum_{j=1}^{n} w_j^2}_{\text{L2 Penalty}}
$$

* **Объяснение:** Минимизируя $J_{Ridge}$, мы одновременно минимизируем ошибку MSE и сумму квадратов весов.

**Градиент L2 Регуляризации:**
Нам нужно найти производные $J_{Ridge}(w)$ по каждому весу $w_k$.
Производная суммы равна сумме производных: $\frac{\partial J_{Ridge}}{\partial w_k} = \frac{\partial J_{original}}{\partial w_k} + \frac{\partial (\text{Penalty}_{L2})}{\partial w_k}$.

**Шаг 1: Производная Штрафа L2 $\frac{\partial (\text{Penalty}_{L2})}{\partial w_k}$**

* **Случай 1: $k = 0$ (для свободного члена $w_0$)**
    Штраф $\text{Penalty}_{L2}$ не зависит от $w_0$ (суммирование с $j=1$).
    $$
    \frac{\partial (\text{Penalty}_{L2})}{\partial w_0} = 0
    $$
* **Случай 2: $k \ge 1$ (для веса признака $w_k$)**
    $$
    \frac{\partial (\text{Penalty}_{L2})}{\partial w_k} = \frac{\partial}{\partial w_k} \left( \frac{\lambda}{2m} \sum_{j=1}^{n} w_j^2 \right)
    $$
    Выносим константу и сумму (производная суммы равна сумме производных):
    $$
    = \frac{\lambda}{2m} \sum_{j=1}^{n} \frac{\partial}{\partial w_k} (w_j^2)
    $$
    Производная $\frac{\partial}{\partial w_k} (w_j^2)$ равна 0, если $j \ne k$, и равна $2w_k$, если $j = k$. Поэтому из всей суммы остается только одно слагаемое при $j=k$:
    $$
    = \frac{\lambda}{2m} (2 w_k) = \frac{\lambda}{m} w_k
    $$
  * **Объяснение:** Производная L2 штрафа по весу $w_k$ пропорциональна самому весу $w_k$.

**Шаг 2: Полный Градиент $J_{Ridge}(w)$**

* **Для $w_0$ ($k=0$):**
    $$
    \boxed{
    \frac{\partial J_{Ridge}}{\partial w_0} = \frac{\partial J_{original}}{\partial w_0} + 0 = \frac{\partial J_{original}}{\partial w_0}
    }
    $$
    *(Производная по $w_0$ такая же, как и без регуляризации).*
* **Для $w_k$ ($k=1, ..., n$):**
    $$
    \boxed{
    \frac{\partial J_{Ridge}}{\partial w_k} = \frac{\partial J_{original}}{\partial w_k} + \frac{\lambda}{m} w_k
    }
    $$
    *(К исходному градиенту добавляется член $\frac{\lambda}{m} w_k$).*

**Обновление Весов в Градиентном Спуске с L2:**

* **Для $w_0$ ($k=0$):**
    $$
    w_0 := w_0 - \alpha \frac{\partial J_{original}}{\partial w_0}
    $$
* **Для $w_k$ ($k=1, ..., n$):**
    $$
    w_k := w_k - \alpha \left( \frac{\partial J_{original}}{\partial w_k} + \frac{\lambda}{m} w_k \right)
    $$
    Перегруппируем члены:
    $$
    w_k := w_k - \alpha \frac{\lambda}{m} w_k - \alpha \frac{\partial J_{original}}{\partial w_k}
    $$
    $$
    \boxed{
    w_k := w_k \left(1 - \alpha \frac{\lambda}{m}\right) - \alpha \frac{\partial J_{original}}{\partial w_k}
    }
    $$
  * **Объяснение (Weight Decay):** Формула показывает, что перед выполнением основного шага градиентного спуска (вычитание $\alpha \frac{\partial J_{original}}{\partial w_k}$), вес $w_k$ умножается на коэффициент $(1 - \alpha \frac{\lambda}{m})$. Так как $\alpha, \lambda, m$ положительны, этот коэффициент меньше 1 (при разумной $\alpha$). Это означает, что вес $w_k$ на каждой итерации немного "уменьшается" или "затухает" (weight decay) по направлению к нулю. Величина затухания пропорциональна текущему значению веса $w_k$. Это и есть механизм "сжатия" весов в L2 регуляризации.

## 3. L1 Регуляризация (Lasso)

Штрафует модель пропорционально сумме модулей весов (кроме $w_0$).

**Определение Штрафа L1 (Penalty):**
$$
\text{Penalty}_{L1}(w) = \frac{\lambda}{m} \sum_{j=1}^{n} |w_j|
$$

* **$|w_j|$**: Модуль (абсолютное значение) веса $w_j$.
* Остальные обозначения ($\lambda, m, \sum_{j=1}^{n}$) те же, что и для L2.

**Регуляризованная Функция Потерь $J_{Lasso}(w)$ (Пример с MSE):**
$$
J_{Lasso}(w) = \underbrace{\frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2}_{J_{original} \text{ (MSE)}} + \underbrace{\frac{\lambda}{m} \sum_{j=1}^{n} |w_j|}_{\text{L1 Penalty}}
$$

* **Объяснение:** Минимизируя $J_{Lasso}$, мы одновременно минимизируем ошибку MSE и сумму модулей весов.

**Градиент L1 Регуляризации (Проблема и Решение):**

**Проблема:** Функция модуля $|w_k|$ не дифференцируема в точке $w_k = 0$. У нее нет однозначно определенной производной в этой точке.

**Решение: Использование Субградиента**
Субградиент - это обобщение понятия градиента для негладких выпуклых функций. Для модуля $|w_k|$ субградиент $\partial |w_k|$ определяется как:
$$
\partial |w_k| = \begin{cases}
    +1, & \text{if } w_k > 0 \\
    -1, & \text{if } w_k < 0 \\
    \text{любое число в } [-1, 1], & \text{if } w_k = 0
\end{cases}
$$
Часто используют обозначение $\text{sign}(w_k)$ для субградиента, где $\text{sign}(0)$ может быть принято равным 0 (хотя это не единственный вариант) для простоты реализации алгоритмов.
$$
\text{sign}(w_k) = \begin{cases}
    +1, & \text{if } w_k > 0 \\
    -1, & \text{if } w_k < 0 \\
    0, & \text{if } w_k = 0
\end{cases}
$$

**Шаг 1: Субградиент Штрафа L1 $\partial (\text{Penalty}_{L1}) / \partial w_k$**

* **Случай 1: $k = 0$ (для свободного члена $w_0$)**
    $$
    \frac{\partial (\text{Penalty}_{L1})}{\partial w_0} = 0
    $$
* **Случай 2: $k \ge 1$ (для веса признака $w_k$)**
    Используя субградиент для $|w_j|$:
    $$
    \frac{\partial (\text{Penalty}_{L1})}{\partial w_k} = \frac{\partial}{\partial w_k} \left( \frac{\lambda}{m} \sum_{j=1}^{n} |w_j| \right) = \frac{\lambda}{m} \sum_{j=1}^{n} \frac{\partial |w_j|}{\partial w_k}
    $$
    Производная (субградиент) $\frac{\partial |w_j|}{\partial w_k}$ равна 0, если $j \ne k$, и равна $\text{sign}(w_k)$, если $j = k$.
    $$
    \frac{\partial (\text{Penalty}_{L1})}{\partial w_k} = \frac{\lambda}{m} \text{sign}(w_k)
    $$
  * **Объяснение:** Субградиент L1 штрафа по весу $w_k$ равен константе ($\lambda/m$ или $-\lambda/m$), знак которой зависит от знака $w_k$.

**Шаг 2: Полный Субградиент $J_{Lasso}(w)$**

* **Для $w_0$ ($k=0$):**
    $$
    \boxed{
    \frac{\partial J_{Lasso}}{\partial w_0} = \frac{\partial J_{original}}{\partial w_0}
    }
    $$
* **Для $w_k$ ($k=1, ..., n$):**
    $$
    \boxed{
    \frac{\partial J_{Lasso}}{\partial w_k} = \frac{\partial J_{original}}{\partial w_k} + \frac{\lambda}{m} \text{sign}(w_k)
    }
    $$

**Обновление Весов в Субградиентном Спуске с L1 (Концептуально):**

* **Для $w_0$ ($k=0$):**
    $$
    w_0 := w_0 - \alpha \frac{\partial J_{original}}{\partial w_0}
    $$
* **Для $w_k$ ($k=1, ..., n$):**
    $$
    \boxed{
    w_k := w_k - \alpha \left( \frac{\partial J_{original}}{\partial w_k} + \frac{\lambda}{m} \text{sign}(w_k) \right)
    }
    $$
* **Объяснение (Почему L1 обнуляет веса):** Рассмотрим член регуляризации $-\alpha \frac{\lambda}{m} \text{sign}(w_k)$. Он вычитает **константу** ($\alpha \lambda / m$), если $w_k > 0$, и прибавляет константу, если $w_k < 0$. Это означает, что L1 регуляризация "тянет" вес к нулю с *постоянной силой*, независимо от величины веса (в отличие от L2, где сила $\frac{\lambda}{m} w_k$ уменьшается по мере приближения к нулю). Из-за этой постоянной "тяги" веса, которые становятся достаточно малыми, могут пересечь ноль и стать равными нулю за конечное число шагов. Это и есть механизм **отбора признаков** в Lasso.
*(Примечание: На практике для L1 часто используют более сложные Проксимальные или Координатные методы спуска, которые корректнее обрабатывают точку $w_k=0$).*

## 4. Elastic Net

Комбинирует L1 и L2 регуляризацию.

**Определение Штрафа Elastic Net:**
$$
\text{Penalty}_{ElasticNet}(w) = \underbrace{\frac{\lambda_1}{m} \sum_{j=1}^{n} |w_j|}_{\text{L1 часть}} + \underbrace{\frac{\lambda_2}{2m} \sum_{j=1}^{n} w_j^2}_{\text{L2 часть}}
$$

* Или с параметром смешивания $\alpha_{mix} \in [0, 1]$:

$$
\text{Penalty}_{ElasticNet}(w) = \lambda \left( \alpha_{mix} \sum_{j=1}^{n} |w_j| + (1 - \alpha_{mix}) \frac{1}{2} \sum_{j=1}^{n} w_j^2 \right)
$$

* **Объяснение:** Позволяет одновременно использовать преимущества L1 (разреженность, отбор признаков) и L2 (стабильность при коррелирующих признаках, сжатие). Требует подбора двух гиперпараметров ($\lambda_1, \lambda_2$ или $\lambda, \alpha_{mix}$) и сложных методов оптимизации.

## 5. Важность Масштабирования Признаков

* **Необходимость:** Перед применением ЛЮБОЙ регуляризации (L1, L2, Elastic Net) **крайне важно** масштабировать признаки $x_1, ..., x_n$, чтобы они имели примерно одинаковый диапазон значений.
* **Причина:** Штрафы $\lambda \sum w_j^2$ и $\lambda \sum |w_j|$ зависят от величины весов $w_j$. Если признаки не масштабированы, то веса $w_j$ будут отражать не только важность признака, но и его масштаб. Регуляризация будет непропорционально сильно штрафовать веса признаков с большим исходным масштабом, что некорректно. Масштабирование уравнивает "стартовые условия" для всех признаков.
* **Методы:** StandardScaler, MinMaxScaler (применять `fit_transform` на трейне, `transform` на валидации/тесте).
