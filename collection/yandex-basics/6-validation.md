## Углубленный Конспект: Валидация и Оценка Модели

### 1. Зачем Нужна Валидация и Оценка? Мотивация

* **Цель Машинного Обучения:** Построить модель, которая хорошо **обобщается (generalizes)**, то есть делает точные предсказания на **новых, невиданных ранее данных**, а не только на тех, на которых она обучалась.
* **Проблема Переобучения (Overfitting):** Модель слишком хорошо "запоминает" обучающие данные, включая шум и случайные выбросы. Она показывает отличные метрики на обучении (train), но плохо работает на новых данных (высокая variance, низкий bias).
* **Проблема Недообучения (Underfitting):** Модель слишком проста, чтобы уловить основные закономерности в данных. Она показывает плохие метрики и на обучении, и на новых данных (высокий bias, низкая variance).
* **Решение:** Нужны механизмы, чтобы:
    1. Оценить, насколько хорошо модель будет работать на новых данных (оценка **обобщающей способности**).
    2. Подобрать **гиперпараметры** модели (например, силу регуляризации $\lambda$, глубину дерева, скорость обучения $\alpha$), которые обеспечивают наилучший баланс между bias и variance (оптимальную обобщающую способность).
    3. Получить **финальную, несмещенную оценку** качества выбранной и настроенной модели.

### 2. Базовое Разделение Данных: Train / Validation / Test Split

* **Концепция:** Простейший способ организовать процесс оценки и настройки – **однократно** разделить *все* доступные данные на три **непересекающиеся** части:
    1. **Обучающая выборка (Train Set):**
        * **Назначение:** Используется **только** для **обучения** модели, т.е., подбора ее **параметров** (весов $w$). Модель "смотрит" на эти данные и минимизирует на них функцию потерь (например, с помощью градиентного спуска).
        * **Размер:** Обычно самая большая часть (60-80% данных).
    2. **Валидационная выборка (Validation Set / Dev Set):**
        * **Назначение:** Используется для:
            * **Настройки гиперпараметров:** Мы пробуем разные значения $\lambda$, $\alpha$, глубины дерева и т.д., обучаем модель на *Train Set* и выбираем те гиперпараметры, которые дают лучшую метрику на *Validation Set*.
            * **Выбора модели:** Сравниваем разные архитектуры или алгоритмы (например, LogReg vs SVM vs RF), обученные на *Train Set*, и выбираем лучший по метрике на *Validation Set*.
            * **Ранней остановки (Early Stopping):** Мониторим метрику на *Validation Set* во время итеративного обучения (GD) и останавливаемся, когда она перестает улучшаться или начинает ухудшаться (признак переобучения).
        * **Важно:** Модель **не обучается** напрямую на этих данных (не используется для подгонки весов $w$).
        * **Размер:** Обычно 10-20% данных.
    3. **Тестовая выборка (Test Set / Hold-out Set):**
        * **Назначение:** Используется **только один раз в самом конце** всего процесса разработки для получения **финальной, объективной и несмещенной оценки** качества выбранной и настроенной модели.
        * **Важно:** Эти данные модель **никогда не должна была "видеть"** ни на этапе обучения, ни на этапе валидации/настройки гиперпараметров. Результат на тесте показывает, как модель, скорее всего, будет работать в реальных условиях на новых данных.
        * **Размер:** Обычно 10-20% данных.

* **Отличие от Кросс-Валидации:** Этот сплит делается *один раз*. Кросс-валидация – это более сложная техника, используемая *вместо* одного статического Validation Set (см. далее).

### 3. Кросс-Валидация (Cross-Validation, CV)

* **Определение:** Техника **перекрестной проверки** (или скользящего контроля), предназначенная для получения более **надежной и стабильной оценки** качества модели и для более эффективной **настройки гиперпараметров**, чем при использовании одного статичного Validation Set.
* **Мотивация:** Оценка на одном Validation Set может сильно зависеть от того, какие именно данные в него попали (особенно при малом размере выборки). CV усредняет оценки по нескольким разным валидационным подвыборкам.
* **Принцип Работы (Общий):** Используются данные, предназначенные для обучения и валидации (т.е., *весь набор данных, кроме отложенного Test Set*). Эти данные многократно разбиваются на обучающие и валидационные фолды (части), модель обучается и оценивается на каждой итерации, результаты усредняются.
* **Основные Виды CV:**
    1. **K-Fold Cross-Validation:**
        * **Алгоритм:**
            1. Данные (Train+Validation) случайным образом **перемешиваются** и делятся на $K$ **равных** (или почти равных) непересекающихся частей (**фолдов / folds**). Типичные значения $K$: 5 или 10.
            2. Запускается цикл из $K$ итераций:
                * На итерации $k$: $k$-ый фолд используется как **валидационный**, а остальные $K-1$ фолдов объединяются и используются как **обучающий**.
                * Модель обучается на обучающем фолде и оценивается (считается метрика $M_k$) на валидационном фолде $k$.
            3. Итоговая оценка метрики – это **среднее** по всем $K$ итерациям:
                $\bar{M} = \frac{1}{K} \sum_{k=1}^K M_k$
                Также часто смотрят на **стандартное отклонение** метрик по фолдам ($std(M_k)$), чтобы оценить стабильность модели.
        * **Применение:** Стандартный выбор для регрессии и сбалансированной классификации.
    2. **Stratified K-Fold Cross-Validation:**
        * **Отличие от K-Fold:** При разделении на $K$ фолдов **сохраняется процентное соотношение классов** целевой переменной в каждом фолде таким же, как и в исходном наборе данных.
        * **Когда Нужен:** **Обязателен** для задач **классификации**, особенно при **дисбалансе классов**. Гарантирует, что в каждом валидационном фолде будут присутствовать представители всех (или почти всех) классов в адекватной пропорции, что делает оценку метрик (Precision, Recall, F1, AUC) более надежной и осмысленной.
        * **Применение:** Стандарт де-факто для большинства задач классификации.
    3. **Leave-One-Out Cross-Validation (LOOCV):**
        * **Алгоритм:** Частный случай K-Fold, где $K$ равно числу объектов $m$. На каждой итерации модель обучается на $m-1$ объектах, а валидируется на **одном** оставшемся объекте. Цикл повторяется $m$ раз.
        * **Плюсы:** Дает почти **несмещенную** оценку ошибки (т.к. обучается почти на всех данных).
        * **Минусы:** Очень **высокая дисперсия** оценки (оценки на соседних фолдах сильно коррелируют). **Вычислительно очень дорогой** ($m$ обучений модели).
        * **Применение:** Только для **очень маленьких** наборов данных, где нельзя позволить себе отложить даже K-тый фолд для валидации.
* **Использование CV для Настройки Гиперпараметров:**
    1. Задается сетка (Grid) или распределение (Random Search) возможных значений гиперпараметров (например, $\lambda \in \{0.01, 0.1, 1, 10\}$).
    2. Для **каждого** набора гиперпараметров выполняется **полная K-Fold (или Stratified K-Fold) CV**.
    3. Рассчитывается **средняя метрика** $\bar{M}$ по фолдам для каждого набора гиперпараметров.
    4. Выбирается тот набор гиперпараметров, который дал **наилучшую среднюю метрику** $\bar{M}$.
    5. (Опционально) Финальная модель обучается на **всех** данных (Train+Validation) с использованием выбранных оптимальных гиперпараметров.

### 4. Утечка Данных (Data Leakage)

* **Определение:** Ситуация, когда в процессе обучения или валидации модели используется информация, которая **не будет доступна модели в момент реального предсказания (на новых данных)**. Это приводит к **нереалистично завышенной** оценке качества модели на валидации/тесте и плохому перформансу в реальных условиях.
* **Ключевая Идея:** Модель "подглядывает в будущее" или использует информацию, которую она не должна знать.
* **Распространенные Виды и Способы Избежать:**
    1. **Утечка через Препроцессинг (Preprocessing Leakage):**
        * **Пример:** Вычисление среднего $\mu$ и ст. отклонения $\sigma$ для StandardScaler **на всей выборке (включая val/test)**, а затем применение этого scaler ко всем частям. Модель на валидации/тесте получает информацию (среднее/ст.откл.) из будущего/невиданных данных.
        * **Как избежать:** **Fit** (вычисление параметров препроцессинга: $\mu, \sigma$, min/max, словарmя для кодировщика и т.д.) выполняется **только на обучающей выборке (Train Set)**. Затем **Transform** (применение препроцессинга) выполняется на Train, Validation и Test сетах с использованием параметров, вычисленных **только на Train**. При кросс-валидации `fit` делается на K-1 фолдах, `transform` - на них же и на валидационном фолде. Использовать `sklearn.pipeline.Pipeline` помогает автоматизировать этот процесс.
    2. **Утечка через Отбор Признаков (Feature Selection Leakage):**
        * **Пример:** Отбор признаков на основе их корреляции с целевой переменной $y$ или с использованием p-value, рассчитанных **на всей выборке** до разделения или до CV.
        * **Как избежать:** Отбор признаков должен производиться **внутри каждого фолда кросс-валидации** (используя только обучающую часть фолда) или на отдельном Validation Set, если не используется CV.
    3. **Утечка Целевой Переменной (Target Leakage):**
        * **Пример:** Использование признака, который напрямую или косвенно содержит информацию о таргете, но не будет известен в момент предсказания. Например, предсказывать отток клиента, используя признак "сумма последнего платежа после оттока".
        * **Как избежать:** Тщательный **анализ данных (EDA)** и **понимание предметной области**. Удаление таких признаков.
    4. **Утечка Временного Порядка (Time Series Leakage):**
        * **Пример:** Использование будущих данных для предсказания прошлого (например, случайное перемешивание временного ряда перед K-Fold CV).
        * **Как избежать:** Использование специальных методов валидации для временных рядов (см. ниже), сохранение хронологического порядка.
    5. **Утечка из Train в Test (Редкая, но возможная):**
        * **Пример:** Наличие явных **дубликатов** объектов, которые случайно попали и в Train, и в Test. Ошибки при ручном разделении выборок.
        * **Как избежать:** Проверка на дубликаты при EDA. Аккуратность при работе с данными.

### 5. Валидация для Временных Рядов

* **Ключевое Ограничение:** Нельзя перемешивать данные, так как временной порядок важен. Стандартный K-Fold/Stratified K-Fold **неприменимы**.
* **Основная Идея:** Обучающая выборка всегда должна предшествовать валидационной во времени.
* **Методы (Используется `sklearn.model_selection.TimeSeriesSplit` или ручная реализация):**
    1. **Expanding Window (Расширяющееся Окно):**
        * Итерация 1: Train =, Val =
        * Итерация 2: Train =, Val =
        * Итерация 3: Train =, Val =
        * ...
        * **Схема:** Используется вся доступная прошлая история для обучения. Размер Train растет.
        * **Когда использовать:** Если предполагается, что вся прошлая история важна для предсказания.
    2. **Sliding Window (Скользящее / Перемещающееся Окно):**
        * Задается фиксированный размер Train ($W_{train}$) и Val ($W_{val}$).
        * Итерация 1: Train = [1...$W_{train}$], Val = [$W_{train}$+1...$W_{train}$+$W_{val}$]
        * Итерация 2: Train = [2...$W_{train}$+1], Val = [$W_{train}$+2...$W_{train}$+$W_{val}$+1]
        * ...
        * **Схема:** Окно обучения и окно валидации сдвигаются во времени, сохраняя свои размеры.
        * **Когда использовать:** Если предполагается, что только недавнее прошлое важно, или если данные нестационарны (старые данные могут мешать).

### 6. Работа с Несбалансированными Данными при Валидации и Обучении

* **Проблема:** В задачах классификации один класс встречается значительно чаще другого (например, 99% класс 0, 1% класс 1). Это мешает модели нормально обучаться на минорном классе и делает стандартные метрики (Accuracy) и схемы валидации неадекватными.
* **Стратегии:**
    1. **Правильная Валидация:**
        * Использовать **Stratified K-Fold CV**, чтобы гарантировать репрезентативность классов в каждом фолде.
    2. **Правильные Метрики:**
        * Не использовать Accuracy.
        * Смотреть на **Precision, Recall, F1-score** (особенно для минорного класса).
        * Использовать **PR AUC** (особенно если важен минорный класс) или **ROC AUC** (с пониманием его ограничений при дисбалансе).
        * Матрица ошибок (Confusion Matrix).
    3. **Изменение Данных (Resampling Techniques):**
        * **Цель:** Сбалансировать классы в **обучающей** выборке.
        * **ВАЖНО:** Применять **ТОЛЬКО к обучающей части (train fold) ВНУТРИ каждого шага кросс-валидации**. Не применять ко всей выборке до CV!
        * **Методы:**
            * **Random Oversampling:** Случайное **дублирование** объектов минорного класса.
                * Плюсы: Просто.
                * Минусы: Легко приводит к **переобучению** на дубликатах. Редко рекомендуется как основной метод.
            * **Random Undersampling:** Случайное **удаление** объектов доминантного (большинства) класса.
                * Плюсы: Уменьшает объем данных, ускоряет обучение.
                * Минусы: **Потеря информации** о доминантном классе, что может ухудшить качество модели (повысить variance или bias, если удалены важные примеры). Рискованно, если данных мало.
            * **SMOTE (Synthetic Minority Over-sampling Technique):** Генерация **синтетических** объектов минорного класса.
                * **Алгоритм:**
                    1. Для каждого объекта $x_i$ минорного класса найти его $k$ ближайших соседей (из минорного же класса).
                    2. Случайно выбрать одного из этих $k$ соседей, $x_{zi}$.
                    3. Сгенерировать новый объект $x_{new}$ на отрезке между $x_i$ и $x_{zi}$:
                        $x_{new} = x_i + \lambda \cdot (x_{zi} - x_i)$, где $\lambda$ - случайное число из.
                * Плюсы: Не теряет информацию, создает новые, похожие примеры, часто работает лучше Oversampling.
                * Минусы: Может создавать шум, если классы сильно пересекаются. Чувствителен к выбору $k$.
    4. **Изменение Алгоритма Обучения (Algorithm Level):**
        * **Взвешивание Классов (Class Weighting):** Модификация функции потерь, чтобы ошибки на объектах минорного класса имели больший вес.
            * **Пример для Log Loss:**
                $J(w) = -\frac{1}{m} \sum_{i=1}^{m} [w_{y^{(i)}} ( y^{(i)} \log(\hat{p}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{p}^{(i)}) ) ]$
                Где $w_{y^{(i)}}$ - вес, соответствующий классу $y^{(i)}$ (например, вес для минорного класса > вес для доминантного).
            * Многие библиотеки (sklearn) имеют параметр `class_weight='balanced'`, который автоматически подбирает веса обратно пропорционально частотам классов.
            * **Преимущество:** Не меняет распределение данных, часто работает хорошо и является предпочтительным методом перед resampling.
        * Некоторые алгоритмы (например, деревья решений) могут быть менее чувствительны к дисбалансу или иметь встроенные механизмы для работы с ним.

### 7. Финальная Оценка Модели

* После выбора лучшей модели и оптимальных гиперпараметров с помощью Train/Validation сетов (или CV), модель **один раз** оценивается на **Test Set**.
* Полученная метрика на Test Set – это финальная, наиболее объективная оценка того, как модель будет работать на новых данных.
* **Важно:** Не использовать Test Set для дальнейшей настройки модели или гиперпараметров! Иначе оценка перестанет быть объективной.

### 8. Статистическая Значимость Оценки (Краткое напоминание)

* Метрика, полученная на Test Set (или средняя по CV) – это **оценка**, имеющая некоторую **случайную вариативность**.
* Чтобы сравнить две модели A и B и понять, действительно ли улучшение метрики у A по сравнению с B **не случайно**, используются **статистические тесты гипотез**:
  * Для сравнения по **фолдам CV**: **Парные тесты** (Парный t-тест или тест Уилкоксона для связанных выборок) на разности метрик $M_{A,k} - M_{B,k}$.
  * Для сравнения на **одном Test Set**: **Bootstrap** (многократная перевыборка с возвращением из Test Set, построение распределения разности метрик, оценка доверительного интервала).
