## Углубленный Конспект: Статистическая Значимость в Оценке Моделей ML

### 1. Введение: Зачем нужна Статистическая Значимость?

* **Проблема:** При сравнении двух моделей (например, твоей модели A и модели B конкурента) на тестовой выборке мы получаем некоторые значения метрик, скажем, $AUC_A = 0.85$ и $AUC_B = 0.83$. Можем ли мы уверенно сказать, что модель A *действительно* лучше? Не факт.
* **Ключевая Идея:** Любая метрика, посчитанная на **конечной тестовой выборке**, является **случайной величиной**. Это лишь **оценка** истинного, неизвестного нам качества модели на всей генеральной совокупности данных. Эта оценка имеет **разброс (дисперсию)** из-за случайности выбора конкретных объектов в тестовый сет.
* **Статистическая Значимость:** Это инструмент, который помогает нам определить, является ли наблюдаемая разница между метриками моделей (например, $AUC_A - AUC_B$) **реальным эффектом** или она могла возникнуть **случайно** из-за вариативности данных.
* **Цель:** Принимать обоснованные решения о том, какая модель лучше, учитывая фактор случайности.

### 2. Проблема Оценки на Одном Тестовом Сете

* **Вопрос (Интервью):** *Почему нельзя просто сравнить метрики двух моделей на одном тестовом сете и выбрать лучшую?*
* **Ответ (Детальный):**
    1. **Метрика как Оценка:** Значение метрики (например, Accuracy, AUC, F1), полученное на тестовом сете, — это **точечная оценка** истинного значения метрики, которое модель показала бы на всех возможных данных.
    2. **Выборочная Вариативность:** Тестовый сет — это всего лишь **случайная выборка** из генеральной совокупности данных. Если бы мы взяли другую случайную выборку в качестве теста, мы, скорее всего, получили бы немного **другое значение** метрики для той же самой модели.
    3. **Дисперсия Оценки:** Из-за этой выборочной вариативности наша оценка метрики имеет **дисперсию (variance)**. Величина этой дисперсии зависит от:
        * Размера тестовой выборки (чем больше выборка, тем меньше дисперсия).
        * Вариативности самих данных (насколько они "шумные").
        * Стабильности самой модели.
    4. **Ненадежное Сравнение:** Сравнивая две точечные оценки ($M_A$ и $M_B$) без учета их возможной дисперсии, мы рискуем ошибиться. Наблюдаемая разница $M_A - M_B$ может быть просто **следствием случайности**, а не реальным превосходством одной модели над другой.
* **Объяснение простыми словами:** Представь, что ты хочешь сравнить средний рост двух групп людей, но измерил рост только у 5 случайно выбранных человек из каждой группы. Если одна группа в среднем оказалась выше на 1 см, это может быть случайностью из-за малого числа измерений. Так же и с метриками на тесте.

### 3. Повышение Надежности Оценки: Кросс-Валидация (CV)

* **Вопрос (Интервью):** *Как можно оценить надежность полученной метрики или снизить ее дисперсию?*
* **Ответ (Детальный):**
    1. **Кросс-Валидация (K-Fold CV):** Основной метод для получения более надежной оценки.
        * **Алгоритм:** Данные делятся на $K$ непересекающихся частей (фолдов). Модель обучается $K$ раз. На $k$-ой итерации модель обучается на $K-1$ фолдах и валидируется на оставшемся $k$-ом фолде.
        * **Результат:** Мы получаем $K$ значений метрики ($M_1, M_2, ..., M_K$), по одному на каждый фолд.
    2. **Снижение Дисперсии Оценки:** Итоговая оценка метрики модели обычно берется как **среднее** по $K$ фолдам: $\bar{M} = \frac{1}{K} \sum_{k=1}^{K} M_k$. Эта усредненная оценка имеет **меньшую дисперсию**, чем оценка на одном фолде (или одном случайном train/test сплите), и поэтому является более **стабильной и надежной**. (Стандартная ошибка среднего: $SE_{\bar{M}} = s_M / \sqrt{K}$, где $s_M$ - ст. откл. метрик по фолдам).
    3. **Оценка Надежности (Доверительный Интервал):** На основе $K$ значений метрики $(M_1, ..., M_K)$ можно построить **доверительный интервал (Confidence Interval, CI)** для истинного среднего значения метрики $\mu_M$.
        * **Формула (примерно, для 95% CI при достаточно большом K или нормальности $M_k$):**
            $\text{CI} = \bar{M} \pm t_{\alpha/2, K-1} \cdot \frac{s_M}{\sqrt{K}}$
            * $\bar{M}$: Среднее значение метрики по фолдам.
            * $s_M$: Стандартное отклонение метрики по фолдам.
            * $K$: Количество фолдов.
            * $t_{\alpha/2, K-1}$: Квантиль t-распределения Стьюдента с $K-1$ степенями свободы для уровня значимости $\alpha$ (для 95% CI, $\alpha=0.05$). При больших $K$ можно использовать z-квантиль $\approx 1.96$.
        * **Интерпретация:** "Мы на 95% уверены, что истинное среднее значение метрики нашей модели лежит в этом интервале". Ширина интервала показывает надежность оценки: чем шире, тем менее надежна.
    4. **Stratified K-Fold:** В задачах классификации (особенно с дисбалансом) важно использовать стратифицированную CV, чтобы **сохранить исходное соотношение классов** в каждом фолде. Это дает более репрезентативные оценки.

### 4. Сравнение Моделей A и B с Помощью CV

* **Вопрос (Интервью):** *Как сравнить две модели A и B, обученные и протестированные с помощью cross-validation, и понять, значимо ли модель A лучше B?*
* **Ответ (Детальный):**
    1. **Парные Данные:** Ключевой момент — оценки метрик для моделей A и B, полученные на *одном и том же* $k$-ом фолде CV ($M_{A,k}$ и $M_{B,k}$), являются **связанными (парными)**, так как они получены на одинаковых обучающих и валидационных данных внутри этого фолда. Мы не можем рассматривать выборки $(M_{A,1}, ..., M_{A,K})$ и $(M_{B,1}, ..., M_{B,K})$ как независимые.
    2. **Анализ Разностей:** Нас интересует, отличается ли *в среднем* метрика модели A от метрики модели B. Для этого мы анализируем **попарные разности** метрик по фолдам: $d_k = M_{A,k} - M_{B,k}$ для $k=1, ..., K$.
    3. **Метод 1: Парный Статистический Тест:** Проверяем гипотезу о том, что средняя разность $\mu_d$ равна нулю.
        * **Гипотезы:**
            * $H_0: \mu_d = 0$ (Нулевая гипотеза: Средние метрики моделей A и B не различаются).
            * $H_1: \mu_d \ne 0$ (Альтернативная гипотеза: Средние метрики различаются). (Или односторонняя $H_1: \mu_d > 0$, если мы хотим доказать, что A лучше B).
        * **Выбор Теста:**
            * **Парный t-критерий Стьюдента (Paired t-test):** Используется, если можно предположить, что **разности $d_k$ распределены примерно нормально** (или если число фолдов $K$ достаточно велико, например, $K \ge 30$, по Центральной Предельной Теореме).
                * **Статистика теста:** $t = \frac{\bar{d} - 0}{s_d / \sqrt{K}}$, где $\bar{d} = \frac{1}{K}\sum d_k$ (средняя разность), $s_d$ (стандартное отклонение разностей $d_k$).
                * **Решение:** Вычисляется **p-value**. Если p-value < $\alpha$ (выбранный уровень значимости, обычно 0.05), то **отвергаем $H_0$** и заключаем, что разница статистически значима.
            * **Критерий Знаковых Рангов Уилкоксона (Wilcoxon Signed-Rank Test):** Непараметрический тест. Используется, если **нет оснований предполагать нормальность** разностей $d_k$. Сравнивает медианы распределений. Менее мощный, чем t-тест, если предположение о нормальности верно.
                * **Решение:** Также вычисляется p-value и сравнивается с $\alpha$.
    4. **Метод 2: Доверительный Интервал для Средней Разности:**
        * Строим CI для **средней разности** $\mu_d$ (используя $\bar{d}$, $s_d$ и $t$-квантиль, как в п.3.3).
        * **Интерпретация:** Если полученный (например, 95%) доверительный интервал **не содержит нуля**, то это эквивалентно отвержению $H_0$ на уровне значимости $\alpha=0.05$ (для двусторонней альтернативы). Мы можем утверждать, что разница между моделями статистически значима.

### 5. Сравнение Моделей A и B на Одном Тестовом Сете ("на коленке")

* **Вопрос (Интервью):** *Как можно оценить статистическую значимость разницы метрик на одном тестовом сете, "на коленке"?*
* **Ответ (Детальный):**
    1. **Метод: Бутстрап (Bootstrap)**
        * **Идея:** Имитировать получение множества тестовых выборок путем **повторной выборки с возвращением (resampling with replacement)** из *имеющегося единственного* тестового сета. Это позволяет оценить распределение и вариативность разницы метрик.
    2. **Алгоритм:**
        * Имеем: Исходный тестовый сет $D_{test}$ размера $N$. Метрики $M_A$ и $M_B$, посчитанные на $D_{test}$. Разница $\Delta = M_A - M_B$.
        * **Цикл Бутстрапа (повторяем $R$ раз, например, $R=1000$):**
            1. Создать **бутстрап-выборку** $D_{boot}$ размера $N$: случайным образом выбрать $N$ объектов из $D_{test}$ **с возвращением** (какой-то объект может быть выбран несколько раз, какой-то ни разу).
            2. Посчитать метрики моделей A и B **на этой бутстрап-выборке** $D_{boot}$: $M_{A, boot}$ и $M_{B, boot}$.
            3. Рассчитать разницу метрик для этой итерации: $\Delta_{boot} = M_{A, boot} - M_{B, boot}$.
        * **Результат:** Получаем $R$ значений разницы $\{\Delta_{boot, 1}, ..., \Delta_{boot, R}\}$. Это **эмпирическое распределение** разницы метрик.
    3. **Оценка Статистической Значимости:**
        * **Способ 1: Доверительный Интервал:**
            1. Отсортировать полученные $R$ значений $\Delta_{boot}$.
            2. Найти перцентили для желаемого уровня доверия (например, для 95% CI берем 2.5-й и 97.5-й перцентили). Обозначим их $\Delta_{low}$ и $\Delta_{high}$.
            3. Полученный 95% CI для разницы метрик: $[\Delta_{low}, \Delta_{high}]$.
            4. **Решение:** Если интервал **не содержит нуля**, то наблюдаемая разница $\Delta$ считается **статистически значимой** на уровне $\alpha=0.05$.
        * **Способ 2: P-value (приблизительно):**
            1. Посчитать долю бутстрап-разниц $\Delta_{boot}$, которые оказались "по другую сторону" от нуля по сравнению с исходной наблюдаемой разницей $\Delta$. Например, если $\Delta > 0$, считаем долю $p_{one-sided} = \frac{\text{count}(\Delta_{boot} \le 0)}{R}$.
            2. Для двустороннего теста (проверка на любое отличие), p-value $\approx 2 \cdot p_{one-sided}$.
            3. **Решение:** Если p-value < $\alpha$, отвергаем $H_0$ о равенстве метрик.
* **Объяснение простыми словами:** Мы как бы "перетряхиваем" наш тестовый сет много раз, создавая похожие на него псевдо-выборки. На каждой считаем разницу метрик. Смотрим, как часто эта разница случайно оказывается нулевой или меняет знак. Если это происходит редко, значит, наша исходная разница, скорее всего, не случайна.

### 6. Ключевые Статистические Концепции

* **Гипотезы:**
  * **Нулевая ($H_0$):** Гипотеза об отсутствии эффекта или разницы (например, $\mu_A = \mu_B$). Статус-кво.
  * **Альтернативная ($H_1$):** Гипотеза о наличии эффекта (например, $\mu_A \ne \mu_B$ или $\mu_A > \mu_B$). То, что мы хотим доказать.
* **Уровень Значимости ($\alpha$):** Порог вероятности. Максимальная вероятность **Ошибки I рода**, которую мы готовы допустить. Обычно $\alpha = 0.05$.
* **Ошибка I рода (False Positive):** Отвергнуть $H_0$, когда она верна. Вероятность = $\alpha$.
* **Ошибка II рода (False Negative):** Не отвергнуть $H_0$, когда она ложна. Вероятность = $\beta$.
* **P-value (Пи-значение):** **Вероятность получить наблюдаемые данные (или еще более экстремальные результаты), *если нулевая гипотеза $H_0$ верна***.
  * **Интерпретация:** Маленькое p-value (p < $\alpha$) означает, что наши данные *маловероятны* при условии $H_0$, поэтому мы **отвергаем $H_0$**. Большое p-value (p $\ge \alpha$) означает, что данные *совместимы* с $H_0$, и у нас **нет оснований отвергать $H_0$** (но это не доказывает $H_0$!).
* **Доверительный Интервал (Confidence Interval - CI):** Интервал, который с заданной **уверенностью (например, 95%)** накрывает **истинное значение** оцениваемого параметра (например, средней метрики $\mu_M$ или средней разности $\mu_d$).
  * **Построение:** $\text{Оценка} \pm (\text{Критическое значение} \times \text{Стандартная Ошибка Оценки})$.
  * **Стандартная Ошибка (Standard Error - SE):** Мера неопределенности нашей оценки (например, $SE_{\bar{M}} = s_M / \sqrt{K}$). Показывает, насколько в среднем наша оценка отклоняется от истинного значения.
  * **Использование для Гипотез:** Если CI для разности $\mu_d$ не содержит 0, это эквивалентно отвержению $H_0: \mu_d = 0$.

### 7. Итоги для Интервью

* Понимать, что метрика на тесте - **случайная величина** с **дисперсией**.
* Знать, что **CV** дает более **надежную оценку** метрики.
* Уметь сравнить две модели по результатам CV с помощью **парных тестов** (t-тест или Уилкоксон) или **CI для разности**. Знать, почему тесты парные.
* Знать метод **Бутстрапа** для сравнения моделей на **одном** тестовом сете.
* Уметь объяснить своими словами, что такое **p-value** и **доверительный интервал**, и как они используются для принятия решения о **статистической значимости**.
