% >>> Контент для шпаргалки по Градиентному Бустингу (GBM) - v2 (Разбитые блоки)

% ================================================
% Введение
% ================================================
\begin{myblock}{Определение Градиентного Бустинга (GBM)}
    \textbf{Градиентный Бустинг (Gradient Boosting Machine, GBM)} — это мощный ансамблевый метод машинного обучения, который строит модели \textbf{последовательно}, где каждая новая модель исправляет ошибки предыдущей. Считается одним из наиболее эффективных алгоритмов для табличных данных.
\end{myblock}

\begin{textbox}{Аналогия: Лепка Скульптуры}
    Процесс бустинга можно сравнить с лепкой скульптуры:
    \begin{itemize}[nosep, leftmargin=*]
        \item Начинаем с грубой основы (первая простая модель).
        \item Замечаем недочеты (ошибки модели).
        \item Добавляем "кусочек глины" там, где нужно (обучаем новую модель на ошибках).
        \item Повторяем добавление "кусочков" (новых моделей), пока результат не станет удовлетворительным.
    \end{itemize}
    Каждое добавление "кусочка глины" — это новая слабая модель в ансамбле бустинга.
\end{textbox}

% ================================================
% Раздел 1: Идея Бустинга (Boosting)
% ================================================
\section{Идея Бустинга (Boosting)}

\begin{myblock}{Отличие от Бэггинга}
    В отличие от \textbf{бэггинга} (например, Random Forest), где модели обучаются независимо и параллельно на разных подвыборках данных, в \textbf{бустинге} модели строятся строго \textbf{последовательно}.
\end{myblock}

\begin{textbox}{Алгоритм Последовательного Исправления Ошибок}
    Общая схема бустинга выглядит так:
    \begin{enumerate}[nosep]
        \item Обучается первая (обычно простая) модель $F_0$ на исходных данных.
        \item Вычисляются ошибки (или остатки) $e_1 = y - F_0(x)$ этой модели.
        \item Следующая модель $h_1$ обучается предсказывать эти ошибки $e_1$.
        \item Предсказание ансамбля обновляется: $F_1(x) = F_0(x) + \nu \cdot h_1(x)$ (где $\nu$ - темп обучения).
        \item Вычисляются новые ошибки $e_2 = y - F_1(x)$.
        \item Обучается следующая модель $h_2$ на ошибках $e_2$.
        \item Ансамбль обновляется: $F_2(x) = F_1(x) + \nu \cdot h_2(x)$.
        \item Шаги повторяются $M$ раз (заданное число моделей) или до остановки по критерию.
    \end{enumerate}
\end{textbox}

\begin{myblock}{Ключевая Идея}
    Ансамбль постепенно "учится" на своих ошибках. Каждая последующая модель фокусируется на тех объектах или аспектах данных, где предыдущие модели ошибались больше всего, тем самым улучшая общее предсказание.
\end{myblock}

% ================================================
% Раздел 2: Градиентный Спуск на Функциях
% ================================================
\section{Градиентный Спуск в Пространстве Функций}

\begin{myblock}{Минимизация Функции Потерь}
    GBM обобщает идею бустинга, используя \textbf{градиентный спуск} для минимизации произвольной дифференцируемой \textbf{функции потерь (Loss Function)} $L(y, F(x))$. Здесь $y$ - истинное значение, $F(x)$ - текущее предсказание ансамбля.
    \newline
    Ключевое отличие от стандартного градиентного спуска: оптимизация происходит не в пространстве параметров модели, а в \textbf{пространстве функций}.
\end{myblock}

\begin{myblock}{Шаги Градиентного Бустинга}
    Процесс обучения на шаге $m$ (для $m = 1, \dots, M$):
    \begin{enumerate}[label=\arabic*., wide, labelindent=0pt, itemsep=1ex]
        \item \textbf{Инициализация:} Начинаем с простого предсказания $F_0(x)$, обычно константы, минимизирующей loss (например, среднее $y$ для MSE, медиана для MAE, логарифм шансов для LogLoss).

        \item \textbf{Вычисление Псевдо-остатков:} Для каждого объекта $i$ вычисляется \textbf{отрицательный градиент} функции потерь по предсказанию ансамбля на предыдущем шаге $F_{m-1}(x)$:
          \[ r_{im} = - \left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F(x) = F_{m-1}(x)} \]
          Эти $r_{im}$ называются \textbf{псевдо-остатками} и показывают, в каком "направлении" нужно изменить предсказание $F_{m-1}(x_i)$, чтобы уменьшить ошибку $L$.

        \item \textbf{Обучение Слабой Модели:} Новая слабая модель $h_m(x)$ (обычно неглубокое дерево решений) обучается аппроксимировать псевдо-остатки $\{ (x_i, r_{im}) \}_{i=1}^N$.

        \item \textbf{Поиск Оптимального Шага (опционально):} Для дерева решений часто находят оптимальные значения $\gamma_{jm}$ в листьях $j$ дерева $h_m$.

        \item \textbf{Обновление Ансамбля:} Предсказание ансамбля обновляется:
          \[ F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x) \]
          где $\nu$ (nu) — это \textbf{темп обучения (learning rate)}, коэффициент $0 < \nu \le 1$ (обычно маленький, e.g., 0.01-0.1), который масштабирует вклад каждой новой модели. Он помогает предотвратить переобучение и делает сходимость более плавной.
    \end{enumerate}
\end{myblock}

\begin{textbox}{Аналогия: Спуск с Холма}
    Представьте функцию потерь как ландшафт, где высота — это ошибка.
    \begin{itemize}[nosep, leftmargin=*]
        \item Ваше текущее положение — предсказание ансамбля $F_{m-1}(x)$.
        \item Градиент $\frac{\partial L}{\partial F}$ показывает направление самого крутого подъема.
        \item Псевдо-остатки ($-\frac{\partial L}{\partial F}$) указывают в сторону спуска (анти-градиент).
        \item Обучение $h_m(x)$ на псевдо-остатках — это попытка найти "шаг" в направлении спуска.
        \item Learning rate $\nu$ — это размер этого шага. Маленькие шаги помогают не "проскочить" долину (минимум ошибки).
    \end{itemize}
\end{textbox}

% ================================================
% Раздел 3: Основные Функции Потерь (Loss Functions)
% ================================================
\section{Основные Функции Потерь (Loss Functions)}

\begin{textbox}{Зависимость от Задачи}
    Выбор функции потерь $L(y, F)$ критически важен и зависит от решаемой задачи (регрессия или классификация) и специфики данных (например, наличие выбросов).
\end{textbox}

\subsection{A Функции Потерь для Регрессии}
\begin{myexampleblock}{Регрессионные Loss-функции}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{MSE (Mean Squared Error) / L2 Loss:}
            \[ L(y, F) = \frac{1}{2}(y - F)^2 \]
            Псевдо-остатки: $r = y - F$. Стандартный выбор, но чувствителен к выбросам из-за квадратичной ошибки.
        \item \textbf{MAE (Mean Absolute Error) / L1 Loss:}
            \[ L(y, F) = |y - F| \]
            Псевдо-остатки: $r = \text{sign}(y - F)$. Менее чувствительна к выбросам, чем MSE.
        \item \textbf{Huber Loss:}
            \[ L(y, F) = \begin{cases} \frac{1}{2}(y - F)^2 & \text{if } |y - F| \le \delta \\ \delta (|y - F| - \frac{1}{2}\delta) & \text{if } |y - F| > \delta \end{cases} \]
            Комбинирует свойства MSE (для малых ошибок) и MAE (для больших ошибок), что делает ее робастной к выбросам. Параметр $\delta$ контролирует порог переключения.
        \item \textbf{Quantile Loss:} Используется для предсказания квантилей распределения целевой переменной.
    \end{itemize}
\end{myexampleblock}

\subsection{B Функции Потерь для Бинарной Классификации}
\begin{myexampleblock}{Бинарные Классификационные Loss-функции}
    Здесь $y \in \{0, 1\}$ или $y \in \{-1, 1\}$, а $F$ обычно представляет логит вероятности $p$, т.е. $F = \log(\frac{p}{1-p})$.
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{LogLoss (Логистическая / Бинарная Кросс-Энтропия):}
            \[ L(y, F) = \log(1 + e^{-F}) \quad \text{(для } y=1 \text{)} \quad \text{или} \quad L(y, F) = \log(1 + e^{F}) \quad \text{(для } y=0 \text{)} \]
            Или общая форма для $y \in \{0, 1\}$: $L(y, p) = -[y \log(p) + (1-y)\log(1-p)]$, где $p = \sigma(F) = \frac{1}{1+e^{-F}}$.
            Стандартный и наиболее распространенный выбор для задач классификации.
            Псевдо-остатки: $r = y - p$.
        \item \textbf{Exponential Loss (Экспоненциальная):}
            \[ L(y, F) = e^{-yF} \quad \text{(для } y \in \{-1, 1\}\text{)} \]
            Используется в алгоритме AdaBoost. Сильнее штрафует за неверные предсказания, может быть менее робастна к шуму/выбросам, чем LogLoss.
    \end{itemize}
\end{myexampleblock}

\subsection{C Функции Потерь для Многоклассовой Классификации}
\begin{myexampleblock}{Многоклассовые Классификационные Loss-функции}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Multinomial LogLoss (Категориальная Кросс-Энтропия):} Обобщение бинарной LogLoss на случай $K > 2$ классов. Ансамбль предсказывает вектор логитов $F = (F_1, \dots, F_K)$, вероятности получаются через Softmax: $p_k = \frac{e^{F_k}}{\sum_{j=1}^K e^{F_j}}$.
            \[ L(y, p) = - \sum_{k=1}^K y_k \log(p_k) \]
            где $y$ - one-hot вектор истинного класса.
    \end{itemize}
\end{myexampleblock}

% ================================================
% Раздел 4: Популярные Библиотеки: XGBoost, LightGBM, CatBoost
% ================================================
\section{Популярные Библиотеки GBM}

\begin{textbox}{Зачем Нужны Продвинутые Реализации?}
    Стандартный алгоритм GBM имеет ряд ограничений (например, склонность к переобучению, не самая высокая скорость). На практике почти всегда используют его улучшенные реализации, такие как XGBoost, LightGBM и CatBoost, которые включают множество оптимизаций и дополнительных возможностей.
\end{textbox}

\subsection{A XGBoost (eXtreme Gradient Boosting)}
\begin{myblock}{Ключевые Особенности XGBoost}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Регуляризация:} В функцию потерь при построении дерева добавляются штрафы L1 (Lasso) и L2 (Ridge) на веса листьев. Это контролирует сложность моделей и эффективно борется с переобучением.
        \item \textbf{Улучшенный Поиск Сплитов:} Использует информацию о второй производной функции потерь (Гессиан) для более точного построения деревьев.
        \item \textbf{Обработка Пропусков (NaN):} Имеет встроенный механизм для работы с пропущенными значениями: при построении дерева алгоритм "учится", в какую ветку (левую или правую) лучше направлять объекты с NaN для каждого признака.
        \item \textbf{Оптимизации Скорости:}
            \begin{itemize}[label=\textbullet, nosep, leftmargin=*]
                 \item Параллельные вычисления на уровне построения дерева (по признакам).
                 \item Приближенные алгоритмы поиска сплитов (quantile approximation) для больших данных.
                 \item Кэширование градиентов и гессианов.
                 \item Блочная структура данных для эффективного доступа к памяти.
            \end{itemize}
        \item \textbf{Кросс-валидация:} Встроенная функция для кросс-валидации при подборе числа деревьев.
    \end{itemize}
    \textit{Позиционирование:} XGBoost долгое время был "золотым стандартом" и де-факто выбором №1 для соревнований и промышленных задач на табличных данных.
\end{myblock}

\subsection{B LightGBM (Light Gradient Boosting Machine)}
\begin{myblock}{Ключевые Особенности LightGBM}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Высокая Скорость и Низкое Потребление Памяти:} Основное преимущество, особенно на больших датасетах. Достигается за счет нескольких техник.
        \item \textbf{Leaf-wise Рост Деревьев:} Вместо роста по уровням (level-wise, как в XGBoost), LightGBM выбирает для расщепления тот лист, который даст максимальное уменьшение функции потерь (gain). Это позволяет строить более глубокие и асимметричные деревья, что часто эффективнее, но требует контроля глубины (max\_depth) для предотвращения переобучения на малых данных.
        \item \textbf{GOSS (Gradient-based One-Side Sampling):} Для ускорения обучения на каждой итерации используются не все данные. Алгоритм сохраняет все объекты с большими градиентами (на которых модель сильно ошибается) и случайно отбирает долю объектов с малыми градиентами. Это позволяет сфокусироваться на "сложных" объектах без большого смещения оценки градиента.
        \item \textbf{EFB (Exclusive Feature Bundling):} Техника для уменьшения числа признаков путем объединения "взаимоисключающих" признаков (тех, которые редко принимают ненулевые значения одновременно, например, one-hot кодированные).
        \item \textbf{Оптимизированная Обработка Категориальных Признаков:} Поддерживает передачу категориальных признаков напрямую (без OHE), используя специальные алгоритмы сплита (Fisher).
    \end{itemize}
    \textit{Позиционирование:} Отличный выбор для очень больших датасетов, где скорость обучения и потребление памяти критичны.
\end{myblock}

\subsection{C CatBoost (Categorical Boosting)}
\begin{myblock}{Ключевые Особенности CatBoost}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Лучшая Обработка Категориальных Признаков:} Главная "фишка". Использует продвинутые методы кодирования категорий "на лету" во время обучения:
            \begin{itemize}[label=\textbullet, nosep, leftmargin=*]
                 \item \textbf{Ordered Target Statistics (TS):} Вычисляет статистики целевой переменной (например, среднее значение y) для каждой категории, но делает это хитро, используя "исторические" данные (только объекты, идущие до текущего в некоторой случайной перестановке), чтобы избежать утечки целевой переменной (target leakage) и переобучения.
                 \item Комбинации категориальных признаков генерируются автоматически.
                 \item Не требует предварительной обработки категорий (как OHE), что упрощает пайплайн и часто дает лучшее качество.
            \end{itemize}
        \item \textbf{Ordered Boosting:} Модификация градиентного бустинга, которая также борется с target leakage и prediction shift, обучая модель на остатках, полученных на данных, не включающих текущий объект (для TS).
        \item \textbf{Симметричные (Oblivious) Деревья:} Все узлы на одном уровне дерева используют одно и то же условие (признак и порог) для сплита. Это:
            \begin{itemize}[label=\textbullet, nosep, leftmargin=*]
                 \item Действует как неявная регуляризация.
                 \item Значительно ускоряет предсказание модели (особенно на CPU).
                 \item Упрощает структуру модели.
            \end{itemize}
        \item \textbf{Меньше Настройки Гиперпараметров:} Часто показывает хорошие результаты "из коробки" с параметрами по умолчанию.
        \item \textbf{Хорошая Визуализация:} Встроенные инструменты для анализа модели и процесса обучения.
    \end{itemize}
    \textit{Позиционирование:} Идеален для задач с большим количеством категориальных признаков. Часто дает высокое качество с минимальной настройкой и имеет очень быстрое время предсказания.
\end{myblock}

% ================================================
% Раздел 5: Важность Признаков (Feature Importance)
% ================================================
\section{Важность Признаков (Feature Importance) в GBM}

\begin{myexampleblock}{Методы Оценки Важности}
    GBM, как и другие ансамбли деревьев, позволяет оценить вклад каждого признака в итоговое предсказание. Основные методы:
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Gain (Прирост / Feature Importance):} Среднее уменьшение функции потерь (или другого критерия, например, Gini impurity/Variance reduction) при использовании признака для сплита во всех деревьях ансамбля. Суммарный gain по всем сплитам признака делится на общее число сплитов по этому признаку (или просто суммируется, зависит от реализации). \textbf{Считается наиболее надежным методом.}
        \item \textbf{Split Count / Frequency (Частота Использования / Weight):} Просто подсчитывает, сколько раз признак был выбран для разделения узла во всех деревьях. Простой метод, но не учитывает, насколько *полезным* был каждый сплит. Может переоценивать важность числовых признаков с большим количеством потенциальных порогов.
        \item \textbf{Coverage (Покрытие):} Среднее количество объектов в обучающей выборке, которые проходят через сплиты по данному признаку (иногда взвешенное по gain или другим метрикам). Показывает, какую долю данных "затрагивает" признак. Предоставляется не всеми библиотеками (есть в XGBoost, LightGBM).
    \end{itemize}
\end{myexampleblock}

\begin{textbox}{Использование Информации о Важности Признаков}
    Знание важности признаков полезно для:
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Интерпретации модели:} Понять, какие факторы наиболее сильно влияют на предсказание (хотя GBM остается моделью "черного ящика" по сравнению с линейными моделями).
        \item \textbf{Отбора признаков (Feature Selection):} Исключить неважные или малозначимые признаки, что может упростить модель, ускорить обучение/предсказание и иногда даже улучшить качество за счет уменьшения шума.
        \item \textbf{Генерации новых признаков (Feature Engineering):} Сосредоточить усилия на создании признаков на основе наиболее важных существующих.
    \end{itemize}
\end{textbox}

\begin{alerttextbox}{Предостережение}
    Следует с осторожностью относиться к результатам оценки важности:
    \begin{itemize}[nosep, leftmargin=*]
        \item Методы (особенно 'Split Count') могут быть \textbf{смещены} в сторону числовых признаков с большим количеством уникальных значений (high cardinality numerical features) или категориальных признаков с большим числом категорий (если используется OHE или простые методы кодирования), так как у них больше потенциальных точек для сплита. CatBoost с его обработкой категорий менее подвержен этой проблеме для категориальных данных.
        \item \textbf{Коррелирующие признаки:} Если два признака сильно коррелируют и оба полезны, модель может использовать для сплитов то один, то другой. В результате их важность может быть "размазана" между ними, и каждый по отдельности будет выглядеть менее важным, чем он есть на самом деле.
    \end{itemize}
\end{alerttextbox}

% --- Конец контента ---