% >>> Контент для шпаргалки по Градиентному Бустингу (GBM)

% --- Введение ---
\begin{myblock}{Что такое Градиентный Бустинг?}
    \textbf{Градиентный Бустинг (Gradient Boosting Machine, GBM)} — это мощный ансамблевый метод машинного обучения, который строит модели \textbf{последовательно}, где каждая новая модель исправляет ошибки предыдущей. Это один из самых эффективных алгоритмов для табличных данных.

    \textit{Аналогия:} Представь, что ты лепишь скульптуру. Вместо того чтобы сразу создать шедевр (что сложно), ты сначала делаешь грубую основу, потом видишь недочеты и добавляешь кусочек глины там, где нужно, потом еще и еще, пока скульптура не станет идеальной. Каждое добавление "кусочка глины" — это новая слабая модель в бустинге.
\end{myblock}

% --- Раздел 1 ---
\section{Идея Бустинга (Boosting)}

\begin{textbox}{Последовательное Исправление Ошибок}
    В отличие от \textbf{бэггинга} (как в Random Forest), где модели обучаются независимо и параллельно, в \textbf{бустинге} модели строятся одна за другой:
    \begin{enumerate}
        \item Обучается первая (обычно простая) модель на исходных данных.
        \item Вычисляются ошибки (остатки) этой модели.
        \item Следующая модель обучается предсказывать эти ошибки (или что-то, связанное с ними).
        \item Предсказания новой модели добавляются к предсказаниям ансамбля (с некоторым весом), чтобы уменьшить общую ошибку.
        \item Шаги 2-4 повторяются много раз, пока ошибка не перестанет уменьшаться или не будет достигнуто заданное число моделей.
    \end{enumerate}
    Ключевая идея: ансамбль "учится" на своих ошибках, постепенно улучшая предсказание. Каждая следующая модель фокусируется на тех данных, где предыдущие модели ошибались больше всего.
\end{textbox}

% --- Раздел 2 ---
\section{Градиентный Спуск на Функциях}

\begin{myblock}{Как Бустинг "Учится" Ошибкам?}
    GBM использует идею \textbf{градиентного спуска}, но не в пространстве параметров (как в нейросетях), а в \textbf{пространстве функций}.
    \begin{itemize}
        \item Мы хотим минимизировать некоторую \textbf{функцию потерь (Loss Function)}, $L(y, F(x))$, где $y$ - истинное значение, $F(x)$ - текущее предсказание ансамбля.
        \item \textbf{Инициализация:} Начальное предсказание $F_0(x)$ обычно простое: константа (например, среднее значение $y$ для регрессии или логарифм шансов для классификации). % <<< ПРАВКА: Добавлено про инициализацию
        \item На каждом шаге $m$ мы вычисляем \textbf{псевдо-остатки} (pseudo-residuals) — это \textbf{отрицательный градиент} функции потерь по предсказанию ансамбля на предыдущем шаге ($F_{m-1}(x)$):
          \[ r_{im} = - \left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F(x) = F_{m-1}(x)} \]
          где $i$ - номер объекта.
        \item Новая слабая модель $h_m(x)$ (обычно дерево решений) обучается предсказывать эти псевдо-остатки $r_{im}$.
        \item Ансамбль обновляется: $F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)$, где $\nu$ (nu) — это \textbf{темп обучения (learning rate)}, маленький коэффициент (например, 0.01-0.1), который уменьшает вклад каждой новой модели и делает обучение более робастным.
    \end{itemize}
    \textit{Аналогия:} Представь, что ты стоишь на холме (твоя текущая ошибка) и хочешь спуститься вниз (минимизировать ошибку). Градиент показывает направление *самого крутого подъема*. Ты делаешь шаг в \textbf{противоположном} направлении (анти-градиент) — это и есть обучение новой модели на псевдо-остатках. Learning rate — это размер твоего шага. Маленькие шаги помогают не проскочить минимум.
\end{myblock}

% --- Раздел 3 ---
\section{Основные Функции Потерь (Loss Functions)}

\begin{textbox}{Выбор Функции Потерь}
    Выбор функции потерь зависит от задачи:
    \begin{itemize}
        \item \textbf{Регрессия:}
            \begin{itemize}
                \item \textbf{MSE (Mean Squared Error):} $L(y, F) = \frac{1}{2}(y - F)^2$. Псевдо-остатки — это просто обычные остатки $(y - F)$. Чувствительна к выбросам.
                \item \textbf{MAE (Mean Absolute Error):} $L(y, F) = |y - F|$. Псевдо-остатки — $\text{sign}(y - F)$. Менее чувствительна к выбросам.
                \item \textbf{Huber Loss:} Комбинация MSE и MAE, робастна к выбросам.
            \end{itemize}
        \item \textbf{Бинарная Классификация:}
            \begin{itemize}
                \item \textbf{LogLoss (Логистическая функция потерь / Бинарная Кросс-Энтропия):} $L(y, F) = y \log(1 + e^{-F}) + (1-y) \log(1 + e^{F})$ (для $y \in \{0, 1\}$). Стандартный выбор. Предсказание $F$ здесь — это логит вероятности.
                \item \textbf{Exponential Loss (Экспоненциальная):} $L(y, F) = e^{-yF}$ (для $y \in \{-1, 1\}$). Используется в классическом AdaBoost. Более агрессивно наказывает за ошибки.
            \end{itemize}
        \item \textbf{Многоклассовая Классификация:} Обычно используется \textbf{Multinomial LogLoss}.
    \end{itemize}
\end{textbox}

% --- Раздел 4 ---
\section{Популярные Библиотеки: XGBoost, LightGBM, CatBoost}

\begin{alerttextbox}{Ключевые Отличия и Фишки (Концептуально)}
    Хотя базовый GBM существует, на практике почти всегда используют его продвинутые реализации. Вот их главные "фишки":
    \begin{itemize}
        \item \textbf{XGBoost (eXtreme Gradient Boosting):}
            \begin{itemize}
                \item \textbf{Регуляризация:} Включает L1 и L2 регуляризацию на веса листьев деревьев, что помогает бороться с переобучением.
                \item \textbf{Обработка пропусков:} Встроенный механизм для работы с NaN (учится, в какую ветку направлять NaN при сплите).
                \item \textbf{Оптимизации скорости:} Параллельные вычисления, приближенные алгоритмы поиска сплитов (quantile approximation), кэширование градиентов.
                \item \textit{Фишка:} Первым предложил многие из этих улучшений, стал "золотым стандартом".
            \end{itemize}
        \item \textbf{LightGBM (Light Gradient Boosting Machine):}
            \begin{itemize}
                \item \textbf{Скорость и память:} Часто быстрее XGBoost и потребляет меньше памяти.
                \item \textbf{Leaf-wise рост деревьев:} Строит дерево не по уровням (level-wise), а выбирая лист, который даст наибольшее уменьшение ошибки (leaf-wise). Это эффективнее, но может привести к переобучению на малых данных.
                \item \textbf{GOSS (Gradient-based One-Side Sampling):} Сохраняет объекты с большими градиентами (те, на которых модель сильно ошибается) и случайно отбрасывает часть объектов с малыми градиентами для ускорения обучения.
                \item \textbf{EFB (Exclusive Feature Bundling):} Объединяет взаимоисключающие признаки (те, что редко одновременно ненулевые, как one-hot encoding) для уменьшения размерности.
                \item \textit{Фишка:} Скорость и эффективность на больших датасетах.
            \end{itemize}
        \item \textbf{CatBoost (Categorical Boosting):}
            \begin{itemize}
                \item \textbf{Обработка категориальных признаков:} Главная фишка! Использует продвинутые методы (Ordered Target Statistics) для кодирования категорий "на лету" без предварительной обработки вроде One-Hot Encoding, что часто дает лучший результат и предотвращает переобучение.
                \item \textbf{Симметричные деревья (Oblivious Trees):} Все узлы на одном уровне дерева используют одно и то же условие для сплита. Это ускоряет предсказание и действует как регуляризация.
                \item \textbf{Меньше тюнинга:} Часто дает хорошие результаты с параметрами по умолчанию.
                \item \textbf{Ordered Boosting:} Вариация бустинга, помогающая бороться со сдвигом предсказаний (prediction shift) из-за использования целевой переменной при кодировании категорий.
                \item \textit{Фишка:} Лучшая (из коробки) работа с категориальными данными и быстрая скорость предсказания. % <<< ПРАВКА: Добавлено про скорость предсказания
            \end{itemize}
    \end{itemize}
\end{alerttextbox}

% --- Раздел 5 ---
\section{Важность Признаков (Feature Importance)}

\begin{myexampleblock}{Важность Признаков}
    Градиентный бустинг, как и другие древовидные модели, позволяет оценить важность признаков:
    \begin{itemize}
        \item \textbf{Gain (Прирост):} Среднее уменьшение ошибки (loss), которое дает сплит по данному признаку во всех деревьях ансамбля. Чем больше признак уменьшает ошибку, тем он важнее. **Часто считается наиболее информативным методом.**
        \item \textbf{Split Count / Frequency (Частота Использования):} Сколько раз признак использовался для сплита во всех деревьях. Проще, но менее точно, так как не учитывает, насколько *полезным* был сплит.
        \item \textbf{Coverage (Покрытие):} Среднее количество объектов, проходящих через сплиты по данному признаку (взвешенное по уровню ошибки). Не все библиотеки предоставляют.
    \end{itemize}
    Знание важности признаков помогает:
    \begin{itemize}
        \item Понять, на какие данные модель опирается больше всего.
        \item Провести отбор признаков (Feature Selection).
        \item Интерпретировать модель (хотя GBM все еще сложнее интерпретировать, чем линейные модели).
    \end{itemize}
    \vspace{1ex} % Небольшой отступ перед предостережением
    \textit{Предостережение:} Важно помнить, что методы оценки важности признаков (особенно 'Split Count') могут быть смещены в сторону признаков с большим количеством уникальных значений (high cardinality numerical features) или категориальных признаков с большим числом категорий (если они не обработаны CatBoost-ом). % <<< ПРАВКА: Добавлено предостережение
\end{myexampleblock}

% --- Конец контента ---