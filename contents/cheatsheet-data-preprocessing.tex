% >>> Контент для шпаргалки по Data Preprocessing и Feature Engineering

% --- Раздел 1 ---
\section{Обработка пропусков (Missing Values)}

\begin{myblock}{Почему пропуски - это проблема?}
    Многие ML-алгоритмы не могут работать с пропущенными значениями (\texttt{NaN}, \texttt{None}). Пропуски могут исказить результаты анализа и снизить производительность модели.
    \begin{itemize}
        \item \textbf{Причины пропусков:} Ошибки ввода, сбои сенсоров, отказ пользователя отвечать, данные недоступны.
    \end{itemize}
\end{myblock}

\begin{textbox}{Стратегии обработки пропусков}
    Выбор стратегии зависит от количества пропусков, типа данных и специфики задачи. (Указанные проценты - это эвристики, а не строгие правила).
    \begin{enumerate}
        \item \textbf{Удаление (Deletion):}
            \begin{itemize}
                \item \textbf{Удаление строк (Listwise deletion):} Удаляем весь объект (строку), если в нем есть хотя бы один пропуск.
                    \textit{Когда?} Мало пропусков (< 5-10\%), данные пропущены случайно (\textbf{MCAR} - Missing Completely At Random, т.е. сам факт пропуска не зависит ни от других признаков, ни от самого пропущенного значения), большой датасет.
                    \textit{Минус:} Потеря данных.
                \item \textbf{Удаление столбцов (Dropping features):} Удаляем весь признак (столбец), если в нем слишком много пропусков (> 50-70\%).
                    \textit{Когда?} Признак не очень важен, или пропусков слишком много для заполнения.
                    \textit{Минус:} Потеря потенциально полезной информации.
            \end{itemize}
        \item \textbf{Заполнение (Imputation):} Заменяем пропуски оценочными значениями.
            \begin{itemize}
                \item \textbf{Простые методы:}
                    \begin{itemize}
                        \item \textbf{Среднее (Mean):} Для числовых признаков без сильных выбросов. Чувствительно к выбросам.
                        \item \textbf{Медиана (Median):} Для числовых признаков с выбросами. Более робастно.
                        \item \textbf{Мода (Mode):} Для категориальных признаков.
                    \end{itemize}
                \item \textbf{Более сложные методы (кратко):}
                    \begin{itemize}
                        \item Заполнение с помощью \textbf{k-ближайших соседей (k-NN Imputer):} Использует значения соседей для оценки пропуска.
                        \item Заполнение с помощью \textbf{моделей (Regression Imputation):} Предсказывает пропуск с помощью регрессии на основе других признаков.
                        \item Создание \textbf{индикаторного признака:} Добавляем новый бинарный столбец, указывающий, было ли значение пропущено (1 - да, 0 - нет), а сам пропуск заполняем (например, нулем или средним). Позволяет модели учесть сам факт пропуска, \textbf{так как иногда сам факт отсутствия данных несет полезную информацию}.
                    \end{itemize}
            \end{itemize}
            \textit{Плюс:} Сохраняет данные. \textit{Минус:} Может внести смещение (bias) в данные.
    \end{enumerate}
\end{textbox}

\begin{myexampleblock}{Аналогия: Детектив и недостающие улики}
    Представьте, что вы детектив, расследующий дело.
    \begin{itemize}
        \item \textbf{Удаление строки:} Если по свидетелю совсем нет информации (много пропусков), вы можете решить его не учитывать (удалить строку). Риск: потеряете ключевого свидетеля.
        \item \textbf{Удаление столбца:} Если какой-то тип улик (например, отпечатки пальцев) не удалось собрать почти нигде на месте преступления (много пропусков в столбце), вы можете перестать на него полагаться (удалить столбец). Риск: упустите важный тип улик.
        \item \textbf{Заполнение (Imputation):} Если у вас нет точного времени события, вы можете предположить его на основе других фактов: среднее время похожих преступлений (\textbf{mean}), наиболее вероятное время (\textbf{mode}), или предсказать его с помощью сложной модели (\textbf{k-NN/Regression}). Риск: ваше предположение может быть неверным.
        \item \textbf{Индикаторный признак:} Вы отмечаете в деле, что точное время \textit{неизвестно} (индикатор=1), но записываете предполагаемое время (например, медиану). Теперь факт неизвестности времени - это тоже улика!
    \end{itemize}
\end{myexampleblock}

% --- Раздел 2 ---
\section{Кодирование категориальных признаков}

\begin{myblock}{Зачем кодировать?}
    Большинство ML-моделей понимают только числа. Категориальные признаки (например, "цвет": "красный", "синий"; "город": "Москва", "СПб") нужно преобразовать в числовой формат.
\end{myblock}

\begin{textbox}{One-Hot Encoding (OHE) vs Label Encoding}
    Два самых популярных метода:

    \textbf{1. One-Hot Encoding (OHE):}
    \begin{itemize}
        \item \textbf{Как работает:} Создает новый бинарный (0 или 1) столбец для каждой уникальной категории признака. В каждой строке только один из этих новых столбцов равен 1, остальные — 0.
        \item \textbf{Когда использовать:} Для \textbf{номинальных} признаков (где нет естественного порядка категорий, например, "цвет", "страна"). Подходит для большинства моделей, особенно линейных, SVM, нейронных сетей.
        \item \textbf{Плюсы:} Не вносит ложного порядка.
        \item \textbf{Минусы:} Сильно увеличивает количество признаков ("проклятие размерности"), если категорий много. Может привести к мультиколлинеарности (часто удаляют один из OHE-столбцов — `drop='first'`).
    \end{itemize}

    \textbf{2. Label Encoding (LE):}
    \begin{itemize}
        \item \textbf{Как работает:} Присваивает каждой уникальной категории целое число (0, 1, 2, ...).
        \item \textbf{Когда использовать:}
            \begin{itemize}
                \item Для \textbf{порядковых} признаков (где есть естественный порядок, например, "размер": "S" < "M" < "L" -> 0, 1, 2).
                \item Иногда для \textbf{древовидных моделей} (Решающее дерево, Случайный лес, Градиентный бустинг), так как они могут разбивать признак по значениям (например, `< 1.5`). \textit{Но будьте осторожны: OHE часто все равно дает лучший результат даже для деревьев.}
            \end{itemize}
        \item \textbf{Плюсы:} Не увеличивает количество признаков.
        \item \textbf{Минусы:} Вносит \textbf{ложный порядок} для номинальных признаков (например, "Москва" = 0, "СПб" = 1, "Казань" = 2 подразумевает, что "СПб" "больше" "Москвы", что неверно). Это может запутать модели, чувствительные к значениям (линейные, SVM).
    \end{itemize}

    \textbf{Итог (Частый вопрос на собеседовании):}
    \begin{itemize}
        \item Есть порядок (ordinal)? $\rightarrow$ \textbf{Label Encoding}.
        \item Нет порядка (nominal)?
            \begin{itemize}
                \item Модель - дерево? $\rightarrow$ Можно попробовать \textbf{Label Encoding} (но OHE часто лучше).
                \item Модель - НЕ дерево (линейная, SVM, NN)? $\rightarrow$ \textbf{One-Hot Encoding}.
                \item Очень много категорий? $\rightarrow$ Рассмотреть другие методы (Target Encoding, Embedding) или Feature Engineering.
            \end{itemize}
    \end{itemize}
\end{textbox}

\begin{myexampleblock}{Аналогия: Маркировка одежды на складе}
    \begin{itemize}
        \item \textbf{Label Encoding (для порядковых):} Размеры футболок "S", "M", "L". Можно просто пронумеровать полки 0, 1, 2. Порядок сохранен, все понятно.
        \item \textbf{Label Encoding (для номинальных - ПЛОХО):} Цвета футболок "Красный", "Синий", "Зеленый". Если пронумеровать полки 0, 1, 2, то это подразумевает, что "Синий" (1) чем-то "больше" "Красного" (0), что бессмысленно. Алгоритм может сделать неверные выводы.
        \item \textbf{One-Hot Encoding (для номинальных):} Для цветов "Красный", "Синий", "Зеленый" создаем три отдельные секции на складе. Если футболка красная, кладем ее в секцию "Красный" (1), а в секциях "Синий" и "Зеленый" ее нет (0). Порядка нет, но признаков (секций) стало больше.
    \end{itemize}
\end{myexampleblock}

% --- Раздел 3 ---
\section{Масштабирование признаков (Feature Scaling)}

\begin{myblock}{Зачем масштабировать?}
    Если числовые признаки имеют разные диапазоны значений (например, возраст [0-100] и зарплата [10k-1M]), то модели, использующие \textbf{меры расстояния} (KNN, SVM, K-Means) или \textbf{градиентный спуск} (Линейная/Логистическая регрессия, Нейронные сети), могут придать неоправданно больший "вес" признакам с бОльшими значениями. Масштабирование приводит все признаки к сопоставимому диапазону.
\end{myblock}

\begin{textbox}{Нормализация vs Стандартизация}
    Два основных метода:

    \textbf{1. Нормализация (Normalization / Min-Max Scaling):}
    \begin{itemize}
        \item \textbf{Как работает:} Сжимает данные в диапазон [0, 1] (или [-1, 1]).
        \item \textbf{Формула:} \[ X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}} \]
        \item \textbf{Когда использовать:} Когда нужен строгий диапазон [0, 1] (например, для обработки изображений, где пиксели 0-255). Если данные не имеют гауссова распределения. Когда известно, что выбросов мало или они обработаны.
        \item \textbf{Минусы:} Чувствительна к \textbf{выбросам}, так как $X_{min}$ и $X_{max}$ могут сильно сдвинуться из-за одного аномального значения.
    \end{itemize}

    \textbf{2. Стандартизация (Standardization / Z-score Scaling):}
    \begin{itemize}
        \item \textbf{Как работает:} Преобразует данные так, чтобы они имели среднее значение 0 и стандартное отклонение 1.
        \item \textbf{Формула:} \[ X_{std} = \frac{X - \mu}{\sigma} \] где $\mu$ - среднее, $\sigma$ - стандартное отклонение.
        \item \textbf{Когда использовать:} \textbf{Чаще всего рекомендуется}. Особенно для алгоритмов, которые предполагают нормальное (гауссово) распределение данных (хотя работает и для других распределений). Менее чувствительна к выбросам, чем нормализация. Подходит для PCA, линейных моделей, SVM, KNN.
        \item \textbf{Минусы:} Не приводит данные к строгому диапазону (значения могут быть > 1 или < -1).
    \end{itemize}

    \textbf{Когда масштабирование НЕ обязательно (или менее критично):}
    \begin{itemize}
        \item \textbf{Древовидные модели} (Решающее Дерево, Случайный Лес, Градиентный Бустинг): Они смотрят на пороги разбиения для каждого признака независимо, поэтому масштаб не так важен. \textit{(Но иногда масштабирование все же может немного улучшить сходимость/стабильность даже в деревьях, хотя и не так критично, как для других моделей).}
    \end{itemize}
\end{textbox}

\begin{myexampleblock}{Аналогия: Сравнение оценок из разных школ}
    Ученик А получил 90 баллов из 100 (шкала 0-100). Ученик Б получил 4 балла из 5 (шкала 0-5). Кто лучше?
    \begin{itemize}
        \item \textbf{Без масштабирования:} 90 > 4, кажется, что А лучше. Но это некорректно.
        \item \textbf{Нормализация (в шкалу [0, 1]):}
            Ученик А: (90 - 0) / (100 - 0) = 0.9
            Ученик Б: (4 - 0) / (5 - 0) = 0.8
            Теперь видно, что результат А немного выше.
        \item \textbf{Стандартизация (относительно среднего по школе):} Допустим, средний балл в школе А был 70 (std=10), а в школе Б - 3 (std=0.5).
            Ученик А: (90 - 70) / 10 = +2 сигмы (значительно выше среднего по своей школе).
            Ученик Б: (4 - 3) / 0.5 = +2 сигмы (тоже значительно выше среднего по своей школе).
            Стандартизация показала, что оба ученика выступили одинаково хорошо \textit{относительно своей группы}.
    \end{itemize}
\end{myexampleblock}

% --- Раздел 4 ---
\section{Feature Engineering (Очень Кратко)}

\begin{myblock}{Идея: Создание лучших признаков для модели}
    Это процесс использования знаний о данных и предметной области для создания признаков, которые делают работу ML-моделей более эффективной. Включает в себя как \textbf{создание} новых признаков, так и \textbf{отбор} или \textbf{извлечение} существующих. \textbf{Часто это самый важный шаг для улучшения модели!}
\end{myblock}

\begin{textbox}{Основные направления Feature Engineering}
    \textbf{1. Отбор признаков (Feature Selection):}
    \begin{itemize}
        \item \textbf{Идея:} Выбрать подмножество \textit{наиболее важных} исходных признаков.
        \item \textbf{Цель:} Уменьшить сложность, ускорить обучение, бороться с переобучением, улучшить интерпретируемость.
        \item \textbf{Примеры методов (названия):} Фильтры (корреляция, Хи-квадрат, ANOVA F-value), Обертки (Recursive Feature Elimination - RFE), Встроенные (Lasso-регрессия L1, Feature Importance из деревьев).
    \end{itemize}

    \textbf{2. Извлечение признаков (Feature Extraction):}
    \begin{itemize}
        \item \textbf{Идея:} Создать \textit{новые} признаки путем комбинации или трансформации исходных, часто с понижением размерности. Новые признаки обычно не имеют прямого физического смысла исходных.
        \item \textbf{Цель:} Уменьшить размерность данных, сохранив при этом максимум полезной информации, бороться с шумом.
        \item \textbf{Примеры методов (названия):} Метод главных компонент (\textbf{PCA} - Principal Component Analysis), Линейный дискриминантный анализ (\textbf{LDA}), разложение матриц (SVD).
    \end{itemize}

    \textbf{3. Создание признаков (Feature Creation):}
    \begin{itemize}
        \item \textbf{Идея:} Генерация новых признаков из существующих на основе знаний о предметной области или анализа данных.
        \item \textbf{Цель:} Добавить в модель полезную информацию, которой не было в исходных признаках в явном виде.
        \item \textbf{Примеры:}
            \begin{itemize}
                \item Полиномиальные признаки ($x_1^2$, $x_1 \times x_2$) - часто для линейных моделей.
                \item Взаимодействия признаков ($x_1 / x_2$, $x_1 + x_2$, разница между датами).
                \item Признаки из дат (день недели, месяц, час, является ли день праздником/выходным).
                \item Агрегированные признаки (например, для клиента: средняя сумма покупки за месяц, количество покупок за неделю).
                \item Биннинг (группировка) непрерывных признаков (например, возраст $\rightarrow$ возрастные группы: "молодой", "средний", "пожилой").
            \end{itemize}
    \end{itemize}
\end{textbox}

\begin{myexampleblock}{Аналогия: Подготовка к походу}
    \begin{itemize}
        \item \textbf{Feature Selection (Отбор):} У вас много снаряжения (признаков). Вы выбираете только самое необходимое для конкретного похода (самые важные признаки): нож, палатку, спальник. Отбрасываете ненужное (менее важные признаки): утюг, фен. Вы используете \textit{исходные} предметы.
        \item \textbf{Feature Extraction (Извлечение):} У вас есть много разных продуктов (исходные признаки). Вместо того чтобы нести их все, вы готовите из них концентрированную, калорийную походную еду (новые признаки), например, сублиматы. Эта еда содержит энергию из исходных продуктов, но сама по себе является чем-то новым и занимает меньше места (понижение размерности).
        \item \textbf{Feature Creation (Создание):} У вас есть орехи и сухофрукты (исходные признаки). Вы смешиваете их и делаете питательный энергетический батончик (новый признак). Батончик - это комбинация исходных, но он удобнее и дает энергию по-другому. Или вы смотрите на карту (данные) и понимаете, что нужно пересечь реку (знание о данных) - вы создаете признак "нужен ли плот" (новый признак).
    \end{itemize}
\end{myexampleblock}

% --- Конец контента ---