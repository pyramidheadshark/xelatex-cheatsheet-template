% >>> Контент для шпаргалки "Деревья Решений"

% --- Введение: Дерево - основа основ ---
\begin{textbox}{Деревья Решений: Базовый Блок Ансамблей}
    Дерево решений — это простой, но мощный алгоритм, который лежит в основе многих продвинутых методов, таких как Случайный Лес (Random Forest) и Градиентный Бустинг. Понимание деревьев — ключ к пониманию ансамблей.
    Представь его как блок-схему или игру в "20 вопросов", где на каждом шаге мы задаем вопрос о данных, чтобы прийти к финальному ответу (прогнозу).
\end{textbox}

% --- Раздел 1: Принцип Работы ---
\section{Как Дерево Принимает Решение?}

\begin{myblock}{Аналогия: Игра в "Угадай Животное"}
    Представь, что ты пытаешься угадать животное, задавая вопросы типа "У него есть перья?", "Он умеет летать?", "Он живет в воде?". Дерево решений работает похожим образом:
    \begin{itemize}
        \item \textbf{Узлы (Nodes):} Каждый узел — это вопрос (проверка условия) по одному из признаков (например, "Возраст > 30?").
        \item \textbf{Ветви (Edges/Branches):} Ответы на вопрос ("Да" / "Нет"), ведущие к следующему узлу.
        \item \textbf{Листья (Leaves):} Конечные узлы, где содержится прогноз (например, класс "Кликнет" / "Не кликнет" или среднее значение для регрессии).
    \end{itemize}
    Объект (например, пользователь) "проходит" по дереву от корня вниз, отвечая на вопросы в узлах, пока не достигнет листа. Прогноз в этом листе и будет результатом для данного объекта.
\end{myblock}

% --- Раздел 2: Как Дерево Строится? Критерии Разделения ---
\section{Выбор Лучшего Вопроса (Разделения)}

\begin{myblock}{Цель: Сделать Группы Чище}
    На каждом шаге дерево ищет \textbf{лучший вопрос} (признак и пороговое значение), который разделит текущие данные на две максимально "чистые" группы по целевой переменной.
    \begin{itemize}
        \item \textbf{Аналогия:} Представь, что у тебя корзина с яблоками и грушами. Хороший вопрос ("Это круглое?") поможет разделить их лучше, чем плохой ("Это тяжелее 100г?").
        \item \textbf{Чистота (Purity):} Группа считается чистой, если в ней преобладают объекты одного класса (в задаче классификации).
        \item \textbf{Меры Нечистоты (Impurity Measures):} Для оценки "качества" разделения используются специальные метрики. Чем ниже значение метрики после разделения, тем лучше. Основные:
            \begin{itemize}
                \item \textbf{Критерий Джини (Gini Impurity):} Измеряет вероятность того, что случайно выбранный элемент из набора будет неправильно классифицирован, если его класс случайно выбирается в соответствии с распределением классов в наборе. Формула: $G = 1 - \sum_{k=1}^{K} p_k^2$, где $p_k$ — доля объектов класса $k$. \textit{Интуиция: ниже Gini — чище узел.}
                \item \textbf{Энтропия (Entropy):} Мера хаоса или неопределенности в узле. Используется в алгоритмах ID3, C4.5. Формула: $E = - \sum_{k=1}^{K} p_k \log_2(p_k)$. \textit{Интуиция: ниже энтропия — меньше хаоса, чище узел.}
            \end{itemize}
        \item \textbf{Information Gain (Прирост информации):} Мера того, насколько разделение \textbf{уменьшает нечистоту} (измеренную с помощью Gini или Entropy). Рассчитывается как нечистота родителя минус средневзвешенная нечистота дочерних узлов. Дерево ищет разделение, которое дает \textbf{максимальный Information Gain}.
    \end{itemize}
    Процесс повторяется рекурсивно для каждого нового узла, пока не будет выполнен критерий остановки. В задачах \textbf{регрессии} вместо мер нечистоты используются критерии, основанные на уменьшении дисперсии (Variance Reduction), например, среднеквадратичная ошибка (MSE).
\end{myblock}

% --- Раздел 3: Главная Проблема - Переобучение ---
\section{Переобучение: Проклятие Деревьев}

\begin{alerttextbox}{Почему Деревья Легко Переобучаются?}
    Деревья по своей природе \textbf{очень гибкие} и могут строить очень сложные структуры. Если не ограничивать их рост, они будут продолжать делиться, пока в каждом листе не останется минимальное количество объектов (в идеале — один).
    \begin{itemize}
        \item \textbf{Аналогия:} Представь студента, который не выучил общие правила, а просто \textbf{зазубрил ответы} на все вопросы из тренировочного билета. На экзамене с новыми вопросами он провалится.
        \item \textbf{Результат:} Дерево идеально "подгоняется" под обучающие данные, запоминая даже шум и выбросы. Оно показывает отличные метрики на обучении, но \textbf{плохо обобщает} знания на новые, невиданные ранее данные (тестовый набор).
    \end{itemize}
    Такое поведение — классический пример \textbf{высокой дисперсии (high variance)} модели при потенциально низкой предвзятости (low bias) на обучающих данных.
\end{alerttextbox}

% --- Раздел 4: Борьба с Переобучением ---
\section{Методы Регуляризации (Ограничения Роста)}

\begin{myexampleblock}{Контроль Сложности Во Время Роста (Pre-pruning)}
    Чтобы дерево не "зубрило", а "учило", его рост ограничивают с помощью гиперпараметров (задаются ДО обучения):
    \begin{itemize}
        \item \texttt{max\_depth}: \textbf{Максимальная глубина дерева.} Ограничивает количество "вопросов" на пути от корня к листу. Меньшая глубина — проще дерево. \textit{Аналогия: Ограничить количество вопросов в игре "Угадай животное".}
        \item \texttt{min\_samples\_split}: \textbf{Минимальное количество объектов в узле}, необходимое для его дальнейшего разделения. Если объектов меньше — узел становится листом. Предотвращает деление на очень маленьких, возможно, шумовых группах. \textit{Аналогия: Не делить группу людей, если их меньше 5 человек.}
        \item \texttt{min\_samples\_leaf}: \textbf{Минимальное количество объектов в листовом узле.} Гарантирует, что каждый прогноз (лист) основан на достаточном количестве примеров. \textit{Аналогия: Каждый финальный ответ должен подтверждаться мнением как минимум 3 экспертов.}
        \item \texttt{max\_features}: Максимальное количество признаков, рассматриваемых при поиске лучшего разделения. Вносит случайность, что \textbf{уменьшает корреляцию} между деревьями в ансамблях (например, в Случайном Лесу).
    \end{itemize}
    Подбор оптимальных значений этих параметров — задача \textbf{кросс-валидации}.
\end{myexampleblock}

\begin{myblock}{Идея Прунинга (Pruning - "Подрезка")}
    Это метод \textbf{упрощения дерева после построения (Post-pruning)}. Прунинг применяется \textbf{ПОСЛЕ} того, как дерево уже построено (часто до максимальной глубины).
    \begin{itemize}
        \item \textbf{Идея:} Упростить дерево, удаляя ("срезая") ветви или узлы, которые вносят малый вклад в точность на валидационном наборе данных или слишком сильно усложняют модель.
        \item \textbf{Типы (основной):} \textbf{Cost Complexity Pruning (CCP)} / Pruning по минимальной стоимости-сложности. Ищет баланс между точностью и сложностью дерева. Параметр \texttt{ccp\_alpha} в scikit-learn контролирует степень прунинга. Значение $\alpha=0$ означает отсутствие прунинга, а увеличение $\alpha$ увеличивает "штраф" за сложность, приводя к более сильной подрезке.
        \item \textbf{Преимущество:} Иногда позволяет найти более оптимальную структуру, чем простое ограничение роста гиперпараметрами (pre-pruning).
    \end{itemize}
\end{myblock}

% --- Конец контента ---