% >>> Контент для шпаргалки 3: Линейные Модели

% --- Раздел III.A: Линейная Регрессия ---
\section{Линейная Регрессия (\texttt{Linear Regression})}

\begin{textbox}{Основная Идея}
Простейшая модель для предсказания \textbf{непрерывного} значения (например, цены дома, температуры). Мы пытаемся найти наилучшую прямую (или гиперплоскость в многомерном случае), которая описывает зависимость между признаками ($X$) и целевой переменной ($y$).

Модель: $y \approx \hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n = \mathbf{w}^T \mathbf{x}$
Где $\mathbf{w}$ - вектор весов (параметров) модели, включая свободный член $w_0$ (bias term), $\mathbf{x}$ - вектор признаков объекта (с добавленным $x_0=1$).
\textbf{Интерпретация весов:} Коэффициент $w_j$ показывает, на сколько в среднем изменится предсказание $\hat{y}$, если признак $x_j$ увеличить на 1, при условии, что все остальные признаки остаются неизменными.
\end{textbox}

\begin{myblock}{Функция Потерь: MSE (\texttt{Mean Squared Error})}
Чтобы понять, насколько хорошо наша прямая подходит к данным, мы измеряем ошибку. Самый частый способ - \textbf{Среднеквадратичная Ошибка (MSE)}. Мы суммируем квадраты разностей между реальными значениями ($y_i$) и предсказаниями модели ($\hat{y}_i$) и делим на количество примеров ($m$).
\[
MSE = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 = \frac{1}{m} \sum_{i=1}^{m} (y_i - \mathbf{w}^T \mathbf{x}_i)^2
\]
\textbf{Почему квадрат?} Он штрафует большие ошибки сильнее и делает функцию потерь дифференцируемой.
\end{myblock}

\begin{myexampleblock}{Обучение: Идея Градиентного Спуска (GD)}
Представьте, что MSE - это холмистая местность, а мы стоим где-то на склоне. Наша цель - найти самую низкую точку (минимум MSE). \textbf{Градиентный спуск (GD)} - это как катиться с горы:
\begin{itemize}
    \item Смотрим, в каком направлении уклон самый крутой (\textbf{градиент} $\nabla MSE$).
    \item Делаем небольшой шаг в \textbf{противоположном} направлении (анти-градиент). Длина шага контролируется \textbf{скоростью обучения} (\textit{learning rate}, $\alpha$).
    \item Повторяем, пока не дойдем до дна (или почти).
\end{itemize}
Формула обновления весов: $\mathbf{w} := \mathbf{w} - \alpha \nabla_{\mathbf{w}} MSE(\mathbf{w})$

\textbf{Варианты GD:}
\begin{itemize}
    \item \textbf{Batch GD}: Градиент считается по \textbf{всей} обучающей выборке на каждом шаге. Точно, но медленно на больших данных.
    \item \textbf{Stochastic GD (SGD)}: Градиент считается по \textbf{одному} случайно выбранному примеру на каждом шаге. Быстро, шумно (может "прыгать" вокруг минимума), хорошо для очень больших данных и онлайн-обучения.
    \item \textbf{Mini-batch GD}: Компромисс. Градиент считается по небольшой \textbf{случайной подвыборке} (\textit{batch}) на каждом шаге. Сочетает преимущества Batch и SGD. Самый популярный вариант.
\end{itemize}
\end{myexampleblock}

\begin{textbox}{Аналитическое Решение (Normal Equation)}
Для линейной регрессии с MSE существует \textbf{точное аналитическое решение}, позволяющее найти оптимальные веса $\mathbf{w}$ без итераций GD.
\[
\mathbf{w}^* = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
\]
Где $\mathbf{X}$ - матрица признаков (объекты по строкам, признаки по столбцам, с добавленным столбцом единиц), $\mathbf{y}$ - вектор целевых значений.

\textbf{Плюсы:} Точно, не требует подбора learning rate.
\textbf{Минусы:} Требует вычисления обратной матрицы $(\mathbf{X}^T \mathbf{X})^{-1}$, что вычислительно сложно ($O(n^3)$, где $n$ - число признаков) и может быть невозможно, если матрица вырождена (признаки линейно зависимы). Непрактично при очень большом числе признаков.
\end{textbox}

\begin{alerttextbox}{Основные Предположения Линейной Регрессии}
Модель работает лучше всего, когда выполняются некоторые предположения (на собеседовании важно знать хотя бы названия):
\begin{itemize}
    \item \textbf{Линейность:} Средняя зависимость $y$ от $X$ является линейной.
    \item \textbf{Независимость ошибок:} Ошибки предсказаний для разных наблюдений независимы.
    \item \textbf{Гомоскедастичность:} Разброс (варианция) ошибок одинаков для всех значений $X$. \textit{Аналогия: толщина "облака" точек вокруг линии регрессии примерно одинакова по всей длине.}
    \item \textbf{Нормальность ошибок:} Ошибки распределены нормально (важно для построения доверительных интервалов).
\end{itemize}
Нарушение предположений не всегда делает модель бесполезной, но может влиять на надежность выводов.
\end{alerttextbox}

\begin{myblock}{Полиномиальная Регрессия}
Что если зависимость нелинейная? Можно добавить \textbf{полиномиальные признаки} - степени существующих признаков ($x^2, x^3$) или их взаимодействия ($x_1 x_2$).
Пример: $y \approx w_0 + w_1 x + w_2 x^2$.
\textbf{Важно:} Модель все еще остается \textbf{линейной по параметрам} $\mathbf{w}$! Мы просто применяем линейную регрессию к расширенному набору признаков ($1, x, x^2$). Легко переобучается, требует регуляризации.
\end{myblock}

% --- Раздел III.C: Логистическая Регрессия ---
\section{Логистическая Регрессия (\texttt{Logistic Regression})}

\begin{textbox}{Основная Идея}
Используется для задач \textbf{бинарной классификации} (ответ 0 или 1, "да" или "нет"). Вместо прямого предсказания класса, она моделирует \textbf{вероятность} принадлежности объекта к классу 1.
\textit{Аналогия: предсказать не "сдал/не сдал" экзамен, а вероятность сдачи в зависимости от часов подготовки.}
\end{textbox}

\begin{myblock}{Сигмоида (Логистическая Функция)}
Линейная комбинация признаков ($\mathbf{w}^T \mathbf{x}$) может дать любое вещественное значение. Чтобы получить вероятность (от 0 до 1), результат пропускают через \textbf{сигмоидную функцию} $\sigma(z)$:
\[
\sigma(z) = \frac{1}{1 + e^{-z}} \quad \text{где } z = \mathbf{w}^T \mathbf{x}
\]
Предсказание модели: $\hat{p} = P(y=1 | \mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x})$.
Решение о классе принимается по порогу (обычно 0.5): если $\hat{p} \ge 0.5$, то класс 1, иначе класс 0.
\end{myblock}

\begin{myblock}{Функция Потерь: LogLoss (Логарифмическая Функция Потерь)}
MSE плохо подходит для вероятностей. Используется \textbf{LogLoss} (или Бинарная Кросс-Энтропия).
\[
LogLoss = -\frac{1}{m} \sum_{i=1}^{m} [ y_i \log(\hat{p}_i) + (1 - y_i) \log(1 - \hat{p}_i) ]
\]
\textbf{Идея:} Сильно штрафует модель, если она уверенно предсказывает неверный класс (например, дает $\hat{p}=0.99$, когда реальный класс $y=0$). Когда предсказание правильное, штраф маленький. Обучается также с помощью GD.
\end{myblock}

\begin{textbox}{Softmax (для Мультиклассовой Классификации)}
Если классов больше двух, используется обобщение логистической регрессии - \textbf{Softmax Regression}. Для каждого класса $k$ вычисляется своя "оценка" $z_k = \mathbf{w}_k^T \mathbf{x}$. Затем эти оценки преобразуются в вероятности с помощью \textbf{функции Softmax}:
\[
P(y=k | \mathbf{x}) = \hat{p}_k = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{для } k = 1, \dots, K
\]
Softmax гарантирует, что все $\hat{p}_k$ будут от 0 до 1 и их сумма будет равна 1. Функция потерь - \textbf{Cross-Entropy Loss}.
\end{textbox}

% --- Раздел III.B: Регуляризация ---
\section{Регуляризация L1 и L2}

\begin{alerttextbox}{Что и Зачем?}
\textbf{Регуляризация} - это техника борьбы с \textbf{переобучением} (\textit{overfitting}) линейных (и не только) моделей. Переобучение происходит, когда модель слишком сложная и "запоминает" обучающие данные вместо того, чтобы выучить общие закономерности.
\textbf{Идея:} Добавить к основной функции потерь (MSE или LogLoss) \textbf{штраф} за большие значения весов $\mathbf{w}$.
\textit{Аналогия: Мы не просто просим модель хорошо описать данные (минимизировать MSE/LogLoss), но и говорим ей: "Будь проще! Не усложняй без необходимости!" (штрафуем за большие веса).}
\[
J_{reg}(\mathbf{w}) = J_{original}(\mathbf{w}) + \lambda \cdot R(\mathbf{w})
\]
Где $J_{original}$ - MSE или LogLoss, $R(\mathbf{w})$ - регуляризационный член, $\lambda$ (\textit{lambda}) - \textbf{коэффициент регуляризации} (гиперпараметр), контролирующий силу штрафа.
\textbf{Важно:} Перед применением регуляризации признаки обычно \textbf{масштабируют} (стандартизируют или нормализуют), чтобы штраф не зависел от исходного масштаба признаков.
\end{alerttextbox}

\begin{myblock}{L2 Регуляризация (Ridge / Гребневая)}
Штраф пропорционален \textbf{сумме квадратов} весов.
\[
R_{L2}(\mathbf{w}) = \sum_{j=1}^{n} w_j^2
\]
\textbf{Примечание:} Свободный член $w_0$ обычно \textbf{не регуляризуют}, т.к. он отвечает за общее смещение модели, а не за ее сложность взаимодействия с признаками.
\textbf{Эффект:} Заставляет веса быть \textbf{маленькими}, но редко обнуляет их полностью. Уменьшает веса пропорционально их величине. Хорошо работает почти всегда.
\textit{Аналогия: Родитель, который немного урезает карманные расходы всем детям (весам), особенно тем, кто тратит больше.}
\end{myblock}

\begin{myblock}{L1 Регуляризация (Lasso)}
Штраф пропорционален \textbf{сумме модулей} весов.
\[
R_{L1}(\mathbf{w}) = \sum_{j=1}^{n} |w_j|
\]
\textbf{Примечание:} Свободный член $w_0$ обычно \textbf{не регуляризуют} по той же причине, что и в L2.
\textbf{Эффект:} Может \textbf{обнулять} некоторые веса, эффективно производя \textbf{отбор признаков} (\textit{feature selection}). Полезна, когда есть подозрение, что многие признаки неинформативны.
\textit{Аналогия: Строгий родитель, который лишает карманных денег (обнуляет вес) некоторых детей (признаков) за провинности, а остальным может тоже немного урезать.}
\end{myblock}

\begin{myexampleblock}{Связь с Bias-Variance Trade-off}
Регуляризация - это инструмент управления компромиссом между смещением (bias) и разбросом (variance):
\begin{itemize}
    \item \textbf{Без регуляризации ($\lambda=0$):} Модель может иметь низкое смещение (хорошо подходит к обучающим данным), но высокий разброс (сильно меняется при изменении данных, переобучается).
    \item \textbf{С регуляризацией ($\lambda > 0$):} Добавляя штраф, мы \textbf{увеличиваем смещение} (модель становится "проще", может хуже подходить к обучающим данным), но \textbf{уменьшаем разброс} (модель становится стабильнее, лучше обобщается на новые данные).
\end{itemize}
Подбор оптимального $\lambda$ (через кросс-валидацию) позволяет найти баланс.
\end{myexampleblock}

% --- Конец контента ---