% >>> Контент для шпаргалки "Cheatsheet 2: Оценка Моделей - Метрики и Валидация"

% --- Раздел 1 ---
\section{Метрики Оценки: Регрессия}

\begin{myblock}{Зачем нужны метрики?}
    Метрики — это численные показатели, позволяющие **объективно оценить качество** работы модели машинного обучения. Для задач регрессии (предсказание непрерывного значения, например, цены дома или температуры) используются свои метрики.
\end{myblock}

\begin{textbox}{Основные метрики регрессии}
    Пусть $y_i$ — истинное значение, а $\hat{y}_i$ — предсказанное моделью значение для $i$-го объекта, $n$ — количество объектов, $\bar{y}$ - среднее истинных значений.
    \begin{itemize}
        \item \textbf{MAE (Mean Absolute Error) / Средняя Абсолютная Ошибка}: Показывает среднее абсолютное отклонение предсказаний от факта. Легко интерпретируется в единицах целевой переменной. Менее чувствительна к выбросам, чем MSE.
            \[ MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| \]
        \item \textbf{MSE (Mean Squared Error) / Среднеквадратичная Ошибка}: Среднее квадратов отклонений. Сильнее штрафует за большие ошибки из-за возведения в квадрат. Используется в оптимизации многих моделей. Единицы измерения - квадрат исходных единиц.
            \[ MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \]
        \item \textbf{RMSE (Root Mean Squared Error) / Корень из Среднеквадратичной Ошибки}: Корень из MSE. Возвращает метрику к исходным единицам измерения, что упрощает интерпретацию. Как и MSE, чувствительна к выбросам.
            \[ RMSE = \sqrt{MSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} \]
        \item \textbf{R² (Коэффициент Детерминации)}: Показывает, какую долю дисперсии зависимой переменной объясняет модель по сравнению с простой моделью, всегда предсказывающей среднее. Значения от $(-\infty)$ до 1. Ближе к 1 — лучше. 0 — модель работает как среднее. Отрицательные значения — модель хуже среднего.
            \[ R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} \]
            \textit{Аналогия R²}: Представьте, что вы пытаетесь предсказать рост людей. Если вы всегда предсказываете средний рост (простая модель), R² будет 0. Если ваша модель идеально предсказывает рост каждого, R² = 1.
    \end{itemize}
\end{textbox}

% --- Раздел 2 ---
\section{Метрики Оценки: Классификация}

\begin{myblock}{Матрица ошибок (Confusion Matrix)}
    Основа для большинства метрик бинарной классификации. Показывает, сколько объектов какого класса и как были классифицированы.
    \begin{itemize}
        \item \textbf{TP (True Positive)}: Истинно положительные. Класс 1, предсказан как 1. (Нашли больного)
        \item \textbf{TN (True Negative)}: Истинно отрицательные. Класс 0, предсказан как 0. (Нашли здорового)
        \item \textbf{FP (False Positive)}: Ложно положительные. \textbf{Ошибка I рода}. Класс 0, предсказан как 1. (Здоровый признан больным)
        \item \textbf{FN (False Negative)}: Ложно отрицательные. \textbf{Ошибка II рода}. Класс 1, предсказан как 0. (Больной признан здоровым)
    \end{itemize}
    \vspace{1ex}
    \textbf{Матрица Ошибок:}
    \vspace{0.5ex}
    \begin{center} % Центрирование таблицы для лучшего вида
    \begin{tabular}{c|c|c}
         & \textbf{Предсказание: 1} & \textbf{Предсказание: 0} \\\hline
        \textbf{Реальность: 1} & TP & FN \\\hline
        \textbf{Реальность: 0} & FP & TN \\
    \end{tabular}
    \end{center}
    \vspace{1ex}
\end{myblock}

\begin{textbox}{Основные метрики классификации}
    \begin{itemize}
        \item \textbf{Accuracy (Доля правильных ответов)}: Общая доля верных предсказаний. \textbf{Плохо работает при дисбалансе классов!}
            \[ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} \]
            \textit{Аналогия}: Если 99\% писем - не спам, модель, всегда говорящая "не спам", будет иметь Accuracy 99%, но она бесполезна для поиска спама.
        \item \textbf{Precision (Точность)}: Какая доля объектов, названных моделью классом 1, действительно являются классом 1? Важна, когда цена FP высока (напр., отправка здорового на дорогую операцию).
            \[ \text{Precision} = \frac{TP}{TP + FP} \]
        \item \textbf{Recall (Полнота, Sensitivity, True Positive Rate - TPR)}: Какую долю объектов класса 1 модель смогла правильно найти? Важна, когда цена FN высока (напр., пропуск больного пациента или мошеннической транзакции).
            \[ \text{Recall} = \frac{TP}{TP + FN} \]
        \item \textbf{F1-мера (F1-Score)}: Гармоническое среднее Precision и Recall. Полезна, когда важен баланс между точностью и полнотой. Стремится к нулю, если хотя бы одна из метрик (Precision или Recall) близка к нулю.
            \[ F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2TP}{2TP + FP + FN} \]
            Можно использовать \textbf{$F_\beta$-меру} для придания большего веса Precision ($\beta < 1$) или Recall ($\beta > 1$).
        \item \textbf{Specificity (Специфичность, True Negative Rate - TNR)}: Какую долю объектов класса 0 модель верно определила?
             \[ \text{Specificity} = \frac{TN}{TN + FP} \]
        \item \textbf{False Positive Rate (FPR)}: Какую долю объектов класса 0 модель неверно назвала классом 1? $FPR = 1 - \text{Specificity}$.
            \[ \text{FPR} = \frac{FP}{TN + FP} \]
    \end{itemize}
    \textit{Аналогия Precision/Recall (Спам-фильтр)}:
    \begin{itemize}
        \item \textbf{Precision}: Из всех писем, что попали в папку "Спам", какая доля реально спам? (Не хотим терять важные письма - высокий Precision).
        \item \textbf{Recall}: Из всех реально спамовых писем, какая доля попала в папку "Спам"? (Хотим отловить как можно больше спама - высокий Recall).
    \end{itemize}
\end{textbox}

\begin{myexampleblock}{ROC AUC (Receiver Operating Characteristic Area Under Curve)}
    Показывает качество модели в задаче \textbf{ранжирования} классов, независимо от выбранного порога классификации.
    \begin{itemize}
        \item \textbf{ROC-кривая}: График зависимости \textbf{TPR (Recall)} от \textbf{FPR} при изменении порога классификации от 1 до 0.
        \item \textbf{AUC (Area Under Curve)}: Площадь под ROC-кривой. Варьируется от 0 до 1.
            \begin{itemize}
                \item AUC = 1: Идеальный классификатор.
                \item AUC = 0.5: Случайное угадывание (модель бесполезна, диагональная линия).
                \item AUC < 0.5: Модель работает хуже случайной (возможно, перепутаны метки классов).
            \end{itemize}
        \item \textbf{Интерпретация AUC}: Вероятность того, что случайно выбранный объект класса 1 получит от модели оценку выше (более высокую вероятность принадлежности к классу 1), чем случайно выбранный объект класса 0.
        \item \textbf{Преимущества}: Относительная устойчивость к дисбалансу классов (по сравнению с Accuracy). Позволяет сравнить модели в целом, без привязки к конкретному порогу.
    \end{itemize}
    \textit{Аналогия ROC AUC}: Представьте соревнование: модели нужно выстроить всех людей в ряд так, чтобы все "больные" (класс 1) оказались правее всех "здоровых" (класс 0). AUC показывает, насколько хорошо модель справляется с этой задачей ранжирования.

    \begin{center} % <<< ДОБАВЛЕН ГРАФИК ROC
    \begin{tikzpicture}
        \begin{axis}[roc curve style, title={Пример ROC-кривой}]
        % Пример данных для хорошей модели
        \addplot coordinates { (0,0) (0.1,0.5) (0.2,0.8) (0.4,0.9) (0.6,0.95) (0.8,0.98) (1,1) };
        \addlegendentry{Модель (AUC $\approx$ 0.9)}
        % Можно добавить кривую для слабой модели
        % \addplot coordinates { (0,0) (0.2,0.2) (0.4,0.4) (0.6,0.6) (0.8,0.8) (1,1) };
        % \addlegendentry{Модель 2 (AUC = 0.5)}
        \end{axis}
    \end{tikzpicture}
    \end{center}
\end{myexampleblock}

\begin{myexampleblock}{Precision-Recall AUC (PR AUC)}
    Альтернатива ROC AUC, особенно полезная при \textbf{сильном дисбалансе классов}, когда важнее всего найти объекты редкого положительного класса.
    \begin{itemize}
        \item \textbf{PR-кривая}: График зависимости \textbf{Precision} от \textbf{Recall (TPR)} при изменении порога классификации.
        \item \textbf{PR AUC}: Площадь под PR-кривой. Также от 0 до 1.
        \item \textbf{Почему при дисбалансе?}: ROC AUC может быть обманчиво высоким при дисбалансе, так как TN обычно много, и FPR остается низким. PR-кривая фокусируется на поиске редкого положительного класса (TP) и цене ошибок на нем (FP), что важнее при дисбалансе.
        \item \textbf{Baseline}: В отличие от ROC AUC (baseline 0.5), baseline для PR AUC зависит от доли положительного класса $P$ в выборке: $\text{baseline} \approx P/(P+N)$. Для сильно несбалансированной выборки baseline PR AUC близок к 0.
    \end{itemize}
    \textit{Аналогия PR AUC}: Представьте поиск иголок (класс 1) в стоге сена (все данные). PR-кривая показывает: при разной степени "старания" (меняем порог -> меняется Recall), насколько точны наши находки (Precision)? Насколько много мусора (FP) мы захватываем вместе с иголками?

    \begin{center} % <<< ДОБАВЛЕН ГРАФИК PR
    \begin{tikzpicture}
        \begin{axis}[pr curve style, title={Пример PR-кривой}]
        % Пример данных для PR кривой (часто не монотонная)
        \addplot coordinates { (0.05,0.95) (0.1,0.9) (0.2,0.85) (0.3,0.8) (0.4,0.7) (0.5,0.6) (0.6,0.5) (0.7,0.4) (0.8,0.3) (0.9,0.2) (1,0.1) };
        \addlegendentry{Модель}
        % Можно показать baseline, если известна доля позитивного класса (P_frac)
        % \addplot[dashed, color=gray] coordinates {(0, P_frac) (1, P_frac)};
        % \addlegendentry{Baseline (случ.)}
        \end{axis}
    \end{tikzpicture}
    \end{center}
\end{myexampleblock}

\begin{alerttextbox}{Выбор метрики}
    Выбор метрики \textbf{критически зависит от бизнес-задачи}!
    \begin{itemize}
        \item **Медицинская диагностика (опасная болезнь):** Важнее найти всех больных (высокий \textbf{Recall}), даже если будут ложные срабатывания (низкий Precision). Цена FN (пропустить больного) очень высока. Используем Recall, F-меру с $\beta > 1$, PR AUC.
        \item **Спам-фильтр:** Важнее не отправлять нужные письма в спам (высокий \textbf{Precision}), даже если часть спама просочится (не идеальный Recall). Цена FP (потерять важное письмо) высока. Используем Precision, F-меру с $\beta < 1$.
        \item **Предсказание кликов (реклама):** Часто интересует общая точность предсказания вероятности клика, могут использовать \textbf{LogLoss} или \textbf{ROC AUC}.
        \item **Сильный дисбаланс классов (поиск мошенников):** Accuracy бесполезна. Смотреть на \textbf{F1-меру}, \textbf{PR AUC}, матрицу ошибок, Precision, Recall.
    \end{itemize}
    Всегда обсуждайте с заказчиком или продакт-менеджером, \textbf{какая ошибка для них страшнее} и как модель будет использоваться!
\end{alerttextbox}

\begin{myblock}{Кратко: Online vs Offline метрики}
    \begin{itemize}
        \item \textbf{Offline метрики}: Рассчитываются на отложенной (исторической) выборке (например, на тестовом датасете). Это все метрики, рассмотренные выше (Accuracy, F1, AUC, MSE и т.д.). Позволяют оценить модель до выкатки в продакшен.
        \item \textbf{Online метрики}: Рассчитываются на реальных данных после внедрения модели в работающую систему. Это обычно \textbf{бизнес-метрики}: CTR (Click-Through Rate), конверсия в покупку, средний чек, время на сайте, отток клиентов и т.д. Оцениваются и сравниваются с помощью **A/B тестирования**.
    \end{itemize}
\end{myblock}

% --- Новый Раздел 3 (заменяет старый раздел про Валидацию) ---
\section{Статистическая Оценка Надежности}

\begin{alerttextbox}{Зачем нужна статистика в ML?}
В ML мы почти всегда работаем с \textbf{ограниченными выборками} данных. Любая метрика (AUC, F1, MSE), посчитанная на такой выборке, является лишь \textbf{оценкой} истинного значения, которое мы получили бы на всех возможных данных (генеральной совокупности). Эта оценка подвержена \textbf{случайной изменчивости} (sampling variability). Статистические методы помогают:
\begin{itemize}
    \item Оценить \textbf{разброс} и \textbf{неопределенность} наших данных и метрик.
    \item Понять, насколько \textbf{надежны} наши выводы (например, действительно ли модель Б лучше модели А, или разница случайна?).
    \item Принимать обоснованные решения на основе данных (например, при A/B тестировании).
\end{itemize}
\end{alerttextbox}

% --- Подраздел 3.1 ---
\subsection{Дисперсия (Variance)}

\begin{textbox}{Что такое Дисперсия?}
\textbf{Дисперсия} — это мера того, насколько сильно значения в наборе данных \textbf{разбросаны} относительно их среднего значения ($\bar{x}$).
\[
\text{Sample Variance (s²)} \approx \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2
\]
(Используем $n-1$ для несмещенной оценки дисперсии генеральной совокупности по выборке).
\textbf{Стандартное отклонение (Standard Deviation, SD, s)} — это корень из дисперсии ($s = \sqrt{s^2}$). Оно измеряется в тех же единицах, что и исходные данные, и его легче интерпретировать.

\textbf{Интуиция:}
\begin{itemize}
    \item \textbf{Низкая дисперсия/SD}: Данные сгруппированы близко к среднему.
    \item \textbf{Высокая дисперсия/SD}: Данные сильно разбросаны.
\end{itemize}

\textbf{Зачем в ML?}
\begin{itemize}
    \item Понимание изменчивости признаков и целевой переменной.
    \item \textbf{Variance} в "Bias-Variance Tradeoff": относится к чувствительности \textit{модели*} к изменениям в обучающих данных (высокая дисперсия модели = переобучение).
    \item Компонент для расчета \textbf{R²}.
    \item Оценка гомоскедастичности (постоянства дисперсии ошибок) в регрессии.
    \item Входной параметр для некоторых стат. тестов (t-test).
\end{itemize}
\end{textbox}

% --- Подраздел 3.2 ---
\subsection{Доверительные Интервалы (Confidence Intervals, CI)}

\begin{textbox}{Что такое Доверительный Интервал?}
Поскольку наша метрика (например, средний AUC по фолдам CV, средний чек в A/B тесте) посчитана по выборке, она не является точным истинным значением. \textbf{Доверительный интервал (CI)} — это диапазон значений, который с определенной долей уверенности (обычно 95\%) \textbf{содержит истинное значение} параметра генеральной совокупности.

\textbf{Формула (упрощенно для среднего):}
\[ CI = \text{Выборочная Оценка} \pm \text{Критическое Значение} \times \text{Стандартная Ошибка Оценки} \]
\begin{itemize}
    \item \textbf{Выборочная Оценка}: Среднее значение метрики по выборке/фолдам.
    \item \textbf{Критическое Значение}: Зависит от уровня доверия (e.g., 1.96 для 95\% CI при использовании z-статистики/нормального распределения, или значение из t-распределения для t-статистики).
    \item \textbf{Стандартная Ошибка (Standard Error, SE)}: Мера точности выборочной оценки (насколько сильно она может варьироваться от выборки к выборке). Обычно это $SE = \frac{\text{SD}}{\sqrt{n}}$, где SD - стандартное отклонение данных, n - размер выборки.
\end{itemize}

\textbf{Интерпретация 95\% CI [A, B]:}
"Мы на 95\% уверены, что истинное значение параметра (например, средний AUC) находится между A и B".
\textit{Более строго:} Если бы мы многократно повторяли наше исследование (брали новые выборки того же размера), то 95\% построенных таким образом доверительных интервалов содержали бы истинное значение параметра.

\textbf{Зачем в ML?}
\begin{itemize}
    \item Понять \textbf{точность и надежность} оценки метрики (широкий CI = большая неопределенность).
    \item Сравнить модели или группы в A/B тесте: Если мы строим CI для \textbf{разницы} между метриками (например, $\text{AUC}_B - \text{AUC}_A$), и этот интервал \textbf{не включает ноль} (например, [0.01, 0.05]), это говорит о статистически значимом различии между моделями/группами на выбранном уровне доверия.
\end{itemize}
\textit{Аналогия CI}: Вы ловите рыбу сетью (строите CI). Вы не знаете точно, где рыба (истинное значение). Но вы знаете, что ваша сеть (метод построения CI) достаточно хороша, чтобы в 95\% случаев поймать рыбу, если вы будете забрасывать ее снова и снова.
\end{textbox}

% --- Подраздел 3.3 ---
\subsection{Статистическая Значимость и p-value}

\begin{textbox}{Гипотезы и Значимость}
Статистическая значимость помогает определить, является ли наблюдаемый эффект (например, разница в метриках между моделями А и Б, или разница в конверсии между вариантами A и B в тесте) \textbf{реальным} или он мог возникнуть \textbf{случайно} из-за вариативности выборки.

\textbf{Процесс проверки гипотез:}
\begin{enumerate}
    \item \textbf{Формулируем Нулевую Гипотезу ($H_0$):} Гипотеза об \textit{отсутствии эффекта} или разницы (e.g., $H_0: \text{AUC}_A = \text{AUC}_B$, $H_0: \text{Конверсия}_A = \text{Конверсия}_B$). Это статус-кво, который мы пытаемся опровергнуть.
    \item \textbf{Формулируем Альтернативную Гипотезу ($H_1$ или $H_a$):} Гипотеза о \textit{наличии эффекта} (e.g., $H_1: \text{AUC}_A \neq \text{AUC}_B$ (двусторонняя), или $H_1: \text{AUC}_B > \text{AUC}_A$ (односторонняя)).
    \item \textbf{Выбираем Уровень Значимости ($\alpha$):} Порог для принятия решения. Обычно $\alpha = 0.05$ (5\%). Это максимальная вероятность \textbf{Ошибки I рода}, которую мы готовы допустить (т.е., отвергнуть $H_0$, когда она на самом деле верна - "ложная тревога").
    \item \textbf{Собираем данные и вычисляем Статистику Теста} (например, z-статистику, t-статистику). Это число, которое измеряет, насколько наши данные отклоняются от того, что ожидалось бы при $H_0$.
    \item \textbf{Вычисляем p-value.}
\end{enumerate}

\textbf{p-value (Уровень Значимости):}
Вероятность получить наблюдаемые данные (или еще более экстремальные результаты), \textbf{если предположить, что Нулевая Гипотеза ($H_0$) верна}.

\textbf{Интерпретация p-value:}
\begin{itemize}
    \item \textbf{p-value < $\alpha$}: Наблюдаемые данные \textit{очень маловероятны}, если $H_0$ верна. Мы \textbf{отвергаем $H_0$} в пользу $H_1$. Результат считается \textbf{статистически значимым} на уровне $\alpha$. (Есть основания полагать, что эффект реален).
    \item \textbf{p-value >= $\alpha$}: Наблюдаемые данные \textit{вполне совместимы} с $H_0$. Мы \textbf{не можем отвергнуть $H_0$}. Результат \textbf{не является статистически значимым}. (\textbf{Важно}: Это не доказывает, что $H_0$ верна!)
\end{itemize}
\textit{Аналогия p-value}: Суд над $H_0$ ("нет разницы"). p-value — сила улик против $H_0$. Если улик мало (p >= $\alpha$), $H_0$ "оправдывают" (не отвергают). Если улик много (p < $\alpha$), $H_0$ "осуждают" (отвергают).

\textbf{Ошибки:}
\begin{itemize}
    \item \textbf{Ошибка I рода ($\alpha$)}: Отвергнуть $H_0$, когда она верна (False Positive).
    \item \textbf{Ошибка II рода ($\beta$)}: Не отвергнуть $H_0$, когда она ложна (False Negative).
    \item \textbf{Мощность теста (Power = $1-\beta$)}: Вероятность правильно отвергнуть ложную $H_0$ (обнаружить реальный эффект).
\end{itemize}
\end{textbox}

% --- Подраздел 3.4 ---
\subsection{Статистические Тесты (z, t, Mann-Whitney)}

\begin{myblock}{Как выбрать тест?}
Выбор теста зависит от:
\begin{itemize}
    \item \textbf{Типа данных}: Непрерывные (средние), категориальные (доли/пропорции), порядковые (ранги).
    \item \textbf{Цели сравнения}: Сравнение с константой, сравнение двух групп, сравнение более двух групп.
    \item \textbf{Зависимости выборок}: Независимые (разные группы людей/объектов) или Зависимые/Парные (одни и те же объекты до/после, \textbf{результаты моделей на одних и тех же фолдах CV}).
    \item \textbf{Распределения данных}: Нормальное или нет.
    \item \textbf{Знания о дисперсии}: Известна или нет (почти всегда неизвестна).
    \item \textbf{Размера выборки}.
\end{itemize}
\end{myblock}

\begin{myexampleblock}{Z-тест}
\begin{itemize}
    \item \textbf{Что проверяет}: Разницу между средними или долями (пропорциями).
    \item \textbf{Ключевые предположения}:
        \begin{itemize}
            \item Дисперсия генеральной совокупности \textbf{известна} (очень редко).
            \item ИЛИ размер выборки \textbf{очень большой} ($n > 30..50$), что позволяет использовать ЦПТ (Центральную Предельную Теорему) и считать выборочное среднее/долю нормально распределенными.
        \end{itemize}
    \item \textbf{Когда использовать в ML?}:
        \begin{itemize}
            \item \textbf{Сравнение долей (конверсий, CTR) в A/B тестах} при больших размерах выборок (тысячи пользователей).
        \end{itemize}
\end{itemize}
\end{myexampleblock}

\begin{myexampleblock}{T-тест}
\begin{itemize}
    \item \textbf{Что проверяет}: Разницу между \textbf{средними} значениями.
    \item \textbf{Ключевые предположения}:
        \begin{itemize}
            \item Дисперсия генеральной совокупности \textbf{неизвестна} (оценивается по выборке).
            \item Данные (или разности для парного теста) должны быть примерно \textbf{нормально распределены}. Тест довольно устойчив к небольшим отклонениям от нормальности при умеренных/больших выборках ($n > 15..30$).
            \item Для независимого t-теста: гомоскедастичность (равенство дисперсий в группах). Если нет - используется \textbf{t-тест Уэлча (Welch's t-test)}, который не требует равенства дисперсий (часто используется по умолчанию в ПО).
        \end{itemize}
    \item \textbf{Виды и Когда использовать в ML?}:
        \begin{itemize}
            \item \textbf{Одновыборочный (One-sample t-test)}: Сравнение среднего выборки с известным значением (редко в ML).
            \item \textbf{Независимый (Independent two-sample t-test)}: Сравнение средних двух \textbf{независимых} групп (например, средний чек в контрольной и тестовой группах A/B теста, если данные примерно нормальны и выборки не гигантские).
            \item \textbf{Парный (Paired t-test)}: Сравнение средних двух \textbf{зависимых} (парных) измерений. \textbf{КРИТИЧЕСКИ ВАЖНО ДЛЯ ML}:
                \begin{itemize}
                    \item Сравнение метрик (например, AUC, F1) двух моделей, посчитанных на \textbf{одних и тех же фолдах кросс-валидации}. Мы смотрим на попарные разности метрик на каждом фолде и проверяем, значимо ли среднее этой разности отличается от нуля.
                    \item Оценка эффекта "до/после" на одних и тех же пользователях/объектах.
                \end{itemize}
        \end{itemize}
\end{itemize}
\end{myexampleblock}

\begin{myexampleblock}{Тест Манна-Уитни (Mann-Whitney U test / Wilcoxon Rank-Sum test)}
\begin{itemize}
    \item \textbf{Что проверяет}: Разницу между \textbf{распределениями} двух \textbf{независимых} выборок. Часто интерпретируется как тест на различие \textbf{медиан}. Является \textbf{непараметрическим} аналогом независимого t-теста.
    \item \textbf{Ключевые предположения}:
        \begin{itemize}
            \item Выборки независимы.
            \item Данные как минимум \textbf{порядковые} (ordinal) или непрерывные.
            \item \textbf{НЕ требует нормальности распределения!}
            \item Для интерпретации как теста на медианы, предполагается, что формы распределений в группах схожи.
        \end{itemize}
    \item \textbf{Когда использовать в ML?}:
        \begin{itemize}
            \item Сравнение двух \textbf{независимых} групп в A/B тесте, когда данные \textbf{сильно ненормальны} (например, время на сайте, доход пользователя, количество покупок - часто имеют выбросы и скошенность) или являются порядковыми (оценки 1-5 звезд).
            \item Когда предположение о нормальности для t-теста явно нарушено.
        \end{itemize}
    \item \textbf{Примечание}: Для \textbf{парных} данных, когда нарушена нормальность, используется \textbf{Тест Уилкоксона для связанных выборок (Wilcoxon signed-rank test)}.
\end{itemize}
\end{myexampleblock}

\begin{myblock}{Итог по статистике}
Понимание дисперсии, доверительных интервалов и методов проверки статистической значимости (p-value, тесты) абсолютно необходимо для корректной интерпретации результатов работы ML моделей, сравнения их между собой и оценки влияния изменений в A/B тестах. Выбор правильного инструмента зависит от данных и задачи.
\end{myblock}

\begin{myblock}{{Кросс-валидация (Cross-Validation, CV)}}
    Метод оценки обобщающей способности модели и получения более надежной оценки метрики, чем на единственном тест-сплите. Помогает бороться с переобучением и оценить стабильность модели.
    \begin{itemize}
        \item \textbf{Идея}: Разделить обучающую выборку на $K$ непересекающихся частей (фолдов). Поочередно использовать $K-1$ часть для обучения модели и 1 оставшуюся часть для валидации (расчета метрики). Повторить $K$ раз, каждый раз меняя валидационный фолд. Итоговая оценка метрики — среднее значение по всем $K$ фолдам. Также смотрят на стандартное отклонение метрики по фолдам для оценки стабильности.
        \item \textbf{K-Fold CV}: Самый распространенный вид. Данные делятся на $K$ фолдов примерно одинакового размера (часто K=5 или K=10).
        \item \textbf{Stratified K-Fold CV}: Вариант K-Fold для задач \textbf{классификации}, особенно при \textbf{дисбалансе классов}. Гарантирует, что в каждом фолде сохраняется исходное соотношение (стратификация) классов. \textbf{Использовать по умолчанию для классификации!}
        \item \textbf{Leave-One-Out CV (LOOCV)}: Частный случай K-Fold, где $K=n$ (количество объектов). Каждый объект по очереди используется как валидационный сет. Долго, но дает почти несмещенную оценку ошибки. Используется редко, на очень маленьких данных.
    \end{itemize}
    \textit{Аналогия K-Fold}: Подготовка к экзамену. У вас есть 5 тем (K=5). Вы 5 раз готовитесь: 1 раз учите темы 1,2,3,4 и отвечаете по теме 5; потом учите 1,2,3,5 и отвечаете по 4, и т.д. Итоговая оценка — среднее по 5 "экзаменам".
\end{myblock}

\begin{textbox}{Проблема Дисбаланса Классов}
    Ситуация, когда объектов одного класса значительно больше, чем другого (например, 99% не-мошенников и 1% мошенников).
    \begin{itemize}
        \item \textbf{Проблема}:
            \begin{itemize}
                \item Accuracy становится бесполезной метрикой.
                \item Модель может "научиться" всегда предсказывать мажоритарный класс и иметь высокую Accuracy.
                \item Стандартный K-Fold может привести к фолдам без (или с очень малым числом) объектов миноритарного класса.
            \end{itemize}
        \item \textbf{Основные подходы к решению}:
            \begin{enumerate}
                \item \textbf{Выбор правильной метрики}: Использовать \textbf{Precision, Recall, F1-меру, ROC AUC, PR AUC}. Анализировать \textbf{матрицу ошибок}.
                \item \textbf{Изменение выборки (Resampling)}:
                    \begin{itemize}
                        \item \textbf{Undersampling}: Удаление части объектов мажоритарного класса. Риск потери информации.
                        \item \textbf{Oversampling}: Дублирование объектов миноритарного класса. Риск переобучения на дубликатах.
                        \item \textbf{SMOTE (Synthetic Minority Over-sampling Technique)} и его варианты: Генерация *синтетических* объектов миноритарного класса на основе их соседей. Часто работает лучше простого oversampling.
                    \end{itemize}
                    \textbf{Внимание!} Методы изменения выборки (Under/Oversampling, SMOTE) должны применяться \textbf{только к обучающей части данных внутри каждого фолда кросс-валидации}, но \textbf{никогда} к валидационной или тестовой выборке, чтобы избежать утечки данных (data leakage). % <<< ДОБАВЛЕНО ПРЕДУПРЕЖДЕНИЕ
                \item \textbf{Взвешивание классов (Class Weighting)}: Назначение большего веса объектам миноритарного класса в функции потерь модели при обучении. Многие алгоритмы (логистическая регрессия, SVM, деревья решений, градиентный бустинг) поддерживают это (например, параметр \texttt{class\_weight='balanced'} или \texttt{scale\_pos\_weight} в scikit-learn и XGBoost/LightGBM).
                \item \textbf{Использование ансамблей}: Специальные методы ансамблирования, учитывающие дисбаланс (например, EasyEnsemble, BalanceCascade).
                \item \textbf{Использовать Stratified K-Fold} при кросс-валидации (как уже упоминалось).
            \end{enumerate}
    \end{itemize}
\end{textbox}

% --- Конец контента ---