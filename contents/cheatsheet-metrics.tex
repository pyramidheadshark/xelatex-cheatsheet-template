% >>> Контент для шпаргалки "Cheatsheet 2: Оценка Моделей - Метрики и Валидация"

% --- Раздел 1 ---
\section{Метрики Оценки: Регрессия}

\begin{myblock}{Зачем нужны метрики?}
    Метрики — это численные показатели, позволяющие **объективно оценить качество** работы модели машинного обучения. Для задач регрессии (предсказание непрерывного значения, например, цены дома или температуры) используются свои метрики.
\end{myblock}

\begin{textbox}{Основные метрики регрессии}
    Пусть $y_i$ — истинное значение, а $\hat{y}_i$ — предсказанное моделью значение для $i$-го объекта, $n$ — количество объектов, $\bar{y}$ - среднее истинных значений.
    \begin{itemize}
        \item \textbf{MAE (Mean Absolute Error) / Средняя Абсолютная Ошибка}: Показывает среднее абсолютное отклонение предсказаний от факта. Легко интерпретируется в единицах целевой переменной. Менее чувствительна к выбросам, чем MSE.
            \[ MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| \]
        \item \textbf{MSE (Mean Squared Error) / Среднеквадратичная Ошибка}: Среднее квадратов отклонений. Сильнее штрафует за большие ошибки из-за возведения в квадрат. Используется в оптимизации многих моделей. Единицы измерения - квадрат исходных единиц.
            \[ MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \]
        \item \textbf{RMSE (Root Mean Squared Error) / Корень из Среднеквадратичной Ошибки}: Корень из MSE. Возвращает метрику к исходным единицам измерения, что упрощает интерпретацию. Как и MSE, чувствительна к выбросам.
            \[ RMSE = \sqrt{MSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} \]
        \item \textbf{R² (Коэффициент Детерминации)}: Показывает, какую долю дисперсии зависимой переменной объясняет модель по сравнению с простой моделью, всегда предсказывающей среднее. Значения от $(-\infty)$ до 1. Ближе к 1 — лучше. 0 — модель работает как среднее. Отрицательные значения — модель хуже среднего.
            \[ R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} \]
            \textit{Аналогия R²}: Представьте, что вы пытаетесь предсказать рост людей. Если вы всегда предсказываете средний рост (простая модель), R² будет 0. Если ваша модель идеально предсказывает рост каждого, R² = 1.
    \end{itemize}
\end{textbox}

% --- Раздел 2 ---
\section{Метрики Оценки: Классификация}

\begin{myblock}{Матрица ошибок (Confusion Matrix)}
    Основа для большинства метрик бинарной классификации. Показывает, сколько объектов какого класса и как были классифицированы.
    \begin{itemize}
        \item \textbf{TP (True Positive)}: Истинно положительные. Класс 1, предсказан как 1. (Нашли больного)
        \item \textbf{TN (True Negative)}: Истинно отрицательные. Класс 0, предсказан как 0. (Нашли здорового)
        \item \textbf{FP (False Positive)}: Ложно положительные. \textbf{Ошибка I рода}. Класс 0, предсказан как 1. (Здоровый признан больным)
        \item \textbf{FN (False Negative)}: Ложно отрицательные. \textbf{Ошибка II рода}. Класс 1, предсказан как 0. (Больной признан здоровым)
    \end{itemize}
    \vspace{1ex}
    \textbf{Матрица Ошибок:}
    \vspace{0.5ex}
    \begin{center} % Центрирование таблицы для лучшего вида
    \begin{tabular}{c|c|c}
         & \textbf{Предсказание: 1} & \textbf{Предсказание: 0} \\\hline
        \textbf{Реальность: 1} & TP & FN \\\hline
        \textbf{Реальность: 0} & FP & TN \\
    \end{tabular}
    \end{center}
    \vspace{1ex}
\end{myblock}

\begin{textbox}{Основные метрики классификации}
    \begin{itemize}
        \item \textbf{Accuracy (Доля правильных ответов)}: Общая доля верных предсказаний. \textbf{Плохо работает при дисбалансе классов!}
            \[ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} \]
            \textit{Аналогия}: Если 99\% писем - не спам, модель, всегда говорящая "не спам", будет иметь Accuracy 99%, но она бесполезна для поиска спама.
        \item \textbf{Precision (Точность)}: Какая доля объектов, названных моделью классом 1, действительно являются классом 1? Важна, когда цена FP высока (напр., отправка здорового на дорогую операцию).
            \[ \text{Precision} = \frac{TP}{TP + FP} \]
        \item \textbf{Recall (Полнота, Sensitivity, True Positive Rate - TPR)}: Какую долю объектов класса 1 модель смогла правильно найти? Важна, когда цена FN высока (напр., пропуск больного пациента или мошеннической транзакции).
            \[ \text{Recall} = \frac{TP}{TP + FN} \]
        \item \textbf{F1-мера (F1-Score)}: Гармоническое среднее Precision и Recall. Полезна, когда важен баланс между точностью и полнотой. Стремится к нулю, если хотя бы одна из метрик (Precision или Recall) близка к нулю.
            \[ F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2TP}{2TP + FP + FN} \]
            Можно использовать \textbf{$F_\beta$-меру} для придания большего веса Precision ($\beta < 1$) или Recall ($\beta > 1$).
        \item \textbf{Specificity (Специфичность, True Negative Rate - TNR)}: Какую долю объектов класса 0 модель верно определила?
             \[ \text{Specificity} = \frac{TN}{TN + FP} \]
        \item \textbf{False Positive Rate (FPR)}: Какую долю объектов класса 0 модель неверно назвала классом 1? $FPR = 1 - \text{Specificity}$.
            \[ \text{FPR} = \frac{FP}{TN + FP} \]
    \end{itemize}
    \textit{Аналогия Precision/Recall (Спам-фильтр)}:
    \begin{itemize}
        \item \textbf{Precision}: Из всех писем, что попали в папку "Спам", какая доля реально спам? (Не хотим терять важные письма - высокий Precision).
        \item \textbf{Recall}: Из всех реально спамовых писем, какая доля попала в папку "Спам"? (Хотим отловить как можно больше спама - высокий Recall).
    \end{itemize}
\end{textbox}

\begin{myexampleblock}{ROC AUC (Receiver Operating Characteristic Area Under Curve)}
    Показывает качество модели в задаче \textbf{ранжирования} классов, независимо от выбранного порога классификации.
    \begin{itemize}
        \item \textbf{ROC-кривая}: График зависимости \textbf{TPR (Recall)} от \textbf{FPR} при изменении порога классификации от 1 до 0.
        \item \textbf{AUC (Area Under Curve)}: Площадь под ROC-кривой. Варьируется от 0 до 1.
            \begin{itemize}
                \item AUC = 1: Идеальный классификатор.
                \item AUC = 0.5: Случайное угадывание (модель бесполезна, диагональная линия).
                \item AUC < 0.5: Модель работает хуже случайной (возможно, перепутаны метки классов).
            \end{itemize}
        \item \textbf{Интерпретация AUC}: Вероятность того, что случайно выбранный объект класса 1 получит от модели оценку выше (более высокую вероятность принадлежности к классу 1), чем случайно выбранный объект класса 0.
        \item \textbf{Преимущества}: Относительная устойчивость к дисбалансу классов (по сравнению с Accuracy). Позволяет сравнить модели в целом, без привязки к конкретному порогу.
    \end{itemize}
    \textit{Аналогия ROC AUC}: Представьте соревнование: модели нужно выстроить всех людей в ряд так, чтобы все "больные" (класс 1) оказались правее всех "здоровых" (класс 0). AUC показывает, насколько хорошо модель справляется с этой задачей ранжирования.

    \begin{center} % <<< ДОБАВЛЕН ГРАФИК ROC
    \begin{tikzpicture}
        \begin{axis}[roc curve style, title={Пример ROC-кривой}]
        % Пример данных для хорошей модели
        \addplot coordinates { (0,0) (0.1,0.5) (0.2,0.8) (0.4,0.9) (0.6,0.95) (0.8,0.98) (1,1) };
        \addlegendentry{Модель (AUC $\approx$ 0.9)}
        % Можно добавить кривую для слабой модели
        % \addplot coordinates { (0,0) (0.2,0.2) (0.4,0.4) (0.6,0.6) (0.8,0.8) (1,1) };
        % \addlegendentry{Модель 2 (AUC = 0.5)}
        \end{axis}
    \end{tikzpicture}
    \end{center}
\end{myexampleblock}

\begin{myexampleblock}{Precision-Recall AUC (PR AUC)}
    Альтернатива ROC AUC, особенно полезная при \textbf{сильном дисбалансе классов}, когда важнее всего найти объекты редкого положительного класса.
    \begin{itemize}
        \item \textbf{PR-кривая}: График зависимости \textbf{Precision} от \textbf{Recall (TPR)} при изменении порога классификации.
        \item \textbf{PR AUC}: Площадь под PR-кривой. Также от 0 до 1.
        \item \textbf{Почему при дисбалансе?}: ROC AUC может быть обманчиво высоким при дисбалансе, так как TN обычно много, и FPR остается низким. PR-кривая фокусируется на поиске редкого положительного класса (TP) и цене ошибок на нем (FP), что важнее при дисбалансе.
        \item \textbf{Baseline}: В отличие от ROC AUC (baseline 0.5), baseline для PR AUC зависит от доли положительного класса $P$ в выборке: $\text{baseline} \approx P/(P+N)$. Для сильно несбалансированной выборки baseline PR AUC близок к 0.
    \end{itemize}
    \textit{Аналогия PR AUC}: Представьте поиск иголок (класс 1) в стоге сена (все данные). PR-кривая показывает: при разной степени "старания" (меняем порог -> меняется Recall), насколько точны наши находки (Precision)? Насколько много мусора (FP) мы захватываем вместе с иголками?

    \begin{center} % <<< ДОБАВЛЕН ГРАФИК PR
    \begin{tikzpicture}
        \begin{axis}[pr curve style, title={Пример PR-кривой}]
        % Пример данных для PR кривой (часто не монотонная)
        \addplot coordinates { (0.05,0.95) (0.1,0.9) (0.2,0.85) (0.3,0.8) (0.4,0.7) (0.5,0.6) (0.6,0.5) (0.7,0.4) (0.8,0.3) (0.9,0.2) (1,0.1) };
        \addlegendentry{Модель}
        % Можно показать baseline, если известна доля позитивного класса (P_frac)
        % \addplot[dashed, color=gray] coordinates {(0, P_frac) (1, P_frac)};
        % \addlegendentry{Baseline (случ.)}
        \end{axis}
    \end{tikzpicture}
    \end{center}
\end{myexampleblock}

\begin{alerttextbox}{Выбор метрики}
    Выбор метрики \textbf{критически зависит от бизнес-задачи}!
    \begin{itemize}
        \item **Медицинская диагностика (опасная болезнь):** Важнее найти всех больных (высокий \textbf{Recall}), даже если будут ложные срабатывания (низкий Precision). Цена FN (пропустить больного) очень высока. Используем Recall, F-меру с $\beta > 1$, PR AUC.
        \item **Спам-фильтр:** Важнее не отправлять нужные письма в спам (высокий \textbf{Precision}), даже если часть спама просочится (не идеальный Recall). Цена FP (потерять важное письмо) высока. Используем Precision, F-меру с $\beta < 1$.
        \item **Предсказание кликов (реклама):** Часто интересует общая точность предсказания вероятности клика, могут использовать \textbf{LogLoss} или \textbf{ROC AUC}.
        \item **Сильный дисбаланс классов (поиск мошенников):** Accuracy бесполезна. Смотреть на \textbf{F1-меру}, \textbf{PR AUC}, матрицу ошибок, Precision, Recall.
    \end{itemize}
    Всегда обсуждайте с заказчиком или продакт-менеджером, \textbf{какая ошибка для них страшнее} и как модель будет использоваться!
\end{alerttextbox}

\begin{myblock}{Кратко: Online vs Offline метрики}
    \begin{itemize}
        \item \textbf{Offline метрики}: Рассчитываются на отложенной (исторической) выборке (например, на тестовом датасете). Это все метрики, рассмотренные выше (Accuracy, F1, AUC, MSE и т.д.). Позволяют оценить модель до выкатки в продакшен.
        \item \textbf{Online метрики}: Рассчитываются на реальных данных после внедрения модели в работающую систему. Это обычно \textbf{бизнес-метрики}: CTR (Click-Through Rate), конверсия в покупку, средний чек, время на сайте, отток клиентов и т.д. Оцениваются и сравниваются с помощью **A/B тестирования**.
    \end{itemize}
\end{myblock}

% --- Раздел 3 ---
\section{Валидация и Надежность Оценки}

\begin{alerttextbox}{Статистическая Значимость [ОЧЕНЬ ВАЖНО]}
    Допустим, модель А дала AUC 0.85, а модель Б - AUC 0.86 на тестовой выборке. Значит ли это, что Б \textit{действительно} лучше? Не обязательно! Различие может быть случайным из-за ограниченности тестовой выборки. Для проверки нужны стат. тесты.
    \begin{itemize}
        \item \textbf{Статистическая гипотеза}: Проверяем нулевую гипотезу $H_0$: "Модели А и Б имеют одинаковое качество (разница в метриках случайна, $\text{AUC}_А = \text{AUC}_Б$)". Альтернативная гипотеза $H_1$: "Модель Б действительно лучше ($\text{AUC}_Б > \text{AUC}_А$)".
        \item \textbf{p-value (Уровень значимости)}: Вероятность получить наблюдаемую (или еще большую) разницу в метриках \textbf{при условии, что нулевая гипотеза верна} (т.е., если на самом деле разницы нет).
            \begin{itemize}
                \item \textbf{p-value < $\alpha$} (часто $\alpha = 0.05$): Считаем результат \textbf{статистически значимым} на уровне $\alpha$. Мы отвергаем $H_0$. Есть основания полагать, что модель Б действительно лучше.
                \item \textbf{p-value >= $\alpha$}: Результат \textbf{не является статистически значимым}. Мы не можем отвергнуть $H_0$. Наблюдаемая разница могла возникнуть случайно.
            \end{itemize}
        \item \textbf{Confidence Interval (CI) / Доверительный Интервал}: Диапазон значений, который с определенной вероятностью (обычно 95%, соответствует $\alpha = 0.05$) содержит истинное значение метрики (или разницы метрик). Если 95% CI для разницы ($\text{AUC}_Б - \text{AUC}_А$) \textbf{не включает ноль} и целиком положителен (например, [0.005, 0.015]), это подтверждает стат. значимость улучшения.
    \end{itemize}
    \textbf{Почему это важно?} Чтобы не принимать бизнес-решения (например, о внедрении новой модели, изменении продукта) на основе случайных колебаний метрик. Это \textbf{основа для интерпретации результатов A/B тестов} и сравнения моделей на offline-выборках. % <<< Усилено здесь
    \textit{Аналогия p-value}: Суд над гипотезой $H_0$ ("разницы нет"). p-value - это сила улик против $H_0$. Если улик мало (p-value большое, >= $\alpha$), мы не можем "осудить" $H_0$ (не отвергаем). Если улик много (p-value маленькое, < $\alpha$), мы "осуждаем" $H_0$ (отвергаем) и принимаем $H_1$.
\end{alerttextbox}

\begin{myblock}{{Кросс-валидация (Cross-Validation, CV)}}
    Метод оценки обобщающей способности модели и получения более надежной оценки метрики, чем на единственном тест-сплите. Помогает бороться с переобучением и оценить стабильность модели.
    \begin{itemize}
        \item \textbf{Идея}: Разделить обучающую выборку на $K$ непересекающихся частей (фолдов). Поочередно использовать $K-1$ часть для обучения модели и 1 оставшуюся часть для валидации (расчета метрики). Повторить $K$ раз, каждый раз меняя валидационный фолд. Итоговая оценка метрики — среднее значение по всем $K$ фолдам. Также смотрят на стандартное отклонение метрики по фолдам для оценки стабильности.
        \item \textbf{K-Fold CV}: Самый распространенный вид. Данные делятся на $K$ фолдов примерно одинакового размера (часто K=5 или K=10).
        \item \textbf{Stratified K-Fold CV}: Вариант K-Fold для задач \textbf{классификации}, особенно при \textbf{дисбалансе классов}. Гарантирует, что в каждом фолде сохраняется исходное соотношение (стратификация) классов. \textbf{Использовать по умолчанию для классификации!}
        \item \textbf{Leave-One-Out CV (LOOCV)}: Частный случай K-Fold, где $K=n$ (количество объектов). Каждый объект по очереди используется как валидационный сет. Долго, но дает почти несмещенную оценку ошибки. Используется редко, на очень маленьких данных.
    \end{itemize}
    \textit{Аналогия K-Fold}: Подготовка к экзамену. У вас есть 5 тем (K=5). Вы 5 раз готовитесь: 1 раз учите темы 1,2,3,4 и отвечаете по теме 5; потом учите 1,2,3,5 и отвечаете по 4, и т.д. Итоговая оценка — среднее по 5 "экзаменам".
\end{myblock}

\begin{textbox}{Проблема Дисбаланса Классов}
    Ситуация, когда объектов одного класса значительно больше, чем другого (например, 99% не-мошенников и 1% мошенников).
    \begin{itemize}
        \item \textbf{Проблема}:
            \begin{itemize}
                \item Accuracy становится бесполезной метрикой.
                \item Модель может "научиться" всегда предсказывать мажоритарный класс и иметь высокую Accuracy.
                \item Стандартный K-Fold может привести к фолдам без (или с очень малым числом) объектов миноритарного класса.
            \end{itemize}
        \item \textbf{Основные подходы к решению}:
            \begin{enumerate}
                \item \textbf{Выбор правильной метрики}: Использовать \textbf{Precision, Recall, F1-меру, ROC AUC, PR AUC}. Анализировать \textbf{матрицу ошибок}.
                \item \textbf{Изменение выборки (Resampling)}:
                    \begin{itemize}
                        \item \textbf{Undersampling}: Удаление части объектов мажоритарного класса. Риск потери информации.
                        \item \textbf{Oversampling}: Дублирование объектов миноритарного класса. Риск переобучения на дубликатах.
                        \item \textbf{SMOTE (Synthetic Minority Over-sampling Technique)} и его варианты: Генерация *синтетических* объектов миноритарного класса на основе их соседей. Часто работает лучше простого oversampling.
                    \end{itemize}
                    \textbf{Внимание!} Методы изменения выборки (Under/Oversampling, SMOTE) должны применяться \textbf{только к обучающей части данных внутри каждого фолда кросс-валидации}, но \textbf{никогда} к валидационной или тестовой выборке, чтобы избежать утечки данных (data leakage). % <<< ДОБАВЛЕНО ПРЕДУПРЕЖДЕНИЕ
                \item \textbf{Взвешивание классов (Class Weighting)}: Назначение большего веса объектам миноритарного класса в функции потерь модели при обучении. Многие алгоритмы (логистическая регрессия, SVM, деревья решений, градиентный бустинг) поддерживают это (например, параметр \texttt{class\_weight='balanced'} или \texttt{scale\_pos\_weight} в scikit-learn и XGBoost/LightGBM).
                \item \textbf{Использование ансамблей}: Специальные методы ансамблирования, учитывающие дисбаланс (например, EasyEnsemble, BalanceCascade).
                \item \textbf{Использовать Stratified K-Fold} при кросс-валидации (как уже упоминалось).
            \end{enumerate}
    \end{itemize}
\end{textbox}

% --- Конец контента ---