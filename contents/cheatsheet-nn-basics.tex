% >>> Обновленный Контент для шпаргалки "Введение в Нейронные Сети (NN)" v3

\section{VI. Введение в Нейронные Сети (NN)}

\begin{textbox}{Цель раздела}
    Понять базовые компоненты нейронных сетей (нейроны, слои, функции активации), основной механизм обучения (Backpropagation) и методы его улучшения (оптимизаторы, регуляризация). Заложить основу для понимания сверточных и рекуррентных сетей.
\end{textbox}

% --- VI.A: Базовые Структуры ---
\subsection{VI.A Базовые Структуры: Нейроны и Слои}

\begin{myblock}{Искусственный Нейрон: Вычислительный Элемент}
    \textbf{Что это:} Математическая модель, имитирующая работу биологического нейрона.
    \textbf{Как работает:}
    \begin{enumerate}
        \item Принимает входы ($x_i$).
        \item Умножает каждый вход на его \textbf{вес} ($w_i$) $\rightarrow$ $w_i x_i$.
        \item Суммирует взвешенные входы $\rightarrow$ $z_{sum} = \sum_{i} w_i x_i$.
        \item Добавляет \textbf{смещение} ($b$) $\rightarrow$ $z = z_{sum} + b$.
        \item Пропускает результат $z$ через \textbf{функцию активации} $f(\cdot)$ $\rightarrow$ $y = f(z)$ (выход нейрона).
    \end{enumerate}
    \textbf{Обучаемые параметры:} Веса $w_i$ и смещение $b$.
\end{myblock}

\begin{myblock}{Многослойный Перцептрон (MLP): Архитектура}
    \textbf{Что это:} Классическая нейросеть из нескольких слоев нейронов.
    \textbf{Слои:}
    \begin{itemize}
        \item \textbf{Входной (Input):} Принимает признаки $X$. Не содержит вычислительных нейронов.
        \item \textbf{Скрытые (Hidden):} Один или более. Здесь происходит основная обработка, извлечение паттернов.
        \item \textbf{Выходной (Output):} Формирует результат. Структура зависит от задачи (1 нейрон/линейная для регрессии, 1 нейрон/сигмоида для бинарной клас., N нейронов/Softmax для многоклассовой).
    \end{itemize}
    \textbf{Связи:} Обычно \textbf{полносвязные} (Dense) — каждый нейрон слоя связан с каждым нейроном следующего.
\end{myblock}

% --- VI.B: Функции активации ---
\subsection{VI.B Функции Активации: Нелинейность и Свойства}

\begin{alerttextbox}{Зачем нужна Нелинейность?}
    Без нелинейных функций активации в скрытых слоях вся сеть была бы эквивалентна простой линейной модели. Нелинейность позволяет изучать сложные зависимости.
\end{alerttextbox}

\begin{myexampleblock}{ReLU (Rectified Linear Unit)}
    \[ f(x) = \max(0, x) \]
    \textbf{Свойства:} Вычислительно проста. Не насыщается для $x>0$ (помогает с затуханием градиента).
    \textbf{Недостаток:} "Умирающие ReLU" (нейрон перестает активироваться и обучаться, если $x$ всегда $\le 0$).
    \textbf{Использование:} Стандартный выбор для скрытых слоев.
\end{myexampleblock}

\begin{myexampleblock}{Leaky ReLU}
    \[ f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \le 0 \end{cases} \quad (\alpha \approx 0.01-0.2) \]
    \textbf{Свойства:} Решает проблему "умирающих ReLU", давая малый ненулевой градиент при $x \le 0$.
\end{myexampleblock}

\begin{myexampleblock}{ELU (Exponential Linear Unit)}
     \[ f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha (e^x - 1) & \text{if } x \le 0 \end{cases} \quad (\alpha > 0) \]
    \textbf{Свойства:} Похожа на Leaky ReLU, но использует экспоненту. Может давать лучшие результаты, чем ReLU/Leaky ReLU. Выход для $x<0$ отрицательный.
\end{myexampleblock}

\begin{myexampleblock}{Sigmoid (Сигмоида)}
    \[ f(x) = \frac{1}{1 + e^{-x}} \]
    \textbf{Свойства:} Выход [0, 1], удобен для вероятностей.
    \textbf{Недостатки:} Затухание градиентов. Выход не центрирован около нуля.
    \textbf{Использование:} Выходной слой бинарной классификации. Редко в скрытых слоях современных сетей.
\end{myexampleblock}

\begin{myexampleblock}{Tanh (Гиперболический тангенс)}
    \[ f(x) = \tanh(x) \]
    \textbf{Свойства:} Выход [-1, 1], центрирован около нуля (лучше Sigmoid для скрытых слоев).
    \textbf{Недостатки:} Затухание градиентов (хотя меньше, чем у Sigmoid).
    \textbf{Использование:} Иногда в скрытых слоях, часто в RNN/LSTM.
\end{myexampleblock}

\begin{myexampleblock}{Softmax}
    \[ f(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]
    \textbf{Свойства:} Преобразует вектор логитов в распределение вероятностей (сумма=1).
    \textbf{Использование:} \textbf{Только} в выходном слое для \textbf{многоклассовой классификации}.
\end{myexampleblock}

% Обновленный график функций активации
\begin{myblock}{}
    {\noindent\small\bfseries\color{mDarkTeal}Графики популярных функций активации\par\vspace{1ex}}
    \begin{center}
    \begin{tikzpicture}
        \begin{axis}[
            cheatsheet plot style,
            title={Функции активации},
            xlabel={$x$},
            ylabel={$f(x)$},
            xmin=-4, xmax=4, % Уменьшим диапазон для лучшей видимости
            ymin=-1.5, ymax=2.5, % Расширим немного вверх
            axis lines=middle,
            legend style={at={(1.05,1)}, anchor=north west}, % Легенда справа вверху
            samples=100
        ]
        \addplot [blue, thick, domain=-4:4] {max(0, x)}; \addlegendentry{ReLU}
        \addplot [alert, thick, domain=-4:4] {1 / (1 + exp(-x))}; \addlegendentry{Sigmoid}
        \addplot [w3schools, thick, domain=-4:4] {tanh(x)}; \addlegendentry{Tanh}
        % Leaky ReLU с alpha=0.1
        \addplot [mDarkTeal, dashed, thick, domain=-4:4] { (x > 0) * x + (x <= 0) * 0.1 * x }; \addlegendentry{Leaky ReLU ($\alpha=0.1$)}
        % ELU с alpha=1
        \addplot [orange, dotted, thick, domain=-4:4] { (x > 0) * x + (x <= 0) * 1 * (exp(x) - 1) }; \addlegendentry{ELU ($\alpha=1$)}
        \end{axis}
    \end{tikzpicture}
    \end{center}
\end{myblock}

\begin{alerttextbox}{Проблема Затухания/Взрыва Градиентов}
    \textbf{Проблема:} При обучении глубоких сетей градиенты могут стать исчезающе малыми (\textbf{затухание}) или аномально большими (\textbf{взрыв}).
    \textbf{Последствия:} Замедление или остановка обучения (затухание), нестабильность (взрыв).
    \textbf{Решения:} Выбор активаций (ReLU и др.), правильная инициализация весов, Batch Normalization, обрезание градиентов (для взрыва).
\end{alerttextbox}

% --- VI.C: Backpropagation ---
\subsection{VI.C Backpropagation: Как Сеть Учится}

\begin{alerttextbox}{Backpropagation: Ключевой Алгоритм Обучения}
    \textbf{Цель:} Эффективно вычислить \textbf{градиенты} функции потерь $J$ по всем обучаемым параметрам ($w, b$). Градиент $\partial J / \partial w$ показывает, как сильно изменение веса $w$ повлияет на итоговую ошибку $J$.
\end{alerttextbox}

\begin{myblock}{Этап 1: Прямой Проход (Forward Pass)}
    \textbf{Что происходит:} Данные $X$ проходят через сеть слой за слоем от входа к выходу. На каждом слое вычисляются взвешенные суммы ($z$) и активации ($a$). Получаем итоговые предсказания $\hat{y}$.
    \textbf{Результат:} Предсказания $\hat{y}$ и значения активаций $a$ на всех слоях (они понадобятся для обратного прохода).
    \textbf{Затем:} Вычисляется \textbf{функция потерь} $J(\hat{y}, y)$, измеряющая ошибку предсказания.
\end{myblock}

\begin{myblock}{Этап 2: Обратный Проход (Backward Pass)}
    \textbf{Что происходит:} "Ошибка" $J$ распространяется обратно от выхода ко входу. На каждом слое вычисляются градиенты по параметрам этого слоя и по его входам (активациям предыдущего слоя).
    \textbf{Шаги (идем от слоя L к слою 1):}
    \begin{enumerate}
        \item \textbf{Слой L (Выходной):}
            \begin{itemize}
                \item Вычисляем $\partial J / \partial a_L$ (как ошибка зависит от выхода сети).
                \item Вычисляем $\partial J / \partial z_L = (\partial J / \partial a_L) \odot f'_L(z_L)$. (\textit{Пояснение: Насколько ошибка зависит от пред-активационного значения $z_L$? Зависит от того, как она зависит от $a_L$ и как $a_L$ меняется с $z_L$ (это $f'_L$). $\odot$ - поэлементное умножение.}).
                \item Вычисляем $\partial J / \partial W_L = (\partial J / \partial z_L) \cdot a_{L-1}^T$ и $\partial J / \partial b_L = \sum (\partial J / \partial z_L)$. (\textit{Пояснение: Зная, как $z_L$ влияет на ошибку, и зная, как $W_L, b_L$ влияют на $z_L$ (через вход $a_{L-1}$), находим градиенты для параметров.}).
            \end{itemize}
        \item \textbf{Слой l (Скрытый):}
             \begin{itemize}
                \item Вычисляем $\partial J / \partial a_l = W_{l+1}^T \cdot (\partial J / \partial z_{l+1})$. (\textit{Пояснение: Ошибка "приходит" из следующего слоя $l+1$. Насколько она зависит от выхода $a_l$ этого слоя? Зависит от того, как ошибка зависит от $z_{l+1}$ и как $z_{l+1}$ зависит от $a_l$ (через веса $W_{l+1}$)}).
                \item Вычисляем $\partial J / \partial z_l = (\partial J / \partial a_l) \odot f'_l(z_l)$. (\textit{Аналогично выходному слою}).
                \item Вычисляем $\partial J / \partial W_l = (\partial J / \partial z_l) \cdot a_{l-1}^T$ и $\partial J / \partial b_l = \sum (\partial J / \partial z_l)$. (\textit{Аналогично выходному слою}).
            \end{itemize}
        \item \textbf{Повторение:} Шаги для скрытого слоя повторяются до слоя 1.
    \end{enumerate}
    \textbf{Результат:} Градиенты $\partial J / \partial W_l$ и $\partial J / \partial b_l$ для всех слоев $l$.
    \textbf{Механизм:} Эффективное применение \textbf{цепного правила (chain rule)} дифференцирования.
\end{myblock}

% --- VI.D: Оптимизаторы ---
\subsection{VI.D Оптимизаторы: Обновление Весов}

\begin{alerttextbox}{Роль Оптимизатора}
    Использует градиенты, полученные от Backpropagation, для вычисления и применения обновлений к весам $\mathbf{w}$ и смещениям $\mathbf{b}$, чтобы минимизировать функцию потерь $J$.
\end{alerttextbox}

\begin{myblock}{SGD (Stochastic Gradient Descent)}
    \textbf{Идея:} Простой шаг в направлении анти-градиента, вычисленного по батчу.
    \textbf{Формула:} $\mathbf{w} := \mathbf{w} - \alpha \cdot \nabla J(\mathbf{w})$.
    \textbf{Параметр:} Learning rate $\alpha$.
\end{myblock}

\begin{myblock}{Momentum}
    \textbf{Идея:} Добавить "инерцию" к SGD. Учитывает предыдущий шаг обновления $v$.
    \textbf{Формула:} $v_t = \beta v_{t-1} + \alpha \nabla J(\mathbf{w})$; $\mathbf{w} := \mathbf{w} - v_t$.
    \textbf{Параметры:} $\alpha$, $\beta$ (момент, обычно ~0.9).
    \textbf{Польза:} Ускоряет сходимость, помогает преодолевать плато.
\end{myblock}

\begin{myblock}{AdaGrad (Adaptive Gradient)}
    \textbf{Идея:} Адаптивный learning rate для каждого параметра. Уменьшает шаг для часто обновляемых параметров.
    \textbf{Формула:} Накапливает квадрат градиента $G$; $\Delta w_i = \frac{\alpha}{\sqrt{G_{ii} + \epsilon}} \nabla J_i(w)$.
    \textbf{Польза:} Хорош для разреженных данных.
    \textbf{Недостаток:} Learning rate может слишком быстро затухнуть.
\end{myblock}

\begin{myblock}{RMSProp}
    \textbf{Идея:} Исправить проблему AdaGrad с затуханием шага. Использует скользящее среднее квадратов градиентов $E[g^2]$.
    \textbf{Формула:} $E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)(\nabla J)^2$; $\Delta w = \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} \nabla J(w)$.
    \textbf{Параметры:} $\alpha$, $\gamma$ (коэфф. затухания, ~0.9).
\end{myblock}

\begin{myblock}{Adam (Adaptive Moment Estimation)}
    \textbf{Идея:} Сочетает Momentum (скользящее среднее градиентов $m$) и RMSProp (скользящее среднее квадратов градиентов $v$).
    \textbf{Формула:} Использует $m$ и $v$ для вычисления адаптивного шага. Включает коррекцию смещения.
    \textbf{Польза:} Часто эффективен по умолчанию, хорошо работает на широком круге задач.
    \textbf{Параметры:} $\alpha$, $\beta_1$ (~0.9), $\beta_2$ (~0.999).
\end{myblock}

% --- VI.E: Dropout, BatchNorm ---
\subsection{VI.E Стабилизация и Регуляризация Обучения}

\begin{myexampleblock}{Dropout (Прореживание)}
    \textbf{Что это:} Метод \textbf{регуляризации} для борьбы с переобучением.
    \textbf{Как работает (на обучении):} Случайным образом обнуляет выходы части нейронов слоя с вероятностью $p$.
    \textbf{Как работает (на предсказании):} Использует все нейроны, но масштабирует их выходы на $(1-p)$.
    \textbf{Эффект:} Заставляет сеть учить более робастные и распределенные представления.
\end{myexampleblock}

\begin{myexampleblock}{Batch Normalization (BatchNorm)}
    \textbf{Что это:} Техника для \textbf{стабилизации и ускорения} обучения.
    \textbf{Как работает (на обучении):}
    \begin{enumerate}
        \item Нормализует входы $z$ слоя по батчу (среднее 0, дисперсия 1).
        \item Масштабирует и сдвигает результат с помощью обучаемых $\gamma$ и $\beta$.
        \item Обновляет скользящие средние $\mu_{run}, \sigma^2_{run}$.
    \end{enumerate}
    \textbf{Как работает (на предсказании):} Использует $\mu_{run}, \sigma^2_{run}$ и обученные $\gamma, \beta$.
    \textbf{Эффект:} Борется с internal covariate shift, позволяет использовать больший learning rate, имеет легкий регуляризующий эффект. Обычно вставляется \textit{до} функции активации.
\end{myexampleblock}

% --- VI.F: CNN и RNN ---
\subsection{VI.F Специализированные Архитектуры}

\begin{textbox}{Зачем нужны специализированные сети?}
    MLP универсальны, но для данных с внутренней структурой (пространственной или временной) CNN и RNN часто более эффективны.
\end{textbox}

\begin{myblock}{CNN (Convolutional Neural Networks)}
    \textbf{Применение:} Изображения, видео, данные с сетчатой структурой.
    \textbf{Ключевые Идеи:}
    \begin{itemize}
        \item \textbf{Сверточный слой:} Применяет \textbf{фильтры (ядра)} для обнаружения локальных паттернов (грани, текстуры). Использует \textit{локальные связи} и \textit{разделяемые веса}. Выход - \textbf{карты признаков}.
        \item \textbf{Пулинг слой:} Уменьшает пространственный размер карт признаков (Max Pooling, Average Pooling), обеспечивая инвариантность к малым сдвигам.
    \end{itemize}
    \textbf{Архитектура:} Чередование [Conv -> Activation -> Pooling]. Затем полносвязные слои для классификации/регрессии.
\end{myblock}

\begin{myblock}{RNN (Recurrent Neural Networks)}
    \textbf{Применение:} Последовательные данные (текст, временные ряды, речь).
    \textbf{Ключевая Идея:} \textbf{Рекуррентная связь} позволяет сети иметь "память" (\textbf{скрытое состояние} $h_t$), передаваемую от шага к шагу.
    \textbf{Простая RNN:} $h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$. Веса $W$ разделяемые по времени.
    \textbf{Проблема:} Затухание/взрыв градиентов на длинных последовательностях (BPTT).
\end{myblock}

\begin{myblock}{LSTM (Long Short-Term Memory) и GRU (Gated Recurrent Unit)}
    \textbf{Что это:} Продвинутые RNN-ячейки для решения проблемы градиентов.
    \textbf{Как работают:} Используют \textbf{гейты} (механизмы управления информацией с Sigmoid/Tanh), чтобы контролировать, что запоминать, что забывать, и что передавать дальше.
    \textbf{LSTM:} Имеет 3 гейта (input, forget, output) и состояние ячейки (cell state).
    \textbf{GRU:} Упрощенная версия с 2 гейтами (reset, update).
    \textbf{Использование:} Стандарт де-факто для задач с последовательностями вместо простых RNN.
\end{myblock}

% --- Конец обновленного контента ---