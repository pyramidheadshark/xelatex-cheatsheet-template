% >>> Контент для шпаргалки "Введение в Нейронные Сети (NN)"

\section{VI. Введение в Нейронные Сети (NN)}

\begin{textbox}{Цель раздела}
    Понять базовые строительные блоки нейронных сетей, их терминологию и основной принцип обучения. Не бояться слов «перцептрон», «backpropagation», «оптимизатор». Это основа для понимания более сложных моделей.
\end{textbox}

% --- VI.A: Перцептрон, MLP - структура ---
\subsection{VI.A Базовые Структуры: Перцептрон и MLP}

\begin{myblock}{Перцептрон: Простейший Нейрон}
    \textbf{Перцептрон} (Perceptron) — это самый базовый, \textit{исторический} элемент нейросети, математическая модель нейрона.
    \begin{itemize}
        \item Он принимает несколько входов (чисел), умножает каждый на свой \textbf{вес} (weight).
        \item Суммирует взвешенные входы и добавляет \textbf{смещение} (bias).
        \item Пропускает результат через \textbf{функцию активации} (в классическом перцептроне — пороговую, «ступеньку»).
    \end{itemize}
    \textbf{Аналогия:} Представьте себе простого решателя, который взвешивает разные «за» и «против» (входы с весами), сравнивает с неким порогом (смещение) и выносит вердикт «да» или «нет» (выход активации).
    \[ y = f(\sum_{i} w_i x_i + b) \]
    Где $x_i$ - входы, $w_i$ - веса, $b$ - смещение, $f$ - функция активации, $y$ - выход.
\end{myblock}

\begin{myblock}{Многослойный Перцептрон (MLP)}
    \textbf{Многослойный Перцептрон} (Multi-Layer Perceptron, MLP) — это уже полноценная нейронная сеть, состоящая из слоев перцептронов (или нейронов с другими, обычно нелинейными, функциями активации).
    \begin{itemize}
        \item \textbf{Входной слой} (Input Layer): Получает исходные данные (признаки). Нейронов столько, сколько признаков.
        \item \textbf{Скрытые слои} (Hidden Layers): Один или несколько слоев между входным и выходным. Здесь происходит основная «магия» обработки информации. Количество слоев и нейронов в них — это гиперпараметры.
        \item \textbf{Выходной слой} (Output Layer): Выдает конечный результат (предсказание класса, число и т.д.). Количество нейронов зависит от задачи (например, 1 для регрессии, N для N-классовой классификации).
        \item Нейроны каждого слоя соединены с нейронами следующего слоя (\textbf{полносвязная сеть}, Fully Connected).
    \end{itemize}
    \textbf{Аналогия:} Это как комитет экспертов. Первая группа (входной слой) просто получает факты. Затем они передают свои выводы следующей группе (скрытый слой), которая их анализирует глубже. Те, в свою очередь, передают дальше... Пока финальная группа (выходной слой) не вынесет итоговое решение.
\end{myblock}

% --- VI.B: Функции активации ---
\subsection{VI.B Функции Активации: Придаем Нелинейность}

\begin{alerttextbox}{Зачем нужны функции активации?}
    Ключевая роль функций активации — \textbf{внести нелинейность} в модель.
    \begin{itemize}
        \item Без нелинейных функций активации вся нейросеть (даже с многими слоями) была бы эквивалентна одному линейному преобразованию (просто большой линейной регрессии).
        \item Нелинейность позволяет сети изучать сложные, нелинейные зависимости в данных.
    \end{itemize}
    \textbf{Аналогия:} Если у вас есть только палки (линейные функции), как бы вы их ни складывали, вы все равно получите палку (другую линейную функцию). Чтобы построить что-то сложное, вам нужны «изгибы» и «углы» (нелинейные функции).
\end{alerttextbox}

\begin{myexampleblock}{Популярные Функции Активации}
    \begin{itemize}
        \item \textbf{ReLU (Rectified Linear Unit):}
            \[ f(x) = \max(0, x) \]
            \textbf{Плюсы:} Вычислительно простая, помогает бороться с проблемой \textbf{затухания градиентов} (vanishing gradients) для положительных значений. Самая популярная в скрытых слоях.
            \textbf{Минусы:} «Умирающие ReLU» (нейроны, которые всегда выдают 0 и не активируются).
            \textbf{Аналогия:} Простой переключатель — пропускает сигнал, если он положительный, и блокирует, если отрицательный.

        \item \textbf{Sigmoid (Сигмоида):}
            \[ f(x) = \frac{1}{1 + e^{-x}} \]
            \textbf{Плюсы:} Сжимает выходные значения в диапазон [0, 1], что удобно для интерпретации как вероятности (например, в выходном слое бинарной классификации).
            \textbf{Минусы:} Сильно страдает от затухания градиентов при значениях $x$, далеких от 0. Выход не центрирован около нуля.
            \textbf{Аналогия:} Регулятор, который плавно переводит любой входной сигнал в значение между 0 и 1.

        \item \textbf{Tanh (Гиперболический тангенс):}
            \[ f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]
            \textbf{Плюсы:} Выход центрирован около нуля (диапазон [-1, 1]), что может ускорять сходимость.
            \textbf{Минусы:} Также страдает от затухания градиентов, хотя и меньше, чем Sigmoid.
            \textbf{Аналогия:} Похож на Sigmoid, но выдает значения от -1 до 1.
    \end{itemize}
    \textbf{Другие важные:} Leaky ReLU, ELU.
\end{myexampleblock}

% ИСПРАВЛЕНО: Заголовок вынесен из аргумента myblock и вставлен вручную внутрь.
\begin{myblock}{} %<-- Пустой аргумент заголовка
    {\noindent\small\bfseries\color{mDarkTeal}Графики функций активации (ReLU, Сигмоида, Tanh)\par\vspace{1ex}} %<-- Ручной заголовок
    % Убедитесь, что стиль cheatsheet plot style определен в вашем шаблоне cheatsheet-template.tex
    \begin{center}
    \begin{tikzpicture}
        \begin{axis}[
            cheatsheet plot style, % Используем стиль из шаблона
            title={Функции активации}, % Заголовок *внутри* графика остается
            xlabel={$x$},
            ylabel={$f(x)$},
            xmin=-5, xmax=5,
            ymin=-1.5, ymax=2, % Немного расширим Y для Tanh
            axis lines=middle, % Оси через центр
            legend pos=outer north east, % Легенда снаружи
            samples=100 % Больше точек для гладкости
        ]
        % ReLU
        \addplot [blue, thick, domain=-5:5] {max(0, x)};
        \addlegendentry{ReLU: $\max(0, x)$}

        % Sigmoid
        \addplot [alert, thick, domain=-5:5] {1 / (1 + exp(-x))};
        \addlegendentry{Sigmoid: $1 / (1 + e^{-x})$}

        % Tanh
        \addplot [w3schools, thick, domain=-5:5] {tanh(x)};
        \addlegendentry{Tanh: $\tanh(x)$}

        \end{axis}
    \end{tikzpicture}
    \end{center}
\end{myblock}

% --- VI.C: Backpropagation ---
\subsection{VI.C Backpropagation: Как Сеть Учится на Ошибках}

\begin{alerttextbox}{Метод Обратного Распространения Ошибки (Backpropagation)}
    \textbf{Backpropagation} — это основной алгоритм для обучения нейронных сетей. Он позволяет эффективно вычислить, как нужно изменить каждый вес в сети, чтобы уменьшить общую ошибку предсказания.

    \textbf{Общая идея (2 этапа):}
    \begin{enumerate}
        \item \textbf{Прямой проход (Forward Pass):} Данные проходят через сеть от входа к выходу. Сеть делает предсказание. Вычисляется \textbf{функция потерь} (Loss Function), которая показывает, насколько предсказание отличается от истинного значения.
        \item \textbf{Обратный проход (Backward Pass):}
            \begin{itemize}
                \item Ошибка «распространяется» обратно по сети, от выхода ко входу.
                \item На каждом шаге вычисляется \textbf{градиент} функции потерь по весам данного слоя (используя \textbf{цепное правило / chain rule} из матанализа). Градиент показывает направление и «силу» необходимого изменения веса для уменьшения ошибки.
                \item Веса обновляются с помощью \textbf{оптимизатора} (например, SGD, Adam) на основе вычисленных градиентов.
            \end{itemize}
    \end{enumerate}
    \textbf{Аналогия:} Представьте «разбор полетов» после командного задания. Сначала команда выполняет задание (прямой проход), затем оценивается результат и ошибка (функция потерь). Потом руководитель (алгоритм) идет «обратно» по цепочке выполнения, выясняя вклад каждого участника (веса) в общую ошибку (градиент), и дает указания, как каждому исправиться (обновление весов), чтобы в следующий раз результат был лучше.
\end{alerttextbox}

% --- VI.D: Оптимизаторы ---
\subsection{VI.D Оптимизаторы: Навигаторы в Пространстве Ошибок}

\begin{myblock}{Алгоритмы Оптимизации}
    \textbf{Оптимизатор} — это алгоритм, который использует градиенты, посчитанные с помощью Backpropagation, чтобы обновить веса нейросети с целью минимизации функции потерь.

    \textbf{Популярные оптимизаторы:}
    \begin{itemize}
        \item \textbf{SGD (Stochastic Gradient Descent)}: Стохастический Градиентный Спуск.
            \begin{itemize}
                \item Базовый и простой.
                \item Обновляет веса на основе градиента, вычисленного по небольшой случайной выборке (\textbf{батчу / batch}) данных.
                \item \textbf{Аналогия:} Спуск с горы в тумане. Вы делаете небольшой шаг в направлении уклона прямо под вашими ногами. Быстро, но можно застрять в яме (локальный минимум) или «скакать» по склону из-за шумных оценок градиента.
                \item Часто используется с \textbf{Momentum} (учет предыдущего шага, как бы «инерция») или \textbf{Nesterov Momentum}.
            \end{itemize}
        \item \textbf{Adam (Adaptive Moment Estimation)}: Адаптивная Оценка Моментов.
            \begin{itemize}
                \item Более продвинутый и часто используемый по умолчанию.
                \item Адаптирует \textbf{скорость обучения (learning rate)} для каждого веса индивидуально. \textit{Часто хорошо работает «из коробки» и является хорошей отправной точкой.}
                \item Учитывает и «инерцию» (как Momentum), и масштабирование градиентов (как RMSprop).
                \item \textbf{Аналогия:} Продвинутый альпинист с навигатором. Он не просто идет вниз, а учитывает свою прошлую скорость, крутизну склона в разных направлениях и подстраивает длину шага для каждого направления, чтобы спускаться эффективнее и не застревать.
            \end{itemize}
    \end{itemize}
    \textbf{Другие:} Adagrad, RMSprop, Adadelta. Выбор оптимизатора и его параметров (особенно learning rate) — важные гиперпараметры.
\end{myblock}

% --- VI.E: Dropout, BatchNorm ---
\subsection{VI.E Dropout и BatchNorm: Стабилизация и Регуляризация}

\begin{myexampleblock}{Техники для Улучшения Обучения}
    Эти техники помогают сделать обучение более стабильным, быстрым и предотвратить переобучение.

    \begin{itemize}
        \item \textbf{Dropout (Прореживание)}:
            \begin{itemize}
                \item \textbf{Что делает:} Во время \textit{обучения} на каждом шаге случайным образом «выключает» (обнуляет выход) часть нейронов скрытого слоя с некоторой вероятностью $p$. На этапе \textit{тестирования/предсказания} все нейроны используются, но их выходы масштабируются.
                \item \textbf{Зачем нужен:} Это мощный метод \textbf{регуляризации} для борьбы с \textbf{переобучением (overfitting)}. Он заставляет сеть не полагаться слишком сильно на отдельные нейроны, а учить более робастные признаки.
                \item \textbf{Аналогия:} Тренировка баскетбольной команды, где на каждой тренировке случайные игроки сидят на скамейке запасных. Это заставляет оставшихся игроков лучше взаимодействовать и развивать разные тактики, не надеясь только на одного «звездного» игрока.
            \end{itemize}

        \item \textbf{Batch Normalization (BatchNorm, Пакетная Нормализация)}:
             \begin{itemize}
                 \item \textbf{Что делает:} Нормализует активации (\textit{обычно до применения функции активации}, но есть варианты и после) внутри сети по каждому \textbf{батчу} данных во время обучения (приводит к нулевому среднему и единичной дисперсии), а затем сдвигает и масштабирует с помощью обучаемых параметров $\gamma$ и $\beta$.
                 \item \textbf{Зачем нужен:}
                     \begin{itemize}
                         \item Стабилизирует и ускоряет процесс обучения (борется с проблемой \textbf{internal covariate shift} — изменением распределения входов слоев во время обучения).
                         \item Позволяет использовать более высокие скорости обучения (learning rates).
                         \item Обладает легким регуляризующим эффектом.
                     \end{itemize}
                 \item \textbf{Аналогия:} Представьте конвейер сборки сложного прибора. BatchNorm — это как станции калибровки и стандартизации деталей на разных этапах конвейера (\textit{перед} следующим этапом сборки). Это гарантирует, что на следующий этап детали поступают в ожидаемом виде, что делает весь процесс сборки более стабильным и быстрым.
             \end{itemize}
    \end{itemize}
\end{myexampleblock}

% --- VI.F: Очень кратко: CNN/RNN - для чего ---
\subsection{VI.F Специализированные Архитектуры: CNN и RNN}

\begin{textbox}{За Пределами MLP: Сети для Конкретных Данных}
    MLP хороши для табличных данных, но для данных со специфической структурой существуют более эффективные архитектуры:

    \begin{itemize}
        \item \textbf{CNN (Convolutional Neural Networks): Сверточные Нейронные Сети}
            \begin{itemize}
                \item \textbf{Для чего:} Идеальны для обработки данных с \textbf{пространственной структурой}, таких как \textbf{изображения} и видео.
                \item \textbf{Ключевые идеи:} Используют \textbf{сверточные слои} (convolutional layers) для обнаружения локальных признаков (грани, текстуры) с помощью обучаемых фильтров и \textbf{пулинг слои} (pooling layers) для уменьшения размерности и инвариантности к небольшим сдвигам.
                \item \textbf{Аналогия:} Поиск узоров на картинке с помощью маленькой «лупы» (фильтра), которая скользит по всему изображению и реагирует на определенные формы или текстуры.
            \end{itemize}

        \item \textbf{RNN (Recurrent Neural Networks): Рекуррентные Нейронные Сети}
            \begin{itemize}
                \item \textbf{Для чего:} Созданы для обработки \textbf{последовательных данных}, где важен порядок элементов, например, \textbf{текст}, \textbf{временные ряды}, речь.
                \item \textbf{Ключевые идеи:} Имеют «память» — внутреннее состояние, которое обновляется на каждом шаге последовательности и влияет на обработку следующего элемента.
                \item \textbf{Аналогия:} Чтение книги. Чтобы понять смысл текущего предложения, вам нужно помнить, о чем говорилось в предыдущих. RNN пытается делать то же самое с последовательностями данных.
                \item \textbf{Важно:} Классические RNN имеют проблемы с долгими зависимостями (затухание/взрыв градиента). На практике почти всегда используют их продвинутые варианты: \textbf{LSTM (Long Short-Term Memory)} и \textbf{GRU (Gated Recurrent Unit)}, которые используют специальные \textit{гейты} (gates) для контроля потока информации и памяти.
            \end{itemize}
    \end{itemize}
\end{textbox}

% --- Конец контента ---