% >>> Обновленный Контент для шпаргалки "Введение в Нейронные Сети (NN)" v3.1 (с улучшенной структурой)
% ================================================
% Секция VI: Введение в Нейронные Сети (NN)
% ================================================
\section{VI. Введение в Нейронные Сети (NN)}

\begin{textbox}{Цель раздела}
    Понять базовые компоненты нейронных сетей (нейроны, слои, функции активации), основной механизм обучения (Backpropagation) и методы его улучшения (оптимизаторы, регуляризация, нормализация). Заложить основу для понимания сверточных и рекуррентных сетей.
\end{textbox}

% --- VI.A: Базовые Структуры ---
\subsection{A Базовые Структуры: Нейроны и Слои}

\begin{myblock}{Искусственный Нейрон: Определение}
    \textbf{Что это:} Математическая модель, имитирующая базовую функцию биологического нейрона. Служит основным вычислительным элементом нейросети.
\end{myblock}

\begin{myexampleblock}{Искусственный Нейрон: Принцип Работы}
    \textbf{Шаги вычисления:}
    \begin{enumerate}[nosep, itemsep=0.5ex]
        \item Принимает входы ($x_i$).
        \item Умножает каждый вход на его \textbf{вес} ($w_i$) $\rightarrow$ $w_i x_i$.
        \item Суммирует взвешенные входы $\rightarrow$ $z_{sum} = \sum_{i} w_i x_i$.
        \item Добавляет \textbf{смещение} ($b$) $\rightarrow$ $z = z_{sum} + b$.
        \item Результат $z$ (\textbf{пред-активация} или \textbf{логит}) пропускается через \textbf{функцию активации} $f(\cdot)$.
        \item Выход нейрона $\rightarrow$ $a = f(z)$ (\textbf{активация}).
    \end{enumerate}
    \textbf{Обучаемые параметры:} Веса $w_i$ и смещение $b$.
\end{myexampleblock}

\begin{myblock}{Многослойный Перцептрон (MLP): Архитектура}
    \textbf{Что это:} Классическая нейросеть из нескольких последовательных слоев нейронов.
    \textbf{Слои:}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Входной (Input):} Принимает признаки $X$. Не содержит вычислительных нейронов.
        \item \textbf{Скрытые (Hidden):} Один или более. Здесь происходит основная обработка, извлечение нелинейных паттернов.
        \item \textbf{Выходной (Output):} Формирует итоговый результат. Структура зависит от задачи (1 нейрон/линейная активация для регрессии, 1 нейрон/сигмоида для бинарной клас., N нейронов/Softmax для многоклассовой).
    \end{itemize}
    \textbf{Связи:} Обычно \textbf{полносвязные} (Dense / Fully Connected) — каждый нейрон слоя связан с каждым нейроном следующего слоя.
\end{myblock}

% --- VI.B: Функции активации ---
\subsection{B Функции Активации: Внесение Нелинейности}

\begin{alerttextbox}{Зачем нужна Нелинейность?}
    Без нелинейных функций активации ($f$) в скрытых слоях вся нейросеть (даже глубокая) была бы математически эквивалентна одному линейному слою (т.е., простой линейной или логистической регрессии). Нелинейность позволяет сети изучать сложные, нелинейные зависимости в данных.
\end{alerttextbox}

% --- ReLU и его вариации ---
\subsubsection{ReLU и его вариации}
\begin{myexampleblock}{ReLU (Rectified Linear Unit)}
    \[ f(x) = \max(0, x) \]
    \textbf{Свойства:} Вычислительно очень проста. Не "насыщается" для $x>0$ (производная = 1), что помогает градиентам проходить через глубокие сети.
    \textbf{Недостаток:} "Умирающие ReLU" (Dying ReLU) — если вход нейрона стабильно $ \le 0$, градиент через него перестает проходить, и нейрон перестает обучаться.
    \textbf{Использование:} Стандартный выбор для скрытых слоев в большинстве современных архитектур.
\end{myexampleblock}

\begin{myexampleblock}{Leaky ReLU}
    \[ f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \le 0 \end{cases} \quad (\alpha \text{ - малый коэфф., e.g., } 0.01) \]
    \textbf{Свойства:} Решает проблему "умирающих ReLU", позволяя небольшому градиенту ($\alpha$) проходить при $x \le 0$.
\end{myexampleblock}

\begin{myexampleblock}{ELU (Exponential Linear Unit)}
     \[ f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha (e^x - 1) & \text{if } x \le 0 \end{cases} \quad (\alpha > 0, \text{ часто } 1) \]
    \textbf{Свойства:} Похожа на Leaky ReLU, но использует экспоненту. Может давать более гладкие градиенты и отрицательные выходы, что иногда полезно.
\end{myexampleblock}

% --- Сигмоида и Тангенс ---
\subsubsection{Sigmoid и Tanh}
\begin{myexampleblock}{Sigmoid (Сигмоида)}
    \[ f(x) = \frac{1}{1 + e^{-x}} \]
    \textbf{Свойства:} Сжимает выход в диапазон [0, 1], удобен для интерпретации как вероятность.
    \textbf{Недостатки:} Сильно "насыщается" при больших $|x|$ (градиент близок к 0) $\rightarrow$ проблема \textbf{затухания градиентов}. Выход не центрирован около нуля.
    \textbf{Использование:} \textbf{Только} выходной слой для \textbf{бинарной классификации}. Избегать в скрытых слоях.
\end{myexampleblock}

\begin{myexampleblock}{Tanh (Гиперболический тангенс)}
    \[ f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]
    \textbf{Свойства:} Сжимает выход в диапазон [-1, 1], выход центрирован около нуля (лучше Sigmoid для скрытых слоев).
    \textbf{Недостатки:} Также страдает от затухания градиентов, хотя и меньше, чем Sigmoid.
    \textbf{Использование:} Иногда в скрытых слоях, часто в ячейках RNN/LSTM/GRU.
\end{myexampleblock}

% --- Softmax ---
\subsubsection{Softmax}
\begin{myexampleblock}{Softmax}
    \[ f(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{N} e^{x_j}} \quad (\text{для } i=1..N \text{ нейронов}) \]
    \textbf{Свойства:} Преобразует вектор "сырых" оценок (логитов $x_i$) в вектор вероятностей (все $f(x_i) \ge 0$ и $\sum f(x_i) = 1$).
    \textbf{Использование:} \textbf{Только} в выходном слое для \textbf{многоклассовой классификации}.
\end{myexampleblock}

% --- График ---
\begin{myblock}{}
    {\noindent\small\bfseries\color{mDarkTeal}Графики популярных функций активации\par\vspace{1ex}}
    \begin{center}
    \begin{tikzpicture}
        \begin{axis}[
            cheatsheet plot style,
            title={Функции активации},
            xlabel={$x$ (вход нейрона, $z$)},
            ylabel={$f(x)$ (выход нейрона, $a$)},
            xmin=-4, xmax=4,
            ymin=-1.5, ymax=2.5,
            axis lines=middle,
            legend style={at={(1.05,1)}, anchor=north west},
            samples=100
        ]
        \addplot [blue, thick, domain=-4:4] {max(0, x)}; \addlegendentry{ReLU}
        \addplot [alert, thick, domain=-4:4] {1 / (1 + exp(-x))}; \addlegendentry{Sigmoid}
        \addplot [w3schools, thick, domain=-4:4] {tanh(x)}; \addlegendentry{Tanh}
        \addplot [mDarkTeal, dashed, thick, domain=-4:4] { (x > 0) * x + (x <= 0) * 0.1 * x }; \addlegendentry{Leaky ReLU ($\alpha=0.1$)}
        \addplot [orange, dotted, thick, domain=-4:4] { (x > 0) * x + (x <= 0) * 1 * (exp(x) - 1) }; \addlegendentry{ELU ($\alpha=1$)}
        \end{axis}
    \end{tikzpicture}
    \end{center}
\end{myblock}

% --- Проблема Градиентов ---
\subsubsection{Проблема Затухания/Взрыва Градиентов}
\begin{alerttextbox}{Определение Проблемы (Vanishing/Exploding Gradients)}
    \textbf{Проблема:} При обучении глубоких сетей (много слоев), во время обратного прохода (Backpropagation) градиенты, передаваемые от слоя к слою, могут либо экспоненциально уменьшаться (\textbf{затухание}, vanishing), становясь близкими к нулю, либо экспоненциально расти (\textbf{взрыв}, exploding).
    \textbf{Причины:} Повторное умножение на веса и производные функций активации (особенно насыщающихся).
\end{alerttextbox}

\begin{myblock}{Последствия Проблемы Градиентов}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Затухание:} Ранние слои сети почти не обучаются, так как до них не доходит "сигнал" ошибки. Обучение очень медленное или останавливается.
        \item \textbf{Взрыв:} Большие изменения весов приводят к нестабильности обучения, расходящимся значениям функции потерь (NaN).
    \end{itemize}
\end{myblock}

\begin{myexampleblock}{Решения Проблемы Градиентов}
    \begin{itemize}[nosep, leftmargin=*]
        \item Использование ненасыщающихся активаций (ReLU и его варианты).
        \item Batch Normalization.
        \item Правильная инициализация весов (Xavier/Glorot, He).
        \item Residual Connections (проброс связей в обход слоев, как в ResNet).
        \item Обрезание градиентов (Gradient Clipping) - в основном для борьбы со взрывом.
    \end{itemize}
\end{myexampleblock}

% --- VI.C: Backpropagation ---
\subsection{C Backpropagation: Как Сеть Учится}

\begin{alerttextbox}{Backpropagation: Ключевой Алгоритм Обучения}
    \textbf{Цель:} Эффективно вычислить \textbf{градиенты} (частные производные) функции потерь $J$ по \textit{всем} обучаемым параметрам ($W_l, b_l$ каждого слоя $l$). Градиент $\partial J / \partial w$ показывает, насколько сильно и в каком направлении изменится итоговая ошибка $J$, если немного изменить вес $w$.
\end{alerttextbox}

\begin{myblock}{Этап 1: Прямой Проход (Forward Pass)}
    \textbf{Что происходит:} Данные $X$ проходят через сеть слой за слоем от входа к выходу. На каждом слое $l$ вычисляются пред-активационные значения $z_l = W_l a_{l-1} + b_l$ и активации $a_l = f_l(z_l)$. Промежуточные $z_l$ и $a_l$ сохраняются.
    \textbf{Результат:} Итоговые предсказания сети $a_L = \hat{y}$.
    \textbf{Затем:} Вычисляется \textbf{функция потерь} $J(\hat{y}, y)$, измеряющая ошибку предсказания.
\end{myblock}

\begin{myblock}{Этап 2: Обратный Проход (Backward Pass) - Идея}
    \textbf{Задача:} Распространить ошибку $J$ обратно через сеть для вычисления градиентов $\partial J / \partial W_l$ и $\partial J / \partial b_l$.
    \textbf{Метод:} Применение \textbf{цепного правила (chain rule)} дифференцирования для вычисления производной сложной функции (функции потерь $J$, которая зависит от всех весов и смещений через цепочку вычислений).
\end{myblock}

\begin{myexampleblock}{Этап 2: Обратный Проход (Backward Pass) - Механика}
    \textbf{Как работает (от слоя $L$ к слою 1):}
    \begin{enumerate}[nosep, itemsep=0.5ex]
        \item \textbf{Начало (выходной слой L):} Вычисляется $\partial J / \partial a_L$ (из производной функции потерь) и $\partial J / \partial z_L = (\partial J / \partial a_L) \odot f'_L(z_L)$.
        \item \textbf{Вычисление градиентов для слоя $l$:} Зная $\partial J / \partial z_l$, вычисляем:
            \begin{itemize}[label=\textbullet, nosep, leftmargin=*]
                \item $\partial J / \partial W_l = (\partial J / \partial z_l) \cdot a_{l-1}^T$
                \item $\partial J / \partial b_l = \sum (\partial J / \partial z_l)$ (суммирование по батчу/примерам)
            \end{itemize}
        \item \textbf{Передача ошибки на предыдущий слой ($l-1$):} Вычисляем $\partial J / \partial a_{l-1} = W_l^T \cdot (\partial J / \partial z_l)$.
        \item \textbf{Переход к $z_{l-1}$:} Вычисляем $\partial J / \partial z_{l-1} = (\partial J / \partial a_{l-1}) \odot f'_{l-1}(z_{l-1})$.
        \item \textbf{Повторение:} Шаги 2-4 повторяются для всех слоев от $L-1$ до 1.
    \end{enumerate}
     (\textit{Примечание: $\odot$ обозначает поэлементное умножение (Hadamard product), $\cdot$ - матричное умножение}).
     \textbf{Результат:} Градиенты $\partial J / \partial W_l$ и $\partial J / \partial b_l$ для всех слоев $l$.
\end{myexampleblock}

% --- VI.D: Оптимизаторы ---
\subsection{D Оптимизаторы: Обновление Весов}

\begin{alerttextbox}{Роль Оптимизатора}
    Использует градиенты $\nabla J$, полученные от Backpropagation, для вычисления и применения обновлений к весам $\mathbf{w}$ и смещениям $\mathbf{b}$ сети, с целью минимизировать функцию потерь $J$. Определяет \textit{как именно} использовать градиент для шага.
\end{alerttextbox}

\begin{myblock}{Mini-batch Gradient Descent (MBGD)}
    \textbf{Подход:} Градиент $\nabla J_{batch}$ вычисляется (усредняется) по небольшой случайной подвыборке (\textbf{мини-батч}, mini-batch) данных.
    \textbf{Обновление:} $\mathbf{w} := \mathbf{w} - \alpha \cdot \nabla J_{batch}(\mathbf{w})$.
    \textbf{Преимущества:} Быстрее Batch GD, стабильнее SGD, позволяет использовать матричные операции.
    \textbf{Гиперпараметр:} Learning rate $\alpha$ (скорость обучения).
\end{myblock}

\begin{myblock}{Momentum}
    \textbf{Идея:} Добавить "инерцию" к шагу, учитывая предыдущее направление движения $v$.
    \textbf{Формула (идея):} $v_t = \beta v_{t-1} + \alpha \nabla J(\mathbf{w})$; $\mathbf{w} := \mathbf{w} - v_t$.
    \textbf{Гиперпараметры:} $\alpha$, $\beta$ (коэфф. момента, обычно ~0.9).
    \textbf{Польза:} Ускоряет сходимость в "оврагах" функции потерь, помогает преодолевать локальные минимумы и плато.
\end{myblock}

\begin{myblock}{AdaGrad (Adaptive Gradient)}
    \textbf{Идея:} Адаптивный learning rate \textit{для каждого параметра} отдельно. Уменьшает шаг для параметров с часто большими градиентами.
    \textbf{Механика:} Накапливает сумму квадратов \textit{всех} прошлых градиентов $G$. Делит learning rate $\alpha$ на $\sqrt{G + \epsilon}$.
    \textbf{Польза:} Хорош для разреженных данных.
    \textbf{Недостаток:} Learning rate может слишком быстро уменьшиться до нуля.
\end{myblock}

\begin{myblock}{RMSProp}
    \textbf{Идея:} Исправить проблему AdaGrad с затуханием шага.
    \textbf{Механика:} Использует \textbf{экспоненциальное скользящее среднее (EMA)} квадратов градиентов $E[g^2]$, "забывая" старые.
    \textbf{Формула (идея):} $E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)(\nabla J)^2$; $\Delta w = -\frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} \nabla J(w)$.
    \textbf{Гиперпараметры:} $\alpha$, $\gamma$ (коэфф. затухания EMA, ~0.9).
\end{myblock}

\begin{myblock}{Adam (Adaptive Moment Estimation)}
    \textbf{Идея:} Сочетает идеи Momentum (EMA градиентов $m$) и RMSProp (EMA квадратов градиентов $v$).
    \textbf{Механика:} Использует $m$ и $v$ для адаптивного шага. Включает коррекцию смещения $m$ и $v$ на начальных этапах.
    \textbf{Польза:} Часто самый эффективный оптимизатор по умолчанию.
    \textbf{Гиперпараметры:} $\alpha$, $\beta_1$ (для $m$, ~0.9), $\beta_2$ (для $v$, ~0.999).
\end{myblock}

% --- VI.E: Стабилизация и Регуляризация Обучения ---
\subsection{E Стабилизация и Регуляризация Обучения}

\subsubsection{Dropout}
\begin{myexampleblock}{Dropout: Определение и Цель}
    \textbf{Что это:} Метод \textbf{регуляризации} для борьбы с переобучением в нейросетях.
    \textbf{Идея:} Создание подобия ансамбля из множества "прореженных" подсетей.
\end{myexampleblock}

\begin{myexampleblock}{Dropout: Механизм Работы}
    \textbf{На обучении:} Перед каждым прямым проходом для каждого нейрона в слое (кроме выходного) с вероятностью $p$ (e.g., $p=0.5$) его выход искусственно \textbf{обнуляется} для данного прохода. Разные нейроны обнуляются на разных проходах.
    \textbf{На предсказании/тесте:} Используются \textbf{все} нейроны, но их выходы умножаются на $(1-p)$ (inverted dropout) или используется усреднение по сетям (менее практично).
    \textbf{Эффект:} Заставляет сеть учить более робастные и распределенные представления, не полагаясь на отдельные нейроны.
\end{myexampleblock}

\subsubsection{Batch Normalization}
\begin{myexampleblock}{Batch Normalization (BatchNorm): Определение и Цель}
    \textbf{Что это:} Техника для \textbf{стабилизации и ускорения} обучения глубоких сетей.
    \textbf{Цель:} Бороться с проблемой \textbf{Internal Covariate Shift} (изменение распределения входов слоев во время обучения из-за изменения параметров предыдущих слоев).
\end{myexampleblock}

\begin{myexampleblock}{BatchNorm: Работа на Обучении}
    \textbf{Процесс для входа $z$ слоя по батчу $B$:}
    \begin{enumerate}[nosep, itemsep=0.5ex]
        \item \textbf{Считаем статистику батча:} $\mu_B = \text{mean}(z)$, $\sigma^2_B = \text{variance}(z)$.
        \item \textbf{Нормализуем:} $\hat{z} = (z - \mu_B) / \sqrt{\sigma^2_B + \epsilon}$. ($\hat{z}$ имеет среднее ~0, дисперсию ~1 \textit{по батчу}).
        \item \textbf{Масштабируем и сдвигаем:} $y_{BN} = \gamma \hat{z} + \beta$. ($\gamma$, $\beta$ - \textbf{обучаемые} параметры).
        \item \textbf{Обновляем скользящие средние:} Параллельно обновляются $\mu_{run}, \sigma^2_{run}$ (оценка среднего и дисперсии по всему датасету через EMA).
    \end{enumerate}
\end{myexampleblock}

\begin{myexampleblock}{BatchNorm: Работа на Предсказании/Тесте}
    \textbf{Процесс:} Использует \textbf{сохраненные} скользящие статистики $\mu_{run}, \sigma^2_{run}$ и \textbf{обученные} параметры $\gamma, \beta$.
    \textbf{Формула:} $y_{BN} = \gamma \frac{z - \mu_{run}}{\sqrt{\sigma^2_{run} + \epsilon}} + \beta$.
\end{myexampleblock}

\begin{myexampleblock}{BatchNorm: Эффекты и Применение}
    \textbf{Эффект:}
    \begin{itemize}[nosep, leftmargin=*]
        \item Стабилизирует и значительно ускоряет обучение.
        \item Позволяет использовать больший learning rate.
        \item Действует как слабая форма регуляризации.
        \item Снижает чувствительность к инициализации весов.
    \end{itemize}
    \textbf{Применение:} Обычно вставляется \textit{между} линейным преобразованием ($W x + b$) и нелинейной активацией ($f$).
\end{myexampleblock}

% --- VI.F: Специализированные Архитектуры ---
\subsection{F Специализированные Архитектуры: CNN и RNN}

\begin{textbox}{Зачем нужны специализированные сети?}
    MLP (полносвязные сети) универсальны, но не учитывают структуру данных (пространственную, временную). Для таких данных CNN и RNN часто гораздо более эффективны и требуют меньше параметров, так как используют априорные знания о структуре через свои архитектурные особенности.
\end{textbox}

% --- CNN ---
\subsubsection{CNN (Convolutional Neural Networks) - Сети для "Зрения"}
\begin{myblock}{Ключевые Идеи CNN}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Локальность Связей:} Нейроны смотрят только на небольшую локальную область входа (\textbf{рецептивное поле}). Учитывает пространственную близость пикселей/элементов.
        \item \textbf{Общие Веса (Parameter Sharing):} Один и тот же набор весов (\textbf{фильтр}) используется для обработки разных частей входа $\rightarrow$ инвариантность к сдвигу, резкое снижение числа параметров.
        \item \textbf{Иерархия Признаков:} Слои учатся распознавать все более сложные паттерны, комбинируя выходы предыдущих слоев (грани $\rightarrow$ текстуры $\rightarrow$ части объектов $\rightarrow$ объекты).
    \end{itemize}
    \textbf{Основное Применение:} Изображения, видео, аудио (спектрограммы), иногда тексты (1D свёртки).
\end{myblock}

\begin{myexampleblock}{Сверточный Слой (Conv Layer)}
    \textbf{Задача:} Извлечение локальных признаков с помощью набора обучаемых \textbf{фильтров (ядер, kernels)}.
    \textbf{Принцип работы:} Каждый фильтр (маленькая матрица весов, e.g., 3x3) "скользит" по входу. В каждой позиции вычисляется скалярное произведение весов фильтра и соответствующего участка входа + смещение. Результаты для одного фильтра формируют \textbf{карту признаков (feature map)}.
    \textbf{Гиперпараметры:} Число фильтров (глубина выхода), размер фильтра (e.g., 3x3, 5x5), шаг (stride), заполнение (padding: 'same' для сохранения размера, 'valid' без заполнения).
    \textbf{После Conv:} Обычно следует функция активации (ReLU).
\end{myexampleblock}

\begin{myexampleblock}{Пулинг Слой (Pooling Layer)}
    \textbf{Задача:} Уменьшение пространственного размера карт признаков (downsampling).
    \textbf{Цели:}
    \begin{itemize}[nosep, leftmargin=*]
        \item Снижение вычислительной нагрузки и числа параметров в последующих слоях.
        \item Повышение робастности к малым сдвигам/искажениям входных данных (инвариантность).
    \end{itemize}
    \textbf{Принцип работы:} Применяет агрегирующую функцию к непересекающимся (обычно) окнам (e.g., 2x2).
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Max Pooling:} Выбирает максимальное значение в окне (сохраняет самые сильные активации признаков). Наиболее распространен.
        \item \textbf{Average Pooling:} Вычисляет среднее значение в окне.
    \end{itemize}
    \textbf{Гиперпараметры:} Тип пулинга, размер окна (pool size), шаг (stride, часто равен размеру окна).
\end{myexampleblock}

\begin{myblock}{Типичная Архитектура CNN}
    Часто состоит из чередующихся блоков свертки и пулинга, за которыми следуют полносвязные слои для классификации/регрессии:
    Вход $\rightarrow$ [Conv $\rightarrow$ Activation $\rightarrow$ (BatchNorm) $\rightarrow$ Pool]*N $\rightarrow$ Flatten $\rightarrow$ [Dense $\rightarrow$ Activation $\rightarrow$ (Dropout/BatchNorm)]*M $\rightarrow$ Выходной Dense Слой
    \textit{(N, M - количество блоков/слоев)}
\end{myblock}

% --- RNN ---
\subsubsection{RNN (Recurrent Neural Networks) - Сети для Последовательностей}
\begin{myblock}{Ключевые Идеи RNN}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Обработка последовательностей:} Предназначены для данных, где важен порядок элементов (текст, временные ряды).
        \item \textbf{Рекуррентная связь ("Память"):} Выход сети на шаге $t$ зависит не только от входа $x_t$, но и от информации с предыдущих шагов, хранящейся в \textbf{скрытом состоянии (hidden state)} $h_t$. Состояние $h_{t-1}$ передается на шаг $t$.
        \item \textbf{Общие Веса во Времени (Parameter Sharing):} Один и тот же набор весов используется для обработки каждого элемента последовательности $x_t$.
    \end{itemize}
    \textbf{Основное Применение:} Обработка естественного языка (NLP), анализ временных рядов, распознавание речи, генерация музыки.
\end{myblock}

\begin{myexampleblock}{Простая RNN Ячейка (Simple RNN / Elman RNN)}
    \textbf{Формула:} $h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$. Новое состояние $h_t$ вычисляется на основе текущего входа $x_t$ и предыдущего состояния $h_{t-1}$ с использованием функции активации $f$ (часто $tanh$).
    \textbf{Выход (опционально):} $y_t = g(W_{hy} h_t + b_y)$.
    \textbf{Основная Проблема:} Затухание/взрыв градиентов при обучении на длинных последовательностях $\rightarrow$ трудности с запоминанием долговременных зависимостей.
\end{myexampleblock}

\begin{myexampleblock}{LSTM Ячейка (Long Short-Term Memory)}
    \textbf{Цель:} Решить проблему градиентов RNN и улучшить долговременную память с помощью \textbf{гейтов} (управляющих механизмов на основе сигмоид).
    \textbf{Ключевые Компоненты:}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Состояние Ячейки ($C_t$):} Основной канал для хранения информации ("конвейер памяти"). Может передавать информацию почти без изменений.
        \item \textbf{Forget Gate ($f_t$):} Решает, какую информацию из $C_{t-1}$ нужно "забыть".
        \item \textbf{Input Gate ($i_t$):} Решает, какая новая информация из входа $x_t$ и $h_{t-1}$ будет сохранена в $C_t$. Состоит из двух частей: сигмоиды $i_t$ и tanh $\tilde{C}_t$ (кандидат на добавление).
        \item \textbf{Output Gate ($o_t$):} Решает, какая часть состояния ячейки $C_t$ будет выведена как скрытое состояние $h_t$.
    \end{itemize}
    \textbf{Результат:} Может эффективно хранить, читать и записывать информацию, управляя потоком данных через гейты.
\end{myexampleblock}

\begin{myexampleblock}{GRU Ячейка (Gated Recurrent Unit)}
    \textbf{Цель:} Упрощенная версия LSTM с меньшим числом параметров, также эффективно борется с проблемой градиентов.
    \textbf{Ключевые Компоненты:}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Update Gate ($z_t$):} Комбинирует Forget и Input гейты LSTM. Определяет, сколько информации из прошлого состояния $h_{t-1}$ сохранить и сколько добавить из нового кандидата $\tilde{h}_t$.
        \item \textbf{Reset Gate ($r_t$):} Определяет, насколько информация из прошлого состояния $h_{t-1}$ будет использоваться для вычисления нового кандидата $\tilde{h}_t$.
    \end{itemize}
    \textbf{Особенности:} Нет отдельного состояния ячейки $C_t$.
    \textbf{Результат:} Сравнимая с LSTM производительность на многих задачах при меньшей сложности.
\end{myexampleblock}

\begin{myblock}{Использование RNN/LSTM/GRU}
    Обрабатывают последовательность входов $x_1, ..., x_T$. Возможные сценарии использования выходов:
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Многие-к-одному (Many-to-one):} Используется только последний выход $y_T$ или состояние $h_T$ (e.g., классификация текста по всей последовательности).
        \item \textbf{Один-ко-многим (One-to-many):} Один вход $x_1$, генерация последовательности $y_1, ..., y_T$ (e.g., генерация описания по картинке).
        \item \textbf{Многие-ко-многим (Many-to-many, синхронный):} Для каждого $x_t$ генерируется $y_t$ (e.g., разметка частей речи в предложении).
        \item \textbf{Многие-ко-многим (Many-to-many, асинхронный):} Вся входная последовательность читается, затем генерируется выходная (seq-to-seq, e.g., машинный перевод).
    \end{itemize}
    Часто используется несколько слоев (Stacked RNN), где выход $h_t$ одного слоя является входом $x_t$ для следующего.
\end{myblock}

% --- Конец Секции VI ---

% --- Конец обновленного контента ---