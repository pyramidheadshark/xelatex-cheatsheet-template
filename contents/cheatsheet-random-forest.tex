% >>> Контент для шпаргалки "Случайный Лес (Random Forest)"

\begin{myblock}{Случайный Лес: Введение}
    \textbf{Случайный Лес (Random Forest, RF)} — это ансамблевый метод машинного обучения, который строит множество деревьев решений во время обучения и выводит класс, который является модой классов (классификация) или средним предсказанием (регрессия) отдельных деревьев. Это один из самых популярных и эффективных "из коробки" алгоритмов.
    
    \textit{Аналогия:} Представь, что тебе нужно принять важное решение. Вместо того чтобы спросить одного эксперта (одно дерево), ты собираешь \textbf{комитет разных экспертов} (много деревьев), каждый из которых смотрит на проблему немного под своим углом, а затем принимаешь решение на основе их коллективного мнения. Случайный лес делает то же самое, но с деревьями решений.
\end{myblock}

\section{Идея Бэггинга (Bagging)}

\begin{textbox}{Bagging = Bootstrap Aggregating}
    \textbf{Бэггинг} — это основной принцип, лежащий в основе Случайного Леса. Он состоит из двух шагов:
    \begin{itemize}
        \item \textbf{Bootstrap (Бутстрэп):} Создается множество (\(N\)) подвыборок из исходного обучающего датасета. Каждая подвыборка формируется путем случайного выбора объектов \textbf{с возвращением}. Это означает, что некоторые объекты могут попасть в одну подвыборку несколько раз, а некоторые — ни разу. Размер каждой подвыборки обычно равен размеру исходного датасета.
        \item \textbf{Aggregating (Агрегация):} На каждой подвыборке независимо обучается своя модель (в случае RF — дерево решений). Затем предсказания всех \(N\) моделей усредняются (для регрессии) или определяется самый популярный класс (для классификации — голосование большинством).
    \end{itemize}
    \textit{Цель бэггинга:} Снизить \textbf{дисперсию (variance)} модели. Индивидуальные деревья могут сильно переобучаться (высокая дисперсия), но усреднение их предсказаний сглаживает ошибки и делает итоговую модель более устойчивой.
    Объекты, не попавшие в конкретную бутстрэп-выборку (\(\approx 37\%\)), называются \textbf{Out-of-Bag (OOB)} и могут использоваться для оценки качества модели (OOB-оценка) без необходимости отдельной валидационной выборки.
\end{textbox}

\section{Случайный Выбор Признаков (Feature Subsampling)}

\begin{myexampleblock}{Дополнительная Случайность}
    В отличие от простого бэггинга деревьев, Случайный Лес вносит \textbf{дополнительный элемент случайности} при построении каждого дерева:
    \begin{itemize}
        \item При поиске лучшего разбиения (split) в каждом узле дерева, алгоритм рассматривает не все доступные признаки, а только их \textbf{случайное подмножество} (размер подмножества, \texttt{max\_features}, является гиперпараметром).
        \item Для задачи классификации обычно берут \(\sqrt{p}\) признаков, для регрессии — \(p/3\), где \(p\) — общее число признаков.
    \end{itemize}
    \textit{Зачем это нужно?} Это делается для \textbf{декорреляции} деревьев. Если бы все деревья видели все признаки, и был бы один очень сильный признак, большинство деревьев использовали бы его для первого разбиения. В результате деревья были бы очень похожи (скоррелированы), и усреднение не дало бы такого сильного эффекта снижения дисперсии. Случайный выбор признаков заставляет деревья быть более разнообразными.

    \textit{Аналогия:} Возвращаясь к комитету экспертов. Чтобы они не пришли к одному и тому же выводу, опираясь на самый очевидный факт, ты просишь каждого эксперта при анализе сосредоточиться только на \textbf{случайном наборе аспектов} проблемы. Это побуждает их исследовать разные стороны вопроса.
\end{myexampleblock}

\section{Как Уменьшает Дисперсию}

\begin{myblock}{Борьба с Переобучением через Усреднение}
    Ключевая сила Случайного Леса — в его способности значительно \textbf{уменьшать дисперсию} по сравнению с одним деревом решений, не сильно увеличивая (или даже немного уменьшая) \textbf{смещение (bias)}.
    \begin{itemize}
        \item \textbf{Одно дерево решений:} Имеет низкое смещение (может хорошо подогнаться под обучающие данные), но высокую дисперсию (сильно меняется при небольшом изменении данных, легко переобучается).
        \item \textbf{Случайный Лес:}
            \begin{itemize}
                \item \textbf{Бэггинг (усреднение):} Усреднение предсказаний \(N\) моделей, ошибки которых не полностью скоррелированы, приводит к снижению общей дисперсии ансамбля. Чем больше деревьев (\(N\)), тем ниже дисперсия (до определенного предела).
                \item \textbf{Случайный выбор признаков (декорреляция):} Уменьшает корреляцию между деревьями, что делает усреднение еще более эффективным для снижения дисперсии.
            \end{itemize}
    \end{itemize}
    В итоге, RF получает модель, которая все еще достаточно гибкая (относительно низкое смещение, унаследованное от деревьев), но гораздо более стабильная и устойчивая к переобучению (значительно сниженная дисперсия). Это классический пример улучшения модели через управление \textbf{компромиссом смещения-дисперсии (Bias-Variance Tradeoff)}.
    
    \textit{Аналогия "Мудрость Толпы":} Один человек может сильно ошибаться в оценке (высокая дисперсия), но если усреднить оценки большой группы людей (где ошибки случайны и не связаны), итоговая оценка будет гораздо ближе к истине (низкая дисперсия).
\end{myblock}

\section{Важность Признаков (Feature Importance)}

\begin{textbox}{Зачем Оценивать Важность?}
    Хотя Случайный Лес — это ансамбль, что усложняет прямую интерпретацию, он предоставляет методы для оценки вклада каждого признака в итоговый результат. Это помогает:
    \begin{itemize}
        \item Понять, какие данные действительно влияют на модель.
        \item Упростить модель через отбор признаков (Feature Selection).
        \item Получить инсайты о предметной области.
    \end{itemize}
    Существует два основных подхода:
\end{textbox}

\subsection{1. Mean Decrease in Impurity (MDI) / Gini Importance}

\begin{myblock}{Идея: Вклад в Чистоту Узлов}
    Этот метод оценивает важность признака на основе того, насколько сильно его использование для разделений в деревьях \textbf{уменьшает нечистоту (Impurity)} узлов (например, Gini Impurity для классификации или MSE для регрессии). Признак считается важным, если он часто выбирается для разделения и эти разделения значительно "очищают" данные. Расчет происходит \textbf{на обучающей выборке} во время построения леса.

    \textbf{Как считается (детально):}
    \begin{enumerate}
        \item Обучаем Random Forest.
        \item Для \textbf{каждого дерева} в лесу:
            \begin{itemize}
                \item Для \textbf{каждого внутреннего узла}, где произошло разделение по признаку $F$:
                \item Рассчитываем \textbf{уменьшение нечистоты (Information Gain или Variance Reduction)} в этом узле: \\ $\Delta Impurity_{node} = Impurity(parent) - WeightedImpurity(children)$.
                \item Умножаем это значение на долю объектов, прошедших через узел, относительно всех объектов ($N_{node} / N_{total}$), чтобы получить взвешенное уменьшение нечистоты для этого узла.
                % Примечание: Иногда просто суммируют невзвешенный IG или IG, взвешенный на N_node. Реализация может отличаться. Возьмем вариант с суммой взвешенных IG.
                % Более стандартный вариант - суммировать взвешенный IG: N_node * IG_node
                % Давайте используем его для ясности
                % Рассчитываем взвешенное уменьшение нечистоты: $WeightedDecrease_{node} = N_{node} \times \Delta Impurity_{node}$.
            \end{itemize}
        \item Для \textbf{каждого признака $F$}:
            \begin{itemize}
                \item Суммируем взвешенные уменьшения нечистоты ($\sum N_{node} \times \Delta Impurity_{node}$) по \textbf{всем узлам всех деревьев}, где признак $F$ использовался для разделения. Это дает "общую важность" $TotalImportance(F)$.
            \end{itemize}
        \item \textbf{Нормализация:} Общую важность каждого признака делят на сумму важностей всех признаков: \\ $Importance(F) = \frac{TotalImportance(F)}{\sum_{j} TotalImportance(F_j)}$. В итоге, сумма всех важностей равна 1.
    \end{enumerate}

    \textbf{Формула (концептуально):}
    \[
    Importance_{MDI}(F) \propto \sum_{\text{trees}} \sum_{\substack{\text{nodes split} \\ \text{on } F}} N_{\text{node}} \cdot \Delta Impurity_{\text{node}}
    \]

    \textbf{Плюсы:}
    \begin{itemize}
        \item \textbf{Быстро} считается (информация доступна сразу после обучения).
        \item Обычно предоставляется по умолчанию в библиотеках (например, \texttt{feature\_importances\_} в scikit-learn).
    \end{itemize}
    \textbf{Минусы:}
    \begin{itemize}
        \item Склонен \textbf{завышать важность} числовых признаков и категориальных признаков с большим количеством уникальных значений (высокой кардинальностью).
        \item Может давать \textbf{неадекватные результаты для скоррелированных признаков} (важность может "делиться" между ними или присваиваться только одному).
        \item Показывает, насколько признак был \textit{полезен для построения деревьев} на обучающих данных, но не обязательно, насколько он важен для \textit{предсказаний} на новых данных.
    \end{itemize}
\end{myblock}

\subsection{2. Mean Decrease in Accuracy (MDA) / Permutation Importance}

\begin{myexampleblock}{Идея: Влияние "Поломки" Признака на Качество}
    Этот метод оценивает важность признака, измеряя, насколько \textbf{ухудшится качество предсказания} модели (например, Accuracy, F1, R², MSE), если "сломать" связь между этим признаком и целевой переменной путем случайного перемешивания его значений. Расчет происходит \textbf{на отложенной (не обучающей!) выборке}.

    \textbf{Как считается (детально):}
    \begin{enumerate}
        \item Обучаем Random Forest.
        \item Выбираем \textbf{отложенную выборку} (Out-of-Bag, валидационную или тестовую).
        \item Рассчитываем \textbf{базовую метрику качества} $Score_{base}$ модели на этой выборке.
        \item Для \textbf{каждого признака $F$}:
            \begin{itemize}
                \item Создаем копию отложенной выборки.
                \item В этой копии \textbf{случайно перемешиваем значения} только в столбце признака $F$. Остальные столбцы остаются без изменений.
                \item Делаем предсказания модели на этой \textbf{модифицированной} выборке.
                \item Рассчитываем метрику качества $Score_{permuted}(F)$ на предсказаниях для перемешанной выборки.
                \item \textbf{Важность признака $F$} = $Score_{base} - Score_{permuted}(F)$.
            \end{itemize}
        \item (Опционально, для стабильности) Повторяем шаг 4 несколько раз с разными случайными перемешиваниями для каждого признака и усредняем полученные значения важности.
    \end{enumerate}

    \textbf{Формула (концептуально):}
    \[
    Importance_{Permutation}(F) = Score_{base} - \mathbb{E}[Score_{permuted}(F)]
    \]
    где $\mathbb{E}[\cdot]$ означает ожидаемое значение по разным перемешиваниям.

    \textbf{Плюсы:}
    \begin{itemize}
        \item Более \textbf{надежен}, чем MDI, особенно при наличии скоррелированных признаков (хотя интерпретация требует осторожности).
        \item Напрямую измеряет влияние признака на \textbf{предсказательную способность} модели на новых данных.
        \item Идея метода \textbf{модель-агностична} (можно применять к любой модели, не только RF).
    \end{itemize}
    \textbf{Минусы:}
    \begin{itemize}
        \item \textbf{Вычислительно затратен} (требует многократных предсказаний модели).
        \item Результат может зависеть от конкретной отложенной выборки и случайности перемешивания (рекомендуется усреднять по нескольким запускам).
        \item Интерпретация при \textbf{сильно скоррелированных признаках} сложна: удаление одного может не сильно влиять на метрику, если модель использует его "заменитель". Может занизить важность обоих.
    \end{itemize}
\end{myexampleblock}

\subsection{Сравнение MDI и Permutation Importance}

\begin{alerttextbox}{Ключевые Различия (Частый Вопрос на Собеседованиях)}
    \begin{center}
    \begin{tabular}{|l|p{5.5cm}|p{5.5cm}|}
        \hline
        \textbf{Характеристика} & \textbf{MDI (Gini Importance)} & \textbf{Permutation Importance (MDA)} \\
        \hline
        \textbf{Что измеряет?} & Насколько признак \textbf{использовался} для уменьшения нечистоты узлов при \textbf{обучении}. & Насколько "поломка" признака \textbf{влияет} на \textbf{качество предсказания} на \textbf{новых} данных. \\
        \hline
        \textbf{На каких данных?} & Обучающая выборка & Отложенная выборка (OOB, validation, test) \\
        \hline
        \textbf{Скорость} & Быстро & Медленно \\
        \hline
        \textbf{Надежность} & Менее надежен, предвзят к типу признаков & Более надежен как показатель предсказательной силы \\
        \hline
        \textbf{Скоррел. признаки} & Может "делить", завышать/занижать важность & Может занижать важность обоих \\
        \hline
        \textbf{Модель-агностичность} & Специфичен для деревьев & Идея применима ко многим моделям \\
        \hline
        \textbf{Основное Применение} & Быстрый анализ, оценка по умолчанию & Надежная оценка влияния, отбор признаков \\
        \hline
    \end{tabular}
    \end{center}
    \textbf{Вывод:} Permutation Importance обычно считается более надежным показателем реальной важности признака для \textbf{производительности} модели.
\end{alerttextbox}

\begin{alerttextbox}{Что Спрашивают на Собеседованиях}
    Часто спрашивают разницу между MDI и Permutation Importance. Важно понимать:
    \begin{itemize}
        \item MDI измеряет, насколько признак \textbf{использовался} деревьями при построении (на основе обучающей выборки).
        \item Permutation Importance измеряет, насколько признак \textbf{влияет} на итоговое качество предсказания модели (на основе отложенной выборки). Permutation Importance обычно считается более надежным показателем реальной важности.
    \end{itemize}
\end{alerttextbox}

\section{Ключевые Гиперпараметры}
Основные параметры для настройки Случайного Леса:
\begin{itemize}
    \item \textbf{\texttt{n\_estimators}:} Количество деревьев в лесу. Чем больше, тем лучше (до некоторого плато), но дольше обучение. Обычно выбирают достаточно большим (100, 500, 1000+).
    \item \textbf{\texttt{max\_features}:} Количество признаков, рассматриваемых при поиске лучшего сплита в каждом узле. Ключевой параметр для контроля корреляции деревьев и борьбы с переобучением. Значения по умолчанию (\(\sqrt{p}\) / \(p/3\)) часто работают хорошо, но стоит подбирать.
    \item \textbf{Параметры деревьев:} Гиперпараметры базовых деревьев решений также влияют на лес (например, \texttt{max\_depth}, \texttt{min\_samples\_split}, \texttt{min\_samples\_leaf}). Часто оставляют деревья достаточно глубокими в RF, полагаясь на усреднение для борьбы с переобучением, но иногда их ограничение тоже помогает.
\end{itemize}

\section{Сравнение с Конкурентами}

\begin{myblock}{RF vs. Одно Дерево Решений}
    \begin{itemize}
        \item \textbf{Плюсы RF:}
            \begin{itemize}
                \item Значительно \textbf{меньше переобучается}, более \textbf{устойчив} (низкая дисперсия).
                \item Обычно \textbf{выше точность} и обобщающая способность.
            \end{itemize}
        \item \textbf{Минусы RF:}
            \begin{itemize}
                \item \textbf{Менее интерпретируем} ("черный ящик" по сравнению с одним деревом).
                \item Требует \textbf{больше ресурсов} для обучения и предсказания (память и время).
            \end{itemize}
    \end{itemize}
\end{myblock}

\begin{myblock}{RF vs. Линейные Модели (Логистическая/Линейная Регрессия)}
    \begin{itemize}
        \item \textbf{Плюсы RF:}
            \begin{itemize}
                \item Легко улавливает \textbf{нелинейные зависимости} и \textbf{взаимодействия} между признаками без необходимости их явного добавления (как в линейных моделях).
                \item \textbf{Не требует масштабирования} признаков (деревьям не важен масштаб).
                \item Менее чувствителен к \textbf{выбросам} (решающие правила деревьев устойчивы).
                \item Хорошо работает "из коробки" с минимальной настройкой.
            \end{itemize}
        \item \textbf{Минусы RF:}
            \begin{itemize}
                \item \textbf{Менее интерпретируем}, чем линейные модели (где можно смотреть на веса).
                \item Может быть \textbf{медленнее} в обучении и предсказании на очень больших данных.
                \item Плохо \textbf{экстраполирует} за пределы диапазона значений признаков, виденных в обучении (предсказание ограничено значениями в "листьях"). Линейные модели могут экстраполировать.
                \item Может требовать \textbf{больше памяти}.
            \end{itemize}
    \end{itemize}
\end{myblock}

% --- Конец контента ---