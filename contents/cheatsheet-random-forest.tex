% >>> Контент для шпаргалки "Случайный Лес (Random Forest)" - v2 (Разбивка)

% ================================================
% Введение в Случайный Лес
% ================================================
\begin{myblock}{Случайный Лес: Определение}
    \textbf{Случайный Лес (Random Forest, RF)} — это ансамблевый метод машинного обучения, который строит множество деревьев решений во время обучения и выводит класс, который является модой классов (классификация) или средним предсказанием (регрессия) отдельных деревьев. Это один из самых популярных и эффективных "из коробки" алгоритмов.
\end{myblock}

\begin{myexampleblock}{Аналогия: Комитет Экспертов}
    Представьте, что для принятия важного решения вы собираете \textbf{комитет разных экспертов} (много деревьев). Каждый эксперт смотрит на проблему немного под своим углом. Вы принимаете решение на основе их коллективного мнения. Случайный лес применяет тот же принцип, но с деревьями решений.
\end{myexampleblock}

% ================================================
% Секция I: Идея Бэггинга (Bagging)
% ================================================
\section{Идея Бэггинга (Bagging)}

\begin{textbox}{Определение: Bagging = Bootstrap Aggregating}
    \textbf{Бэггинг} — это основной принцип, лежащий в основе Случайного Леса. Он сочетает две ключевые идеи: Bootstrap и Aggregating.
\end{textbox}

\begin{myblock}{Шаг 1: Bootstrap (Бутстрэп)}
    Создается множество (\(N\)) подвыборок из исходного обучающего датасета. Каждая подвыборка формируется путем случайного выбора объектов \textbf{с возвращением}.
    \begin{itemize}[nosep, leftmargin=*]
        \item Некоторые объекты могут попасть в одну подвыборку несколько раз.
        \item Некоторые объекты могут не попасть ни разу.
        \item Размер каждой подвыборки обычно равен размеру исходного датасета.
    \end{itemize}
    Объекты, не попавшие в конкретную бутстрэп-выборку (\(\approx 37\%\)), называются \textbf{Out-of-Bag (OOB)}.
\end{myblock}

\begin{myblock}{Шаг 2: Aggregating (Агрегация)}
    На каждой Bootstrap-подвыборке независимо обучается своя модель (в случае RF — дерево решений). Затем предсказания всех \(N\) моделей агрегируются:
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Регрессия:} Усреднение предсказаний.
        \item \textbf{Классификация:} Голосование большинством (выбор самого популярного класса).
    \end{itemize}
\end{myblock}

\begin{textbox}{Цель Бэггинга и OOB-оценка}
    \textbf{Цель бэггинга:} Снизить \textbf{дисперсию (variance)} ансамблевой модели. Индивидуальные деревья могут иметь высокую дисперсию (переобучаться), но усреднение их предсказаний сглаживает ошибки и повышает устойчивость. \newline
    \textbf{OOB-оценка:} Объекты Out-of-Bag могут использоваться для оценки качества модели (OOB-оценка) без необходимости выделения отдельной валидационной выборки. Для каждого OOB-объекта предсказание делается ансамблем деревьев, которые \textit{не обучались} на этом объекте.
\end{textbox}

% ================================================
% Секция II: Случайный Выбор Признаков (Feature Subsampling)
% ================================================
\section{Случайный Выбор Признаков (Feature Subsampling)}

\begin{myblock}{Дополнительная Случайность в RF}
    В отличие от простого бэггинга деревьев, Случайный Лес вносит \textbf{дополнительный элемент случайности} при построении каждого дерева:
    \begin{itemize}[nosep, leftmargin=*]
        \item При поиске лучшего разбиения (split) в каждом узле дерева, алгоритм рассматривает не все доступные признаки, а только их \textbf{случайное подмножество}.
        \item Размер этого подмножества (\texttt{max\_features}) является важным гиперпараметром.
        \item Типичные значения \texttt{max\_features}: \(\sqrt{p}\) для классификации, \(p/3\) для регрессии (где \(p\) — общее число признаков).
    \end{itemize}
\end{myblock}

\begin{textbox}{Цель Случайного Выбора Признаков: Декорреляция Деревьев}
    \textit{Зачем это нужно?} Это делается для \textbf{декорреляции} деревьев в ансамбле.
    \begin{itemize}[nosep, leftmargin=*]
        \item Если бы все деревья видели все признаки, и существовал бы один очень сильный признак, большинство деревьев использовали бы его для первого (и, возможно, последующих) разбиений.
        \item В результате деревья были бы очень похожими (скоррелированными).
        \item Усреднение предсказаний сильно скоррелированных моделей не дает значительного снижения дисперсии.
    \end{itemize}
    Случайный выбор признаков заставляет разные деревья фокусироваться на разных наборах признаков, делая их более \textbf{разнообразными} и независимыми.
\end{textbox}

\begin{myexampleblock}{Аналогия: Разные Аспекты для Экспертов}
    Возвращаясь к комитету экспертов: чтобы они не пришли к одному выводу, опираясь на самый очевидный факт, вы просите каждого эксперта при анализе сосредоточиться только на \textbf{случайном наборе аспектов} проблемы. Это побуждает их исследовать разные стороны вопроса и дает более надежный коллективный результат.
\end{myexampleblock}

% ================================================
% Секция III: Как RF Уменьшает Дисперсию
% ================================================
\section{Как Уменьшает Дисперсию}

\begin{myblock}{Борьба с Переобучением через Усреднение}
    Ключевая сила Случайного Леса — в его способности значительно \textbf{уменьшать дисперсию} по сравнению с одним деревом решений, при этом не сильно увеличивая (или даже немного уменьшая) \textbf{смещение (bias)}.
\end{myblock}

\begin{myblock}{Сравнение Bias/Variance: Одно Дерево vs RF}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Одно дерево решений:}
            \begin{itemize}[label=\textbullet, nosep, leftmargin=*]
                \item \textit{Низкое смещение:} Может хорошо подогнаться под обучающие данные, уловить сложные зависимости.
                \item \textit{Высокая дисперсия:} Сильно меняется при небольшом изменении данных, легко переобучается.
            \end{itemize}
        \item \textbf{Случайный Лес (RF):}
            \begin{itemize}[label=\textbullet, nosep, leftmargin=*]
                \item \textit{Относительно низкое смещение:} Наследует гибкость от деревьев.
                \item \textit{Значительно сниженная дисперсия:} Благодаря усреднению и декорреляции.
            \end{itemize}
    \end{itemize}
\end{myblock}

\begin{textbox}{Механизмы Снижения Дисперсии в RF}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Бэггинг (усреднение):} Усреднение предсказаний \(N\) моделей, ошибки которых не полностью скоррелированы, приводит к снижению общей дисперсии ансамбля. Чем больше деревьев (\(N\)), тем ниже дисперсия (до определенного предела).
        \item \textbf{Случайный выбор признаков (декорреляция):} Уменьшает корреляцию между деревьями, что делает усреднение еще более эффективным для снижения дисперсии.
    \end{itemize}
    В итоге, RF достигает хорошего \textbf{компромисса смещения-дисперсии (Bias-Variance Tradeoff)}, создавая модель, которая одновременно гибкая и устойчивая к переобучению.
\end{textbox}

\begin{myexampleblock}{Аналогия: Мудрость Толпы}
    Один человек может сильно ошибаться в оценке (высокая дисперсия). Но если усреднить оценки большой группы людей, где ошибки случайны и не связаны, итоговая оценка будет гораздо ближе к истине (низкая дисперсия). RF использует похожий принцип.
\end{myexampleblock}

% ================================================
% Секция IV: Важность Признаков (Feature Importance)
% ================================================
\section{Важность Признаков (Feature Importance)}

\begin{textbox}{Зачем Оценивать Важность?}
    Хотя Случайный Лес — это ансамбль, что усложняет прямую интерпретацию, он предоставляет методы для оценки вклада каждого признака. Это помогает:
    \begin{itemize}[nosep, leftmargin=*]
        \item Понять, какие данные действительно влияют на модель.
        \item Упростить модель через отбор признаков (Feature Selection).
        \item Получить инсайты о предметной области.
    \end{itemize}
    Существует два основных подхода: MDI (Gini Importance) и Permutation Importance (MDA).
\end{textbox}

% --- IV.A: Mean Decrease in Impurity (MDI) / Gini Importance ---
\subsection{A Mean Decrease in Impurity (MDI) / Gini Importance}

\begin{myblock}{Идея MDI: Вклад в Чистоту Узлов}
    Этот метод оценивает важность признака на основе того, насколько сильно его использование для разделений в деревьях \textbf{уменьшает нечистоту (Impurity)} узлов (например, Gini Impurity для классификации или MSE для регрессии). Признак считается важным, если он часто выбирается для разделения и эти разделения значительно "очищают" данные. Расчет происходит \textbf{на обучающей выборке} во время построения леса.
\end{myblock}

\begin{myexampleblock}{Как Считается MDI (Детально)}
    \begin{enumerate}[nosep, wide, labelindent=0pt]
        \item Обучаем Random Forest.
        \item Для \textbf{каждого дерева} в лесу:
            \begin{itemize}[nosep, leftmargin=*]
                \item Для \textbf{каждого внутреннего узла}, где произошло разделение по признаку $F$:
                    \begin{itemize}[label=\textbullet, nosep, leftmargin=*]
                    \item Рассчитываем \textbf{уменьшение нечистоты (Information Gain / Variance Reduction)} в этом узле: $\Delta Impurity_{node} = Impurity(parent) - WeightedImpurity(children)$.
                    \item Рассчитываем \textbf{взвешенное уменьшение нечистоты}: $WeightedDecrease_{node} = N_{node} \times \Delta Impurity_{node}$, где $N_{node}$ — количество объектов в узле.
                    \end{itemize}
            \end{itemize}
        \item Для \textbf{каждого признака $F$}:
            \begin{itemize}[nosep, leftmargin=*]
                \item Суммируем взвешенные уменьшения нечистоты ($WeightedDecrease_{node}$) по \textbf{всем узлам всех деревьев}, где признак $F$ использовался для разделения. Это дает "общую важность" $TotalImportance(F)$.
            \end{itemize}
        \item \textbf{Нормализация:} Общую важность каждого признака делят на сумму важностей всех признаков: $Importance(F) = \frac{TotalImportance(F)}{\sum_{j} TotalImportance(F_j)}$.
    \end{enumerate}
    \textbf{Формула (концептуально):}
    \[
    Importance_{MDI}(F) \propto \sum_{\text{trees}} \sum_{\substack{\text{nodes split} \\ \text{on } F}} N_{\text{node}} \cdot \Delta Impurity_{\text{node}}
    \]
\end{myexampleblock}

\begin{myblock}{Плюсы MDI}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Быстро} считается (информация доступна сразу после обучения).
        \item Обычно предоставляется по умолчанию в библиотеках (например, \texttt{feature\_importances\_} в scikit-learn).
    \end{itemize}
\end{myblock}

\begin{alerttextbox}{Минусы и Предостережения по MDI}
    \begin{itemize}[nosep, leftmargin=*]
        \item Склонен \textbf{завышать важность} числовых признаков и категориальных признаков с большим количеством уникальных значений (высокой кардинальностью).
        \item Может давать \textbf{неадекватные результаты для скоррелированных признаков} (важность может "делиться" между ними или присваиваться только одному).
        \item Показывает, насколько признак был \textit{полезен для построения деревьев} на обучающих данных, но не обязательно, насколько он важен для \textit{предсказаний} на новых данных. \textbf{Использовать с осторожностью!}
    \end{itemize}
\end{alerttextbox}

% --- IV.B: Mean Decrease in Accuracy (MDA) / Permutation Importance ---
\subsection{B Mean Decrease in Accuracy (MDA) / Permutation Importance}

\begin{myblock}{Идея MDA: Влияние "Поломки" Признака на Качество}
    Этот метод оценивает важность признака, измеряя, насколько \textbf{ухудшится качество предсказания} модели (например, Accuracy, F1, R², MSE), если "сломать" связь между этим признаком и целевой переменной путем случайного перемешивания его значений. Расчет происходит \textbf{на отложенной (не обучающей!) выборке}.
\end{myblock}

\begin{myexampleblock}{Как Считается Permutation Importance (Детально)}
    \begin{enumerate}[nosep, wide, labelindent=0pt]
        \item Обучаем Random Forest.
        \item Выбираем \textbf{отложенную выборку} (OOB, валидационную или тестовую).
        \item Рассчитываем \textbf{базовую метрику качества} $Score_{base}$ модели на этой выборке.
        \item Для \textbf{каждого признака $F$}:
            \begin{itemize}[nosep, leftmargin=*]
                \item Создаем копию отложенной выборки.
                \item В этой копии \textbf{случайно перемешиваем значения} только в столбце признака $F$.
                \item Делаем предсказания модели на \textbf{модифицированной} выборке.
                \item Рассчитываем метрику качества $Score_{permuted}(F)$ на этих предсказаниях.
                \item \textbf{Важность признака $F$} = $Score_{base} - Score_{permuted}(F)$.
            \end{itemize}
        \item (Опционально, для стабильности) Повторяем шаг 4 несколько раз с разными случайными перемешиваниями для каждого признака и усредняем полученные значения важности.
    \end{enumerate}
    \textbf{Формула (концептуально):}
    \[
    Importance_{Permutation}(F) = Score_{base} - \mathbb{E}[Score_{permuted}(F)]
    \]
    где $\mathbb{E}[\cdot]$ означает ожидаемое значение по разным перемешиваниям.
\end{myexampleblock}

\begin{myblock}{Плюсы Permutation Importance}
    \begin{itemize}[nosep, leftmargin=*]
        \item Более \textbf{надежен}, чем MDI, как показатель реального влияния на предсказания.
        \item Напрямую измеряет влияние признака на \textbf{предсказательную способность} модели на новых данных.
        \item Идея метода \textbf{модель-агностична} (можно применять к любой обученной модели).
    \end{itemize}
\end{myblock}

\begin{alerttextbox}{Минусы и Предостережения по Permutation Importance}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Вычислительно затратен} (требует многократных предсказаний модели).
        \item Результат может зависеть от конкретной отложенной выборки и случайности перемешивания (рекомендуется усреднять по нескольким запускам).
        \item Интерпретация при \textbf{сильно скоррелированных признаках} требует осторожности: перемешивание одного признака может не сильно ухудшить метрику, если модель может использовать его коррелированный "заменитель". Это может привести к занижению важности обоих признаков.
    \end{itemize}
\end{alerttextbox}

% --- IV.C: Сравнение MDI и Permutation Importance ---
\subsection{C Сравнение MDI и Permutation Importance}

\begin{alerttextbox}{Ключевые Различия (Частый Вопрос на Собеседованиях)}
    \begin{center}
    \begin{tabular}{|l|p{5.5cm}|p{5.5cm}|}
        \hline
        \textbf{Характеристика} & \textbf{MDI (Gini Importance)} & \textbf{Permutation Importance (MDA)} \\
        \hline
        \textbf{Что измеряет?} & Насколько признак \textbf{использовался} для уменьшения нечистоты узлов при \textbf{обучении}. & Насколько "поломка" признака \textbf{влияет} на \textbf{качество предсказания} на \textbf{новых} данных. \\
        \hline
        \textbf{На каких данных?} & Обучающая выборка & Отложенная выборка (OOB, validation, test) \\
        \hline
        \textbf{Скорость} & Быстро & Медленно \\
        \hline
        \textbf{Надежность} & Менее надежен, предвзят к типу признаков, обучающей выборке & Более надежен как показатель предсказательной силы \\
        \hline
        \textbf{Скоррел. признаки} & Может "делить", завышать/занижать важность & Может занижать важность обоих \\
        \hline
        \textbf{Модель-агностичность} & Специфичен для деревьев & Идея применима ко многим моделям \\
        \hline
        \textbf{Основное Применение} & Быстрый анализ, оценка по умолчанию & Надежная оценка влияния, отбор признаков \\
        \hline
    \end{tabular}
    \end{center}
    \textbf{Ключевой вывод:} Permutation Importance обычно считается более надежным показателем реальной важности признака для \textbf{производительности} модели. MDI показывает "популярность" признака при построении модели.
\end{alerttextbox}

% ================================================
% Секция V: Ключевые Гиперпараметры
% ================================================
\section{Ключевые Гиперпараметры}

\begin{textbox}{Основные Параметры для Настройки RF}
    Хотя RF часто хорошо работает "из коробки", тюнинг гиперпараметров может улучшить результат. Важнейшие из них:
\end{textbox}

\begin{myblock}{\texttt{n\_estimators}}
    Количество деревьев в лесу.
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Влияние:} Больше деревьев -> ниже дисперсия ансамбля (до некоторого предела), стабильнее результат, но дольше обучение и предсказание.
        \item \textbf{Типичные значения:} 100, 500, 1000 и более. Обычно выбирают достаточно большим значением, пока производительность на валидации не перестанет расти или время обучения не станет чрезмерным.
    \end{itemize}
\end{myblock}

\begin{myblock}{\texttt{max\_features}}
    Количество признаков, случайно выбираемых для рассмотрения при поиске лучшего сплита в каждом узле.
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Влияние:} Ключевой параметр для контроля корреляции между деревьями. Меньшее значение -> более декоррелированные деревья -> большее снижение дисперсии, но потенциально большее смещение (каждое дерево слабее). Большее значение -> деревья более похожи -> меньшее снижение дисперсии, но потенциально меньшее смещение.
        \item \textbf{Типичные значения (и отправные точки):} \texttt{sqrt(p)} (классификация), \texttt{p/3} или \texttt{log2(p)} (регрессия). Часто требует подбора через кросс-валидацию.
    \end{itemize}
\end{myblock}

\begin{myblock}{Параметры Отдельных Деревьев}
    Гиперпараметры базовых деревьев решений также влияют на лес и могут использоваться для контроля сложности и предотвращения переобучения, хотя RF менее чувствителен к ним, чем одно дерево.
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{\texttt{max\_depth}:} Максимальная глубина деревьев. Ограничение глубины уменьшает сложность и дисперсию отдельных деревьев.
        \item \textbf{\texttt{min\_samples\_split}:} Минимальное количество объектов в узле для его разделения.
        \item \textbf{\texttt{min\_samples\_leaf}:} Минимальное количество объектов в листовом узле.
    \end{itemize}
    \textit{Стратегия:} Часто в RF деревья строят почти до максимальной глубины (e.g., \texttt{max\_depth=None}), полагаясь на усреднение и \texttt{max\_features} для контроля переобучения. Однако ограничение глубины или увеличение \texttt{min\_samples\_leaf}/\texttt{min\_samples\_split} может быть полезно для уменьшения размера модели и времени обучения, иногда даже улучшая качество.
\end{myblock}

% ================================================
% Секция VI: Сравнение с Конкурентами
% ================================================
\section{Сравнение с Конкурентами}

% --- VI.A: RF vs. Одно Дерево Решений ---
\subsection{A RF vs. Одно Дерево Решений}

\begin{myblock}{Преимущества RF перед Одним Деревом}
    \begin{itemize}[nosep, leftmargin=*]
        \item Значительно \textbf{меньше переобучается} благодаря усреднению и декорреляции.
        \item Гораздо \textbf{более устойчив} к изменениям в данных (низкая дисперсия).
        \item Обычно показывает \textbf{более высокую точность} и обобщающую способность на практике.
    \end{itemize}
\end{myblock}

\begin{myblock}{Недостатки RF перед Одним Деревом}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Менее интерпретируем.} Сложно понять логику принятия решения ансамбля по сравнению с одним деревом, которое можно визуализировать.
        \item Требует \textbf{больше вычислительных ресурсов} (память для хранения деревьев, время для обучения и предсказания).
    \end{itemize}
\end{myblock}

% --- VI.B: RF vs. Линейные Модели ---
\subsection{B RF vs. Линейные Модели (Логистическая/Линейная Регрессия)}

\begin{myblock}{Преимущества RF перед Линейными Моделями}
    \begin{itemize}[nosep, leftmargin=*]
        \item Легко улавливает \textbf{нелинейные зависимости} между признаками и целью.
        \item Автоматически обрабатывает \textbf{взаимодействия} между признаками.
        \item \textbf{Не требует масштабирования} признаков (решения в узлах основаны на порогах).
        \item Менее чувствителен к \textbf{выбросам} в признаках.
        \item Часто дает хорошее качество \textbf{"из коробки"} с минимальной предобработкой данных и настройкой.
    \end{itemize}
\end{myblock}

\begin{myblock}{Недостатки RF перед Линейными Моделями}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Менее интерпретируем}, чем линейные модели, где веса имеют ясный смысл (при условии корректной подготовки данных).
        \item Может быть \textbf{медленнее} в обучении и особенно в предсказании на очень больших датасетах или при большом количестве деревьев.
        \item Плохо \textbf{экстраполирует}. Предсказания RF ограничены диапазоном значений целевой переменной, виденных в обучающих данных (по сути, среднее по листьям). Линейные модели могут экстраполировать.
        \item Может требовать \textbf{значительно больше памяти}.
        \item На \textbf{очень разреженных данных} (много нулей, как в тексте) линейные модели часто работают лучше и быстрее.
    \end{itemize}
\end{myblock}

% --- Конец контента ---