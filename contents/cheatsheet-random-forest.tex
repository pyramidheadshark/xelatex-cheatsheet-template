% >>> Контент для шпаргалки "Случайный Лес (Random Forest)"

\begin{myblock}{Случайный Лес: Введение}
    \textbf{Случайный Лес (Random Forest, RF)} — это ансамблевый метод машинного обучения, который строит множество деревьев решений во время обучения и выводит класс, который является модой классов (классификация) или средним предсказанием (регрессия) отдельных деревьев. Это один из самых популярных и эффективных "из коробки" алгоритмов.
    
    \textit{Аналогия:} Представь, что тебе нужно принять важное решение. Вместо того чтобы спросить одного эксперта (одно дерево), ты собираешь \textbf{комитет разных экспертов} (много деревьев), каждый из которых смотрит на проблему немного под своим углом, а затем принимаешь решение на основе их коллективного мнения. Случайный лес делает то же самое, но с деревьями решений.
\end{myblock}

\section{Идея Бэггинга (Bagging)}

\begin{textbox}{Bagging = Bootstrap Aggregating}
    \textbf{Бэггинг} — это основной принцип, лежащий в основе Случайного Леса. Он состоит из двух шагов:
    \begin{itemize}
        \item \textbf{Bootstrap (Бутстрэп):} Создается множество (\(N\)) подвыборок из исходного обучающего датасета. Каждая подвыборка формируется путем случайного выбора объектов \textbf{с возвращением}. Это означает, что некоторые объекты могут попасть в одну подвыборку несколько раз, а некоторые — ни разу. Размер каждой подвыборки обычно равен размеру исходного датасета.
        \item \textbf{Aggregating (Агрегация):} На каждой подвыборке независимо обучается своя модель (в случае RF — дерево решений). Затем предсказания всех \(N\) моделей усредняются (для регрессии) или определяется самый популярный класс (для классификации — голосование большинством).
    \end{itemize}
    \textit{Цель бэггинга:} Снизить \textbf{дисперсию (variance)} модели. Индивидуальные деревья могут сильно переобучаться (высокая дисперсия), но усреднение их предсказаний сглаживает ошибки и делает итоговую модель более устойчивой.
    Объекты, не попавшие в конкретную бутстрэп-выборку (\(\approx 37\%\)), называются \textbf{Out-of-Bag (OOB)} и могут использоваться для оценки качества модели (OOB-оценка) без необходимости отдельной валидационной выборки.
\end{textbox}

\section{Случайный Выбор Признаков (Feature Subsampling)}

\begin{myexampleblock}{Дополнительная Случайность}
    В отличие от простого бэггинга деревьев, Случайный Лес вносит \textbf{дополнительный элемент случайности} при построении каждого дерева:
    \begin{itemize}
        \item При поиске лучшего разбиения (split) в каждом узле дерева, алгоритм рассматривает не все доступные признаки, а только их \textbf{случайное подмножество} (размер подмножества, \texttt{max\_features}, является гиперпараметром).
        \item Для задачи классификации обычно берут \(\sqrt{p}\) признаков, для регрессии — \(p/3\), где \(p\) — общее число признаков.
    \end{itemize}
    \textit{Зачем это нужно?} Это делается для \textbf{декорреляции} деревьев. Если бы все деревья видели все признаки, и был бы один очень сильный признак, большинство деревьев использовали бы его для первого разбиения. В результате деревья были бы очень похожи (скоррелированы), и усреднение не дало бы такого сильного эффекта снижения дисперсии. Случайный выбор признаков заставляет деревья быть более разнообразными.

    \textit{Аналогия:} Возвращаясь к комитету экспертов. Чтобы они не пришли к одному и тому же выводу, опираясь на самый очевидный факт, ты просишь каждого эксперта при анализе сосредоточиться только на \textbf{случайном наборе аспектов} проблемы. Это побуждает их исследовать разные стороны вопроса.
\end{myexampleblock}

\section{Как Уменьшает Дисперсию}

\begin{myblock}{Борьба с Переобучением через Усреднение}
    Ключевая сила Случайного Леса — в его способности значительно \textbf{уменьшать дисперсию} по сравнению с одним деревом решений, не сильно увеличивая (или даже немного уменьшая) \textbf{смещение (bias)}.
    \begin{itemize}
        \item \textbf{Одно дерево решений:} Имеет низкое смещение (может хорошо подогнаться под обучающие данные), но высокую дисперсию (сильно меняется при небольшом изменении данных, легко переобучается).
        \item \textbf{Случайный Лес:}
            \begin{itemize}
                \item \textbf{Бэггинг (усреднение):} Усреднение предсказаний \(N\) моделей, ошибки которых не полностью скоррелированы, приводит к снижению общей дисперсии ансамбля. Чем больше деревьев (\(N\)), тем ниже дисперсия (до определенного предела).
                \item \textbf{Случайный выбор признаков (декорреляция):} Уменьшает корреляцию между деревьями, что делает усреднение еще более эффективным для снижения дисперсии.
            \end{itemize}
    \end{itemize}
    В итоге, RF получает модель, которая все еще достаточно гибкая (относительно низкое смещение, унаследованное от деревьев), но гораздо более стабильная и устойчивая к переобучению (значительно сниженная дисперсия). Это классический пример улучшения модели через управление \textbf{компромиссом смещения-дисперсии (Bias-Variance Tradeoff)}.
    
    \textit{Аналогия "Мудрость Толпы":} Один человек может сильно ошибаться в оценке (высокая дисперсия), но если усреднить оценки большой группы людей (где ошибки случайны и не связаны), итоговая оценка будет гораздо ближе к истине (низкая дисперсия).
\end{myblock}

\section{Важность Признаков (Feature Importance)}

\begin{textbox}{Оценка Влияния Признаков}
    Хотя Случайный Лес менее интерпретируем, чем одно дерево, он позволяет оценить \textbf{важность} каждого признака для предсказания. Основные подходы:
    \begin{itemize}
        \item \textbf{Mean Decrease in Impurity (MDI) / Gini Importance:}
            \begin{itemize}
                \item \textit{Как считается:} Для каждого признака суммируется уменьшение критерия неопределенности (например, индекса Джини для классификации или MSE для регрессии) по всем узлам всех деревьев, где этот признак использовался для разбиения. Затем эти суммы усредняются по всем деревьям и нормализуются.
                \item \textit{Плюсы:} Быстро считается (информация доступна после обучения).
                \item \textit{Минусы:} Склонен завышать важность числовых признаков и признаков с большим количеством категорий. Может давать неверные результаты для скоррелированных признаков.
            \end{itemize}
        \item \textbf{Mean Decrease in Accuracy (MDA) / Permutation Importance:}
            \begin{itemize}
                \item \textit{Как считается:}
                    1. Оценивается качество модели (например, Accuracy, R2) на отложенной выборке (Out-of-Bag или отдельный validation set).
                    2. Значения одного признака случайно перемешиваются во всей отложенной выборке (нарушается связь между этим признаком и целевой переменной).
                    3. Качество модели повторно оценивается на перемешанных данных.
                    4. Уменьшение качества модели и есть важность этого признака. Повторяется для всех признаков.
                \item \textit{Плюсы:} Более надежен, чем MDI, особенно при наличии скоррелированных признаков. Показывает реальное влияние признака на \textbf{производительность модели} на новых данных. Идея метода часто \textbf{модель-агностична} (применима не только к RF).
                \item \textit{Минусы:} Требует дополнительных вычислений (может быть \textbf{медленным} на больших данных или при большом числе признаков). Результат может зависеть от конкретной отложенной выборки.
            \end{itemize}
    \end{itemize}
\end{textbox}

\begin{alerttextbox}{Что Спрашивают на Собеседованиях}
    Часто спрашивают разницу между MDI и Permutation Importance. Важно понимать:
    \begin{itemize}
        \item MDI измеряет, насколько признак \textbf{использовался} деревьями при построении (на основе обучающей выборки).
        \item Permutation Importance измеряет, насколько признак \textbf{влияет} на итоговое качество предсказания модели (на основе отложенной выборки). Permutation Importance обычно считается более надежным показателем реальной важности.
    \end{itemize}
\end{alerttextbox}

\section{Ключевые Гиперпараметры}
Основные параметры для настройки Случайного Леса:
\begin{itemize}
    \item \textbf{\texttt{n\_estimators}:} Количество деревьев в лесу. Чем больше, тем лучше (до некоторого плато), но дольше обучение. Обычно выбирают достаточно большим (100, 500, 1000+).
    \item \textbf{\texttt{max\_features}:} Количество признаков, рассматриваемых при поиске лучшего сплита в каждом узле. Ключевой параметр для контроля корреляции деревьев и борьбы с переобучением. Значения по умолчанию (\(\sqrt{p}\) / \(p/3\)) часто работают хорошо, но стоит подбирать.
    \item \textbf{Параметры деревьев:} Гиперпараметры базовых деревьев решений также влияют на лес (например, \texttt{max\_depth}, \texttt{min\_samples\_split}, \texttt{min\_samples\_leaf}). Часто оставляют деревья достаточно глубокими в RF, полагаясь на усреднение для борьбы с переобучением, но иногда их ограничение тоже помогает.
\end{itemize}

\section{Сравнение с Конкурентами}

\begin{myblock}{RF vs. Одно Дерево Решений}
    \begin{itemize}
        \item \textbf{Плюсы RF:}
            \begin{itemize}
                \item Значительно \textbf{меньше переобучается}, более \textbf{устойчив} (низкая дисперсия).
                \item Обычно \textbf{выше точность} и обобщающая способность.
            \end{itemize}
        \item \textbf{Минусы RF:}
            \begin{itemize}
                \item \textbf{Менее интерпретируем} ("черный ящик" по сравнению с одним деревом).
                \item Требует \textbf{больше ресурсов} для обучения и предсказания (память и время).
            \end{itemize}
    \end{itemize}
\end{myblock}

\begin{myblock}{RF vs. Линейные Модели (Логистическая/Линейная Регрессия)}
    \begin{itemize}
        \item \textbf{Плюсы RF:}
            \begin{itemize}
                \item Легко улавливает \textbf{нелинейные зависимости} и \textbf{взаимодействия} между признаками без необходимости их явного добавления (как в линейных моделях).
                \item \textbf{Не требует масштабирования} признаков (деревьям не важен масштаб).
                \item Менее чувствителен к \textbf{выбросам} (решающие правила деревьев устойчивы).
                \item Хорошо работает "из коробки" с минимальной настройкой.
            \end{itemize}
        \item \textbf{Минусы RF:}
            \begin{itemize}
                \item \textbf{Менее интерпретируем}, чем линейные модели (где можно смотреть на веса).
                \item Может быть \textbf{медленнее} в обучении и предсказании на очень больших данных.
                \item Плохо \textbf{экстраполирует} за пределы диапазона значений признаков, виденных в обучении (предсказание ограничено значениями в "листьях"). Линейные модели могут экстраполировать.
                \item Может требовать \textbf{больше памяти}.
            \end{itemize}
    \end{itemize}
\end{myblock}

% --- Конец контента ---