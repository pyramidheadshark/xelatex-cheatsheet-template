% >>> Контент для шпаргалки по Reinforcement Learning (Основы)

\section{RL: Основная Идея}

\begin{myblock}{Что такое Reinforcement Learning (RL)?}
    \textbf{Обучение с подкреплением (RL)} — это область машинного обучения, где \textbf{агент} учится принимать решения, взаимодействуя со \textbf{средой}. Цель агента — максимизировать суммарную \textbf{награду}, получаемую от среды за свои действия. Обучение происходит методом проб и ошибок.

    \textbf{Ключевые отличия:}
    \begin{itemize}
        \item \textbf{От Supervised Learning:} В RL нет готовых пар "вход-правильный выход". Агент сам должен выяснить, какие действия ведут к лучшим результатам, ориентируясь только на сигнал награды (часто отложенный во времени).
        \item \textbf{От Unsupervised Learning:} В RL есть явный сигнал обратной связи — \textbf{награда}, который направляет обучение, в то время как в Unsupervised Learning основной целью является поиск структуры в данных без какой-либо явной обратной связи.
    \end{itemize}
    \textbf{Аналогия:} Подумай о дрессировке щенка. Щенок (агент) выполняет команды (действия) в комнате (среда). Если он выполняет команду правильно (например, садится), ты даешь ему лакомство (положительная награда). Если делает что-то не то (например, грызет тапок), ты можешь сказать "фу!" (отрицательная награда или ее отсутствие). Щенок постепенно учится выполнять действия, которые приносят больше "лакомств".
\end{myblock}

\section{Ключевые Компоненты и Терминология}

\begin{textbox}{Основные понятия RL (Важно для собеседования!)}
    \begin{itemize}
        \item \textbf{Агент (Agent):} Сущность, которая обучается и принимает решения (например, игрок в игре, робот, система рекомендаций).
        \item \textbf{Среда (Environment):} Внешний мир, с которым взаимодействует агент (например, игровое поле, комната для робота, веб-сайт для рекомендаций). Среда реагирует на действия агента, изменяет свое состояние и выдает награду.
        \item \textbf{Состояние (State, $S$):} Конкретная ситуация или конфигурация среды, которую наблюдает агент в данный момент времени (например, позиция фигур на доске, показания датчиков робота, история просмотров пользователя). Множество всех возможных состояний называется пространством состояний.
        \item \textbf{Действие (Action, $A$):} Выбор, который агент может сделать в данном состоянии (например, ход фигурой, движение мотора робота, показ определенного товара). Множество всех доступных действий (в данном состоянии или вообще) называется пространством действий.
        \item \textbf{Награда (Reward, $R$):} Числовой сигнал, получаемый агентом от среды после выполнения действия $a$ в состоянии $s$. Показывает, насколько "хорошим" было это действие \textit{сиюминутно}. Цель агента — максимизировать \textit{суммарную} награду в долгосрочной перспективе, а не только немедленную.
        \item \textbf{Политика (Policy, $\pi$):} Стратегия агента, определяющая его поведение. Это отображение состояний в действия (детерминированная политика $\pi: S \to A$) или в распределение вероятностей над действиями (стохастическая политика $\pi(a|s) = P(A_t = a | S_t = s)$). \textbf{Именно оптимальную политику мы и стремимся найти в RL.}
        \item \textbf{Ценность Состояния (State-Value Function, $V^\pi(s)$):} Проще говоря, V-функция показывает, насколько "хорошо" находиться в состоянии $s$ в долгосрочной перспективе, если следовать политике $\pi$. Формально: это ожидаемая суммарная дисконтированная награда, начиная из состояния $s$ и далее следуя политике $\pi$. $V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s \right]$.
        \item \textbf{Ценность Действия (Action-Value Function, Q-function, $Q^\pi(s, a)$):} Проще говоря, Q-функция показывает, насколько "хорошо" выполнить действие $a$ в состоянии $s$ и затем действовать согласно политике $\pi$. Формально: это ожидаемая суммарная дисконтированная награда, начиная из состояния $s$, совершив действие $a$, и далее следуя политике $\pi$. $Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a \right]$. \textbf{Q-функция часто используется для выбора лучшего действия}, даже если сама политика не задана явно (выбираем действие $a$ с максимальным $Q(s,a)$).
        \item \textbf{Эпизод (Episode):} Полная последовательность взаимодействий агента со средой от начального состояния до терминального (конечного) состояния. Актуально для \textit{эпизодических задач} (игры, лабиринты). В \textit{непрерывных задачах} (управление процессом) понятия эпизода может не быть, и для обеспечения сходимости суммарной награды часто необходимо использовать дисконтирование ($\gamma < 1$).
    \end{itemize}
\end{textbox}

\section{Цель Обучения и Дисконтирование}

\begin{myblock}{Максимизация Накопленной Награды}
    Основная цель RL — найти такую политику $\pi$, которая максимизирует ожидаемую \textbf{суммарную дисконтированную награду}. Мы не просто хотим получить большую награду сейчас, а максимизировать сумму наград на протяжении всего времени (эпизода или бесконечного горизонта).

    \textbf{Дисконтирование (Discount Factor, $\gamma$):}
    Это параметр (число от 0 до 1, обычно близкое к 1, например, 0.99), который определяет важность будущих наград по сравнению с немедленными. Суммарная дисконтированная награда (Return, $G_t$) в момент времени $t$ рассчитывается как:
    \[ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \]
    Зачем нужно дисконтирование ($\gamma < 1$)?
    \begin{enumerate}
        \item \textbf{Математическая необходимость:} Для задач с бесконечным горизонтом (без конечного состояния) сумма наград может расходиться. Дисконтирование гарантирует сходимость ряда, если награды ограничены.
        \item \textbf{Интуитивная целесообразность:} Часто награды, полученные раньше, более важны и предсказуемы, чем награды, полученные далеко в будущем. Мы предпочитаем получить 100 долларов сегодня, а не через 10 лет.
    \end{enumerate}
    Если $\gamma = 0$, агент становится "близоруким" и учитывает только немедленную награду. Если $\gamma = 1$, будущие награды учитываются так же, как и немедленные (используется для эпизодических задач, где сумма конечна).
\end{myblock}

\section{Дилемма Exploration vs. Exploitation}

\begin{myexampleblock}{Исследование или Использование?}
    Одна из фундаментальных проблем в RL — это баланс между исследованием и эксплуатацией.
    \begin{itemize}
        \item \textbf{Exploitation (Эксплуатация):} Использовать текущие знания о среде для выбора действий, которые, как известно агенту, приносят наибольшую награду. Пример: ходить в уже знакомый ресторан, который точно нравится.
        \item \textbf{Exploration (Исследование):} Пробовать новые, ранее не изведанные (или мало изведанные) действия, чтобы получить больше информации о среде и, возможно, найти лучший путь/стратегию, чем известный сейчас. Пример: попробовать новый ресторан, который может оказаться лучше (или хуже) знакомого.
    \end{itemize}
    \textbf{Суть дилеммы:}
    \begin{itemize}
        \item Слишком много эксплуатации $\implies$ агент может "застрять" в локальном оптимуме, не найдя глобально наилучшей стратегии.
        \item Слишком много исследования $\implies$ агент будет тратить много времени на неоптимальные действия, получая меньше награды в процессе обучения.
    \end{itemize}
    Необходим умный компромисс.

    \textbf{Простая стратегия: $\epsilon$-greedy (эпсилон-жадная):}
    С вероятностью $(1-\epsilon)$ агент выбирает действие, которое считается лучшим согласно текущей оценке (например, с максимальным Q-значением) — это \textit{exploitation}. С небольшой вероятностью $\epsilon$ (эпсилон, например, 0.1) агент выбирает случайное действие из всех доступных — это \textit{exploration}. Часто $\epsilon$ уменьшают со временем: в начале обучения исследуем больше, затем — больше эксплуатируем накопленные знания.
\end{myexampleblock}

\section{Основные Подходы к RL (Обзорно)}

\begin{myblock}{Классификация Методов RL}
    Существует несколько способов классифицировать RL алгоритмы.

    \textbf{1. Model-Based vs. Model-Free:}
    \begin{itemize}
        \item \textbf{Model-Based (На основе модели):} Агент сначала пытается построить \textit{модель среды} (то есть выучить функции перехода $P(s'|s,a)$ и награды $R(s,a)$). Затем использует эту модель для \textit{планирования} оптимальных действий (например, с помощью динамического программирования или поиска по дереву).
        \item \textbf{Model-Free (Без модели):} Агент учит \textit{политику} или \textit{функцию ценности} напрямую из опыта (взаимодействий со средой), не строя явной модели среды. Этот подход часто более применим к сложным задачам, где построение точной модели затруднительно или невозможно. \textbf{Большинство известных RL алгоритмов (Q-learning, DQN, Policy Gradients, A3C) — model-free.}
    \end{itemize}

    \textbf{2. Value-Based vs. Policy-Based (и Actor-Critic):} (Классификация для Model-Free методов)
    \begin{itemize}
        \item \textbf{Value-Based (На основе ценности):}
            \begin{itemize}
                \item \textit{Идея:} Обучить функцию ценности (обычно Q-функцию $Q(s,a)$).
                \item \textit{Политика:} Неявная (implicit). Агент выбирает действие, максимизирующее выученную Q-функцию в текущем состоянии: $\pi(s) = \arg\max_a Q(s,a)$.
                \item \textit{Пример: \textbf{Q-Learning}}. Это классический \textbf{off-policy} (внеполитиковый) алгоритм. Off-policy означает, что алгоритм может обучаться оптимальной Q-функции (и, соответственно, оптимальной политике), используя данные, собранные при следовании другой, возможно, неоптимальной или исследовательской политике (например, $\epsilon$-greedy). Он итеративно обновляет оценку Q-функции для пар (состояние, действие), используя полученный опыт $(s, a, r, s')$:
                \[ Q(s,a) \leftarrow Q(s,a) + \alpha \big[ r + \gamma \max_{a'} Q(s', a') - Q(s,a) \big] \]
                \textit{Смысл формулы:} Новая оценка $Q(s,a)$ сдвигается в сторону "целевого значения" (target): $r + \gamma \max_{a'} Q(s', a')$. Это целевое значение состоит из немедленной награды $r$ и максимальной ожидаемой будущей награды из следующего состояния $s'$ (оцененной по текущей Q-функции). $\alpha$ (alpha) — это \textit{скорость обучения} (learning rate).
                \item \textit{Другой пример:} Deep Q-Network (DQN) — использует нейронную сеть для аппроксимации Q-функции в задачах с большим пространством состояний.
            \end{itemize}
        \item \textbf{Policy-Based (На основе политики):}
            \begin{itemize}
                \item \textit{Идея:} Напрямую параметризовать политику $\pi(a|s; \theta)$ (например, нейронной сетью с параметрами $\theta$) и оптимизировать эти параметры для максимизации ожидаемой награды.
                \item \textit{Подход:} Обычно используются методы градиентного подъема (\textbf{Policy Gradients}), которые корректируют параметры $\theta$ в направлении, \textit{максимизирующем ожидаемую суммарную дисконтированную награду (Return)}.
                \item \textit{Преимущество:} Хорошо работают в непрерывных пространствах действий, могут изучать стохастические политики.
                \item \textit{Пример:} REINFORCE.
            \end{itemize}
        \item \textbf{Actor-Critic:}
            \begin{itemize}
                \item \textit{Идея:} Комбинировать Value-Based и Policy-Based подходы. Используются две модели (или две части одной модели):
                    \begin{itemize}
                        \item \textbf{Actor (Актер):} Отвечает за выбор действий (учит \textit{политику} $\pi(a|s; \theta)$).
                        \item \textbf{Critic (Критик):} Оценивает действия, выбранные Актером (учит \textit{функцию ценности}, например, $V(s; w)$ или $Q(s,a; w)$).
                    \end{itemize}
                \item \textit{Принцип работы:} Критик помогает Актеру понять, насколько хороши были его действия. Критик предоставляет \textit{более стабильную и менее шумную оценку качества действий} (по сравнению с использованием только сырой награды от среды), что ускоряет и стабилизирует обучение политики Актера.
                \item \textit{Примеры:} A2C (Advantage Actor-Critic), A3C (Asynchronous Advantage Actor-Critic), DDPG, SAC.
            \end{itemize}
    \end{itemize}
\end{myblock}

\section{Примеры Применения RL}

\begin{textbox}{Где используется Обучение с Подкреплением?}
    RL добился впечатляющих результатов во многих областях:
    \begin{itemize}
        \item \textbf{Игры:} Обучение агентов игре на уровне человека или сверхчеловека (шахматы, го - AlphaGo/AlphaZero, видеоигры Atari, StarCraft).
        \item \textbf{Робототехника:} Обучение роботов ходьбе, манипуляциям с объектами, навигации.
        \item \textbf{Системы рекомендаций:} Персонализация контента или товаров, оптимизация долгосрочного вовлечения пользователя.
        \item \textbf{Оптимизация ресурсов:} Управление трафиком, распределение ресурсов в сетях, оптимизация рекламных кампаний и ставок (bidding).
        \item \textbf{Автономное вождение:} Принятие решений в сложных дорожных ситуациях (частично).
        \item \textbf{Химия и Биология:} Поиск новых молекул, оптимизация химических реакций.
    \end{itemize}
\end{textbox}

\section{Основные Вызовы в RL}

\begin{alerttextbox}{Сложности и Ограничения}
    Несмотря на успехи, RL сталкивается с рядом серьезных проблем:
    \begin{itemize}
        \item \textbf{Требовательность к данным (Sample Inefficiency):} Большинству RL-алгоритмов (особенно model-free) требуется огромное количество взаимодействий со средой для обучения эффективной политике, что может быть дорого или невозможно в реальном мире.
        \item \textbf{Проблема присвоения награды (Credit Assignment Problem):} Сложно понять, какое именно действие в длинной последовательности привело к итоговой награде (особенно если награда редкая или отложенная).
        \item \textbf{Разработка функции награды (Reward Shaping):} Создание хорошей функции награды, которая корректно отражает желаемую цель и не приводит к нежелательному поведению агента, — часто сложная и нетривиальная задача.
        \item \textbf{Большие/Непрерывные пространства состояний и действий:} Стандартные методы (вроде табличного Q-learning) не работают. Требуется использование аппроксимации функций (нейронные сети) и более сложных алгоритмов.
        \item \textbf{Стабильность и Воспроизводимость:} Обучение RL-агентов может быть нестабильным, чувствительным к гиперпараметрам, и результаты бывает сложно воспроизвести.
        \item \textbf{Безопасность и Исследование:} Как позволить агенту безопасно исследовать среду, не совершая катастрофических ошибок (особенно в реальном мире, например, с роботами или автопилотами)?
    \end{itemize}
\end{alerttextbox}

% --- Конец контента ---