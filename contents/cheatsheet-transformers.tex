% >>> Контент для шпаргалки "Cheatsheet 9: Введение в Трансформеры" (Версия 2, с учетом правок)

\section{Введение в Трансформеры (Attention, Architecture, LLMs)}

\subsection{Механизм Внимания (Attention)}

\begin{textbox}{Основная Идея Attention}
    Представьте, что вы переводите длинное предложение. Когда вы пишете очередное слово перевода, вы не смотрите на \textit{все} слова исходного предложения одинаково. Вы обращаете \textbf{внимание} на одно или несколько конкретных слов оригинала, которые наиболее важны для перевода \textit{именно этого} слова.
    Механизм \textbf{Attention} (Внимание) в нейронных сетях делает то же самое: он позволяет модели динамически фокусироваться на наиболее релевантных частях входной последовательности при генерации каждой части выходной последовательности.
    Технически, Attention вычисляет \textbf{веса (weights)} важности для каждого элемента входа относительно текущей задачи, показывая, "насколько" нужно обратить внимание на тот или иной элемент.

    \textbf{Ключевая цель:} Дать модели понять, какие части входа наиболее важны для текущего шага обработки.
\end{textbox}

\begin{myexampleblock}{Аналогия: Повар и Рецепт}
    Представьте, что вы готовите сложное блюдо по рецепту (входная последовательность). На каждом шаге (генерация действия) вы не перечитываете весь рецепт заново. Вы \textbf{обращаете внимание} на конкретный пункт или ингредиент, релевантный для \textit{текущего} действия (например, "добавить 2 яйца", "взбивать 5 минут"). Механизм Attention работает похожим образом.
\end{myexampleblock}

\begin{myblock}{Self-Attention (Внимание к Себе)}
    Это особый вид Attention, где модель взвешивает важность \textbf{разных слов внутри одной и той же последовательности} по отношению друг к другу.
    \begin{itemize}
        \item Позволяет модели понять внутренние зависимости в предложении. Например, в предложении "Банк выдал кредит, потому что \textbf{он} был одобрен", Self-Attention помогает понять, что местоимение "\textbf{он}" относится к слову "кредит", а не "Банк".
        \item Является ключевым компонентом архитектуры \textbf{Transformer}.
    \end{itemize}
    \textbf{Суть:} Каждое слово "смотрит" на все остальные слова в предложении (включая себя) и решает, насколько они важны для понимания его собственного контекста.
\end{myblock}

\subsection{Архитектура Трансформера (Общая Схема)}

\begin{textbox}{Encoder-Decoder Структура и Ключевые Элементы}
    Классическая архитектура \textbf{Transformer} часто состоит из двух основных частей:
    \begin{itemize}
        \item \textbf{Encoder (Кодировщик):} Читает всю входную последовательность (например, предложение на русском) и создает ее богатое контекстуальное представление. Он использует \textbf{Self-Attention}, чтобы понять взаимосвязи между словами во входных данных.
        \item \textbf{Decoder (Декодировщик):} Генерирует выходную последовательность (например, перевод на английский) шаг за шагом. На каждом шаге он использует:
            \begin{itemize}
                \item \textbf{Self-Attention} для учета уже сгенерированных им слов.
                \item \textbf{Encoder-Decoder Attention} для фокусировки на релевантных частях представления, полученного от кодировщика.
            \end{itemize}
    \end{itemize}
    \textbf{Ключевые "строительные блоки"} внутри Encoder и Decoder:
    \begin{itemize}
        \item \textbf{Multi-Head Attention} (позволяет механизму внимания одновременно фокусироваться на разных типах зависимостей или 'аспектах' информации, как если бы у вас было несколько 'голов', читающих текст с разными целями).
        \item \textbf{Feed-Forward Networks} (обычные полносвязные сети, применяемые к каждому элементу последовательности независимо).
        \item \textbf{Positional Encoding (Позиционное Кодирование):} \textbf{Критически важно!} Сам по себе Self-Attention не учитывает порядок слов (для него "собака укусила человека" и "человека укусила собака" – одно и то же, так как он смотрит на все слова сразу). Positional Encoding решает эту проблему, добавляя к эмбеддингу каждого слова специальный вектор, содержащий информацию о его \textit{позиции} в последовательности. \textbf{Аналогия:} Как номер страницы в книге помогает понять порядок глав.
    \end{itemize}
    \textit{Примечание: Существуют модели, использующие только Encoder (как BERT) или только Decoder (как GPT).}
\end{textbox}

\begin{myexampleblock}{Аналогия: Переводчик-Человек}
    \begin{itemize}
        \item \textbf{Encoder:} Опытный переводчик читает исходное предложение целиком, вникает в смысл и взаимосвязи слов (Self-Attention), учитывая их порядок (Positional Encoding). Он формирует у себя в голове полное понимание ("контекстуальное представление").
        \item \textbf{Decoder:} Переводчик начинает писать перевод. Для каждого слова перевода он вспоминает, что уже написал (Decoder Self-Attention + Positional Encoding) и обращается к своему пониманию оригинала, фокусируясь на нужных словах (Encoder-Decoder Attention).
    \end{itemize}
\end{myexampleblock}

\subsection{Примеры: BERT и GPT (Большие Языковые Модели - LLM)}

\begin{myblock}{Большие Языковые Модели (LLM - Large Language Models)}
    Это модели (часто основанные на архитектуре Transformer), обученные на \textbf{огромных} объемах текстовых данных. Они способны понимать и генерировать человеческий язык с высокой степенью точности.
    \begin{itemize}
        \item \textbf{BERT (Bidirectional Encoder Representations from Transformers):}
            \begin{itemize}
                \item Использует в основном \textbf{Encoder} часть Трансформера.
                \item Обучается предсказывать пропущенные ("замаскированные") слова в тексте (\textbf{Masked Language Model - MLM}), а также предсказывать, является ли одно предложение логическим продолжением другого. Это позволяет ему учитывать контекст \textbf{с обеих сторон} от слова (Bidirectional), что дает глубокое понимание.
                \item Отлично подходит для задач, требующих глубокого понимания контекста: классификация текста, извлечение именованных сущностей (NER), ответы на вопросы.
                \item \textbf{Аналогия BERT:} Студент, которому дали текст с пропусками, и он должен их заполнить, видя весь остальной текст, чтобы доказать свое понимание.
            \end{itemize}
        \item \textbf{GPT (Generative Pre-trained Transformer):}
            \begin{itemize}
                \item Использует в основном \textbf{Decoder} часть Трансформера.
                \item Обучается предсказывать \textbf{следующее слово} в последовательности (\textbf{Causal Language Model - CLM}), опираясь только на \textit{предшествующий} контекст. Это делает модель \textbf{авторегрессионной} (генерирует слово за словом).
                \item Отлично подходит для генеративных задач: написание текстов, создание диалоговых систем, продолжение кода.
                \item \textbf{Аналогия GPT:} Опытный рассказчик, который может продолжить любую начатую историю, предсказывая наиболее вероятное следующее слово на основе уже сказанного.
            \end{itemize}
    \end{itemize}
\end{myblock}

\begin{alerttextbox}{Ключевое для Собеседования}
    \textbf{Главное:} Понимать концепцию \textbf{Attention} как механизма вычисления весов важности частей входа. Знать, что \textbf{Self-Attention} смотрит на зависимости внутри одной последовательности, но \textbf{не видит порядок} слов, поэтому необходимо \textbf{Positional Encoding}. Представлять \textbf{Transformer} как архитектуру (часто Encoder-Decoder) с \textbf{Multi-Head Attention} и \textbf{Feed-Forward} блоками. Знать, что \textbf{BERT} (Encoder, MLM, Bidirectional) и \textbf{GPT} (Decoder, CLM, Autoregressive) - это примеры \textbf{LLM} на базе Трансформеров с разными архитектурными фокусами и задачами предобучения, определяющими их сильные стороны (понимание контекста vs генерация).
\end{alerttextbox}

% --- Конец контента ---