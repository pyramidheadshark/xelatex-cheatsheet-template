% >>> Улучшенный Контент для шпаргалки Cheatsheet 7: Основы Unsupervised Learning (PCA & K-Means + DBSCAN)

% --- Введение ---
\begin{textbox}{Обучение без учителя (Unsupervised Learning): Введение}
    В отличие от обучения с учителем, здесь у нас \textbf{нет правильных ответов} (меток) для данных. Цель — найти \textbf{скрытые структуры}, закономерности или группы в самих данных. Представь, что тебе дали кучу разных носков, и ты должен рассортировать их по парам, не зная изначально, какие носки составляют пару — ты ищешь схожесть сам.
    \begin{itemize}
        \item \textbf{Основные задачи:} Понижение размерности, кластеризация, поиск аномалий, изучение ассоциативных правил.
        \item \textbf{Ключевые методы для старта:} PCA (понижение размерности) и K-Means/DBSCAN (кластеризация).
    \end{itemize}
\end{textbox}

% --- Раздел 1: PCA ---
\section{Метод Главных Компонент (PCA - Principal Component Analysis)}

\begin{myblock}{Идея: Понижение размерности}
    Представь, что у тебя есть сложный многомерный объект (данные с большим числом признаков), и ты хочешь сделать его "плоскую", но информативную фотографию (понизить размерность до меньшего числа признаков). PCA именно это и делает — находит новые, самые информативные "ракурсы" или "проекции" данных.
    \vspace{0.5ex}
    \textbf{Цель:} Уменьшить количество признаков (столбцов), сохранив при этом максимальное количество \textbf{вариативности (дисперсии)} исходных данных.
    \vspace{0.5ex}
    \textbf{Почему дисперсия = информация?} В контексте PCA предполагается, что направление, вдоль которого данные сильнее всего "разбросаны" (имеют большую дисперсию), несет больше всего информации об их различиях. Если вдоль какого-то направления все точки почти одинаковы (малая дисперсия), это направление мало что добавляет к пониманию структуры данных.
\end{myblock}

\begin{textbox}{Как работает PCA (Механика и Интуиция)}
    PCA находит новые оси (называемые \textbf{главными компонентами}, PC) в пространстве исходных признаков. Вот как это происходит по шагам:
    \begin{enumerate}
        \item \textbf{Стандартизация данных:} \textit{Критически важно!} Вычитаем среднее и делим на стандартное отклонение для \textbf{каждого} признака. \textbf{Зачем?} PCA чувствителен к масштабу. Без стандартизации признаки с большими значениями (например, доход в рублях) будут доминировать над признаками с малыми значениями (например, стаж в годах), даже если последние не менее важны по смыслу, так как дисперсия первых будет искусственно завышена.
        \item \textbf{Расчет ковариационной матрицы:} Строится матрица, показывающая, как стандартизированные признаки изменяются \textit{совместно}. На диагонали — дисперсия каждого признака (после стандартизации равна 1), вне диагонали — ковариация между парами признаков (показывает их линейную связь).
        \item \textbf{Нахождение собственных векторов и значений:} Для ковариационной матрицы вычисляются её \textbf{собственные векторы} (\textit{eigenvectors}) и \textbf{собственные значения} (\textit{eigenvalues}).
            \begin{itemize}
                \item \textbf{Собственный вектор:} Это \textbf{направление} в пространстве признаков.
                \item \textbf{Собственное значение:} Это число, показывающее, \textbf{сколько дисперсии} исходных данных объясняется вдоль направления, заданного соответствующим собственным вектором.
            \end{itemize}
        \item \textbf{Сортировка и выбор компонент:} Собственные векторы сортируются по убыванию их собственных значений. Первый собственный вектор (с самым большим $\lambda$) задает направление \textbf{первой главной компоненты (PC1)} — направление максимальной дисперсии. Второй (с вторым по величине $\lambda$) задает \textbf{PC2}, ортогональную PC1 и объясняющую максимум \textit{оставшейся} дисперсии, и т.д. Все главные компоненты ортогональны друг другу (не коррелируют).
        \item \textbf{Проецирование:} Исходные (стандартизированные) данные проецируются на выбранные $k$ главных компонент (те, что с наибольшими $\lambda$). Результат проекции — это новый набор данных с $k$ признаками, где каждый новый признак является линейной комбинацией исходных.
    \end{enumerate}
    \textbf{Аналогия "Эллипс и Тени":} Представь облако точек данных как эллипс. Главные компоненты — это оси этого эллипса (самая длинная — PC1, следующая перпендикулярная ей — PC2 и т.д.). PCA находит эти оси и "поворачивает" данные так, чтобы оси совпали с осями координат. Затем он "отбрасывает" оси с наименьшей длиной (наименьшей дисперсией), оставляя проекцию на самые важные (длинные) оси.
\end{textbox}

\begin{alerttextbox}{Когда применять PCA и Как выбрать компоненты?}
    \begin{itemize}
        \item \textbf{Применения:} Визуализация (до 2D/3D), уменьшение шума, борьба с мультиколлинеарностью, ускорение обучения других моделей, сжатие данных.
        \item \textbf{Чувствительность к масштабу:} \textbf{Всегда стандартизируйте данные перед PCA!} Иначе признаки с большим масштабом "перетянут" всю дисперсию на себя.
        \item \textbf{Выбор количества компонент (k):}
            \begin{itemize}
                \item \textbf{Доля объясненной дисперсии:} Смотрят на кумулятивную (накопленную) долю $\sum_{i=1}^{k} \lambda_i / \sum_{j=1}^{N} \lambda_j$. Выбирают $k$ так, чтобы объяснить достаточный процент (например, 90-99\%) общей дисперсии.
                \item \textbf{Метод Локтя:} Ищут "изгиб" на графике кумулятивной объясненной дисперсии или на графике самих собственных значений ($\lambda_i$ от $i$, т.н. \textit{scree plot}). Точка изгиба показывает, где добавление новой компоненты перестает давать существенный прирост информации.
                \item \textbf{Исходя из задачи:} Для визуализации $k=2$ или $k=3$. Для других задач можно подбирать $k$ по кросс-валидации для *конечной* ML-модели.
            \end{itemize}
    \end{itemize}
\end{alerttextbox}

% --- Раздел 2: K-Means ---
\section{Кластеризация K-Means}

\begin{myblock}{Идея: Кластеризация}
    Разделить все объекты на заранее заданное число ($K$) групп (кластеров) так, чтобы объекты внутри кластера были максимально похожи (близки) друг на друга, а объекты из разных кластеров — максимально различны. Цель — минимизировать суммарное квадратичное расстояние от точек до центров их кластеров (WCSS).
    \vspace{0.5ex}
    \textbf{Аналогия "Почтовые отделения":} Открыть $K$ почтовых отделений (центроидов) в городе так, чтобы суммарное \textit{квадратичное} расстояние от всех жителей (точек данных) до ближайшего к ним отделения было минимальным.
\end{myblock}

\begin{textbox}{Как работает K-Means (Алгоритм)}
    Это итеративный алгоритм:
    \begin{enumerate}
        \item \textbf{Инициализация:} Выбрать $K$. Разместить $K$ \textbf{центроидов} (начальных центров кластеров). Лучше использовать "умную" инициализацию \textbf{K-Means++} (размещает центроиды подальше друг от друга), чем чисто случайную.
        \item \textbf{Шаг присваивания (Assignment):} \textit{Цикл по каждой точке данных:} Вычислить расстояние от точки до \textbf{каждого} из $K$ центроидов. Присвоить точку тому кластеру, чей центроид \textbf{ближе всего} (обычно по евклидову расстоянию).
        \item \textbf{Шаг обновления (Update):} \textit{Цикл по каждому кластеру:} Найти новый центр масс (среднее арифметическое координат) \textbf{всех} точек, которые были присвоены этому кластеру на шаге 2. Переместить центроид кластера в эту новую точку.
        \item \textbf{Повторение:} Повторять шаги 2 и 3 до тех пор, пока центроиды почти не перестанут смещаться или точки не перестанут менять кластеры (или достигнуто макс. число итераций).
    \end{enumerate}
    \textbf{Метрики расстояния:} Чаще всего используется \textbf{Евклидово} расстояние ($\sqrt{\sum(x_i-y_i)^2}$), которое хорошо работает для сферических кластеров. Иногда могут быть полезны: \textbf{Манхэттенское} ($\sum|x_i-y_i|$, "расстояние городских кварталов") или \textbf{Косинусное} ($1 - \cos(\theta)$, измеряет угол, полезно для текстов).
\end{textbox}

\subsection{Выбор количества кластеров (K)}
\begin{myexampleblock}{Как подобрать K?}
    Выбор оптимального K — нетривиальная задача. Популярные методы:
    \begin{itemize}
        \item \textbf{Метод Локтя (Elbow Method):} Строится график \textbf{WCSS} (Within-Cluster Sum of Squares - сумма квадратов расстояний от точек до центроидов их кластеров) от числа кластеров K. Ищется точка "изгиба" (локтя), после которой WCSS убывает значительно медленнее.
        \item \textbf{Метод Силуэта (Silhouette Method):} Запускают K-Means для разных K. Для каждого K вычисляют \textbf{средний Силуэт} по всем точкам. Выбирают то K, для которого средний Силуэт максимален (ближе к 1). См. ниже подробнее.
        \item \textbf{Знание предметной области:} Иногда K можно определить из контекста задачи.
    \end{itemize}
    % График для метода локтя оставлен без изменений
    \begin{center}
    \begin{tikzpicture}
        \begin{axis}[
            cheatsheet plot style, title={Метод Локтя (Пример)}, xlabel={Число кластеров (K)}, ylabel={WCSS (Инерция)},
            xmin=1, xmax=10, ymin=0, xtick={1,2,3,4,5,6,7,8,9,10}, legend pos=north east, grid=major, ]
        \addplot+[smooth, mark=*, blue] coordinates {(1, 300) (2, 180) (3, 100) (4, 70) (5, 50) (6, 40) (7, 35) (8, 32) (9, 30) (10, 29)};
        \addlegendentry{WCSS}
        \draw [red, dashed, thick] (axis cs:3,0) -- (axis cs:3,100);
        \node[red, anchor=west] at (axis cs:3.2, 110) {Возможный "локоть"};
        \end{axis}
    \end{tikzpicture}
    \end{center}
\end{myexampleblock}

\subsection{Оценка качества: Силуэт (Silhouette Score)}
\begin{myexampleblock}{Интуиция и Расчет Силуэта}
    Метрика "Силуэт" оценивает, насколько хорошо точка "сидит" в своем кластере по сравнению с соседними. Рассчитывается для каждой точки $i$:
    \begin{itemize}
        \item $a(i)$: Среднее расстояние от точки $i$ до \textbf{всех других точек в её собственном} кластере. \textit{(Насколько точка близка к "своим"? Чем меньше, тем лучше).}
        \item $b(i)$: \textbf{Минимальное} из средних расстояний от точки $i$ до \textbf{всех точек в каждом из других} ("соседних") кластеров. \textit{(Насколько точка далека от "ближайших чужих"? Чем больше, тем лучше).}
    \end{itemize}
    \textbf{Силуэт точки $i$}: \[ s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))} \]
    \textbf{Интерпретация значений $s(i)$:} От -1 до +1.
    \begin{itemize}
        \item $\approx +1$: Точка плотно сидит в своем кластере, далеко от других (хорошо).
        \item $\approx 0$: Точка на границе между кластерами.
        \item $\approx -1$: Точка, вероятно, попала не в тот кластер (плохо).
    \end{itemize}
    \textbf{Общий Силуэт:} Усредняют $s(i)$ по всем точкам. Чем ближе средний силуэт к 1, тем лучше разделение на кластеры для данного K.
    \vspace{0.5ex}
    \textbf{Аналогия "Районы города":} Высокий силуэт жителя ($s(i) \approx 1$) — он живет в центре четко очерченного района, далеко от других районов. Низкий ($s(i) \approx 0$) — живет на границе. Отрицательный ($s(i) \approx -1$) — живет ближе к центру соседнего района, чем к своему.
\end{myexampleblock}

\subsection{Плюсы и Минусы K-Means}
\begin{textbox}{Преимущества}
    \begin{itemize}
        \item \textbf{Простота и скорость:} Легко понять, реализовать, относительно быстро работает.
        \item \textbf{Интерпретируемость:} Концепция центроидов как "представителей" кластера понятна.
    \end{itemize}
\end{textbox}

\begin{alerttextbox}{Ограничения}
    \begin{itemize}
        \item \textbf{Чувствительность к инициализации:} Результат зависит от начального положения центроидов. \textbf{Решение:} Использовать K-Means++ и многократные запуски с разной инициализацией (\texttt{n\_init} в scikit-learn).
        \item \textbf{Необходимость задавать K:} Нужно знать K заранее или подбирать эвристиками.
        \item \textbf{Предположение о форме кластеров:} K-Means ищет \textbf{выпуклые, сферические} кластеры примерно одинакового размера. Плохо работает с вытянутыми, вогнутыми кластерами или кластерами разной плотности. (Для таких случаев лучше DBSCAN или спектральная кластеризация).
        \item \textbf{Чувствительность к выбросам:} Выбросы могут сильно смещать центроиды.
    \end{itemize}
\end{alerttextbox}

% --- Раздел 3: DBSCAN ---
\section{Кластеризация DBSCAN (Density-Based)}

\begin{myblock}{Идея: Кластеризация на основе плотности}
    В отличие от K-Means, DBSCAN не ищет центры, а находит \textbf{плотные регионы} точек, разделенные областями с низкой плотностью. Позволяет находить кластеры \textbf{произвольной формы} и автоматически определяет \textbf{выбросы (шум)}.
    \vspace{0.5ex}
    \textbf{Аналогия "Поиск островов":} Представь карту с точками-домами. DBSCAN ищет "острова" (кластеры), где дома стоят близко друг к другу, отделенные "водой" (разреженные области). Дома, стоящие совсем одиноко в "воде", считаются шумом.
\end{myblock}

\begin{textbox}{Как работает DBSCAN (Концепция)}
    Требует два параметра:
    \begin{itemize}
        \item \texttt{eps ($\epsilon$)}: Радиус окрестности. Максимальное расстояние между двумя точками, чтобы считать их соседями.
        \item \texttt{min\_samples}: Минимальное число соседей (включая саму точку) в $\epsilon$-окрестности, чтобы точка считалась "основной".
    \end{itemize}
    Основные понятия:
    \begin{itemize}
        \item \textbf{Основная точка (Core Point):} Точка, у которой в $\epsilon$-окрестности $\ge$ \texttt{min\_samples} точек. Находится внутри плотного региона.
        \item \textbf{Граничная точка (Border Point):} Точка, у которой < \texttt{min\_samples} соседей, но она сама является соседом какой-либо \textit{основной} точки. Лежит на краю кластера.
        \item \textbf{Шум (Noise Point):} Точка, которая не является ни основной, ни граничной. Выброс.
    \end{itemize}
    \textbf{Алгоритм (кратко):} Начинает с произвольной точки. Если она основная, "выращивает" кластер, рекурсивно добавляя всех достижимых по плотности соседей (основных и граничных). Если точка не основная, переходит к следующей. Точки, не попавшие ни в один кластер, помечаются как шум.
\end{textbox}

\begin{alerttextbox}{Плюсы и Минусы DBSCAN}
    \begin{itemize}
        \item \textbf{Плюсы:} Не нужно задавать K, находит кластеры сложной формы, устойчив к выбросам (явно их определяет).
        \item \textbf{Минусы:} Чувствителен к выбору \texttt{eps} и \texttt{min\_samples} (требует подбора), плохо работает с кластерами сильно разной плотности, может быть медленным на очень больших данных (хотя есть оптимизации).
    \end{itemize}
\end{alerttextbox}

% --- Конец контента ---