% >>> Улучшенный Контент для шпаргалки Cheatsheet 7: Основы Unsupervised Learning (PCA & K-Means + DBSCAN) v2 - Разделенные блоки

% --- Введение ---
\begin{textbox}{Обучение без учителя (Unsupervised Learning): Введение}
    В отличие от обучения с учителем, здесь \textbf{нет правильных ответов} (меток) для данных. Цель — найти \textbf{скрытые структуры}, закономерности или группы в самих данных.
\end{textbox}

\begin{myexampleblock}{Пример и Задачи}
    \textbf{Пример:} Представь, что тебе дали кучу разных носков, и ты должен рассортировать их по парам, не зная изначально, какие носки составляют пару — ты ищешь схожесть сам.
    \vspace{0.5ex} % Небольшой отступ
    \textbf{Основные задачи:}
    \begin{itemize}[nosep, leftmargin=*]
        \item Понижение размерности
        \item Кластеризация
        \item Поиск аномалий
        \item Изучение ассоциативных правил
    \end{itemize}
    \textbf{Ключевые методы для старта:} PCA (понижение размерности) и K-Means/DBSCAN (кластеризация).
\end{myexampleblock}

% --- Раздел 1: PCA ---
\section{Метод Главных Компонент (PCA - Principal Component Analysis)}

\begin{myblock}{Идея: Понижение размерности}
    PCA — это метод понижения размерности, который находит новые, самые информативные "проекции" или "ракурсы" данных (главные компоненты).
    \vspace{0.5ex}
    \textbf{Цель:} Уменьшить количество признаков (столбцов), сохранив при этом максимальное количество \textbf{вариативности (дисперсии)} исходных данных.
\end{myblock}

\begin{textbox}{Почему Дисперсия = Информация?}
    В контексте PCA предполагается, что направление, вдоль которого данные сильнее всего "разбросаны" (имеют большую дисперсию), несет больше всего информации об их различиях. Если вдоль какого-то направления все точки почти одинаковы (малая дисперсия), это направление мало что добавляет к пониманию структуры данных.
\end{textbox}

\begin{textbox}{Механика PCA: Шаги}
    PCA находит новые оси (главные компоненты, PC) в пространстве исходных признаков. Алгоритм включает следующие шаги:
    \begin{enumerate}[nosep, itemsep=0.5ex] % Уменьшил отступы в списке
        \item \textbf{Стандартизация данных:} \textit{Критически важно!} Вычитаем среднее и делим на стандартное отклонение для \textbf{каждого} признака. PCA чувствителен к масштабу; без стандартизации признаки с большими значениями будут доминировать.
        \item \textbf{Расчет ковариационной матрицы:} Строится матрица, показывающая, как стандартизированные признаки изменяются \textit{совместно}.
        \item \textbf{Нахождение собственных векторов и значений ($\lambda$):} Для ковариационной матрицы вычисляются её собственные векторы (\textit{eigenvectors}) и значения (\textit{eigenvalues}).
            \begin{itemize}[nosep, leftmargin=1.5em] % Уменьшил отступ в подсписке
                \item \textbf{Собственный вектор:} Указывает \textbf{направление} главной компоненты.
                \item \textbf{Собственное значение $\lambda$:} Показывает, \textbf{сколько дисперсии} объясняется вдоль этого направления.
            \end{itemize}
        \item \textbf{Сортировка и выбор компонент:} Собственные векторы сортируются по убыванию их $\lambda$. PC1 соответствует наибольшему $\lambda$, PC2 — второму по величине $\lambda$ (и ортогональна PC1), и т.д. Компоненты ортогональны друг другу.
        \item \textbf{Проецирование:} Исходные (стандартизированные) данные проецируются на выбранные $k$ главных компонент (с наибольшими $\lambda$). Результат — новый набор данных с $k$ признаками.
    \end{enumerate}
\end{textbox}

\begin{myexampleblock}{Аналогия "Эллипс и Тени"}
    Представь облако точек данных как эллипс. Главные компоненты — это оси этого эллипса (самая длинная — PC1, следующая перпендикулярная ей — PC2 и т.д.). PCA находит эти оси, поворачивает данные и отбрасывает оси с наименьшей длиной (дисперсией), оставляя проекцию на самые важные.
\end{myexampleblock}

\begin{alerttextbox}{Применения PCA и Чувствительность к Масштабу}
    \textbf{Применения:}
    \begin{itemize}[nosep, leftmargin=*]
        \item Визуализация данных (понижение до 2D/3D).
        \item Уменьшение шума.
        \item Борьба с мультиколлинеарностью.
        \item Ускорение обучения других моделей.
        \item Сжатие данных.
    \end{itemize}
    \textbf{Чувствительность к масштабу:} \textbf{Всегда стандартизируйте данные перед PCA!} Иначе признаки с большим масштабом "перетянут" всю дисперсию на себя.
\end{alerttextbox}

\begin{textbox}{Выбор количества компонент (k) в PCA}
    Как определить оптимальное число компонент $k$:
    \begin{itemize}[nosep, itemsep=0.5ex, leftmargin=*]
        \item \textbf{Доля объясненной дисперсии:} Смотрят на кумулятивную (накопленную) долю $\sum_{i=1}^{k} \lambda_i / \sum_{j=1}^{N} \lambda_j$. Выбирают $k$ так, чтобы объяснить достаточный процент (например, 90-99\%) общей дисперсии.
        \item \textbf{Метод Локтя (\textit{Scree Plot}):} Ищут "изгиб" на графике кумулятивной объясненной дисперсии или на графике самих собственных значений ($\lambda_i$ от $i$). Точка изгиба показывает, где добавление новой компоненты перестает давать существенный прирост информации.
        \item \textbf{Исходя из задачи:} Для визуализации $k=2$ или $k=3$. Для других задач можно подбирать $k$ по кросс-валидации для *конечной* ML-модели (например, классификатора, который будет использовать PCA-признаки).
    \end{itemize}
\end{textbox}

% --- Раздел 2: K-Means ---
\section{Кластеризация K-Means}

\begin{myblock}{Идея: Центроидная Кластеризация}
    Разделить все объекты на заранее заданное число ($K$) групп (кластеров) так, чтобы объекты внутри кластера были максимально похожи (близки) друг на друга, а объекты из разных кластеров — максимально различны.
    \vspace{0.5ex}
    \textbf{Цель:} Минимизировать суммарное квадратичное расстояние от точек до центров их кластеров (WCSS - Within-Cluster Sum of Squares).
\end{myblock}

\begin{myexampleblock}{Аналогия "Почтовые отделения"}
    Открыть $K$ почтовых отделений (центроидов) в городе так, чтобы суммарное \textit{квадратичное} расстояние от всех жителей (точек данных) до ближайшего к ним отделения было минимальным.
\end{myexampleblock}

\begin{textbox}{Алгоритм K-Means}
    Это итеративный алгоритм:
    \begin{enumerate}[nosep, itemsep=0.5ex]
        \item \textbf{Инициализация:} Выбрать $K$. Разместить $K$ \textbf{центроидов}. Рекомендуется "умная" инициализация \textbf{K-Means++} (размещает центроиды подальше друг от друга).
        \item \textbf{Шаг присваивания (Assignment):} Для каждой точки найти ближайший центроид (обычно по евклидову расстоянию) и присвоить точку его кластеру.
        \item \textbf{Шаг обновления (Update):} Для каждого кластера пересчитать положение его центроида как центр масс (среднее) всех точек, попавших в этот кластер.
        \item \textbf{Повторение:} Повторять шаги 2 и 3 до сходимости (центроиды почти не смещаются, или точки не меняют кластеры).
    \end{enumerate}
\end{textbox}

\begin{myblock}{Метрики расстояния в K-Means}
    Выбор метрики зависит от данных и задачи:
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Евклидово:} $\sqrt{\sum(x_i-y_i)^2}$. Стандартный выбор, хорошо для сферических кластеров.
        \item \textbf{Манхэттенское:} $\sum|x_i-y_i|$. "Расстояние городских кварталов".
        \item \textbf{Косинусное:} $1 - \cos(\theta) = 1 - \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|}$. Измеряет угол между векторами, полезно для текстов (TF-IDF).
    \end{itemize}
\end{myblock}

% --- Подраздел K-Means: Выбор K ---
\subsection{Выбор количества кластеров (K)}
\begin{myexampleblock}{Методы подбора K}
    Выбор оптимального K — нетривиальная задача. Популярные методы:
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Метод Локтя (Elbow Method):} Строится график WCSS от K. Ищется точка "изгиба" (локтя), после которой WCSS убывает значительно медленнее. (См. график ниже).
        \item \textbf{Метод Силуэта (Silhouette Method):} Вычисляют средний Силуэт для разных K. Выбирают K с максимальным средним Силуэтом. (См. следующий раздел).
        \item \textbf{Знание предметной области:} Иногда K можно определить из контекста задачи.
    \end{itemize}
    % График для метода локтя оставлен без изменений
    \begin{center}
    \begin{tikzpicture}
        \begin{axis}[
            cheatsheet plot style, title={Метод Локтя (Пример)}, xlabel={Число кластеров (K)}, ylabel={WCSS (Инерция)},
            xmin=1, xmax=10, ymin=0, xtick={1,2,3,4,5,6,7,8,9,10}, legend pos=north east, grid=major, ]
        \addplot+[smooth, mark=*, blue] coordinates {(1, 300) (2, 180) (3, 100) (4, 70) (5, 50) (6, 40) (7, 35) (8, 32) (9, 30) (10, 29)};
        \addlegendentry{WCSS}
        \draw [red, dashed, thick] (axis cs:3,0) -- (axis cs:3,100);
        \node[red, anchor=west] at (axis cs:3.2, 110) {Возможный "локоть"};
        \end{axis}
    \end{tikzpicture}
    \end{center}
\end{myexampleblock}

% --- Подраздел K-Means: Силуэт ---
\subsection{Оценка качества: Силуэт (Silhouette Score)}
\begin{textbox}{Интуиция Силуэта}
    Метрика "Силуэт" оценивает, насколько хорошо точка "сидит" в своем кластере по сравнению с соседними. Показывает качество разделения.
\end{textbox}

\begin{myblock}{Расчет Силуэта для точки $i$}
    \begin{itemize}[nosep, leftmargin=*]
        \item $a(i)$: Среднее расстояние от точки $i$ до \textbf{всех других точек в её собственном} кластере. \textit{(Внутрикластерная схожесть. Меньше = лучше).}
        \item $b(i)$: \textbf{Минимальное} из средних расстояний от точки $i$ до \textbf{всех точек в каждом из других} ("соседних") кластеров. \textit{(Межкластерное различие. Больше = лучше).}
    \end{itemize}
    \textbf{Формула Силуэта точки $i$}: \[ s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))} \]
\end{myblock}

\begin{myexampleblock}{Интерпретация и Аналогия Силуэта}
    \textbf{Значения $s(i)$:} От -1 до +1.
    \begin{itemize}[nosep, leftmargin=*]
        \item $s(i) \approx +1$: Точка плотно сидит в своем кластере, далеко от других (отлично).
        \item $s(i) \approx 0$: Точка на границе между кластерами.
        \item $s(i) \approx -1$: Точка, вероятно, попала не в тот кластер (плохо).
    \end{itemize}
    \textbf{Общий Силуэт:} Усредняют $s(i)$ по всем точкам. Чем ближе средний силуэт к 1, тем лучше разделение на кластеры для данного K.
    \vspace{0.5ex}
    \textbf{Аналогия "Районы города":} Высокий силуэт жителя ($s(i) \approx 1$) — живет в центре четко очерченного района. Низкий ($s(i) \approx 0$) — на границе. Отрицательный ($s(i) \approx -1$) — ближе к центру соседнего района.
\end{myexampleblock}

% --- Подраздел K-Means: Плюсы и Минусы ---
\subsection{Плюсы и Минусы K-Means}
\begin{textbox}{Преимущества K-Means}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Простота и скорость:} Легко понять, реализовать, относительно быстро работает на средних данных.
        \item \textbf{Масштабируемость:} Существуют вариации (MiniBatch K-Means) для больших данных.
        \item \textbf{Интерпретируемость:} Концепция центроидов как "представителей" кластера понятна.
    \end{itemize}
\end{textbox}

\begin{alerttextbox}{Ограничения K-Means}
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Чувствительность к инициализации:} Результат зависит от начального положения центроидов. \textbf{Решение:} Использовать K-Means++ и многократные запуски (\texttt{n\_init} в scikit-learn).
        \item \textbf{Необходимость задавать K:} Нужно знать K заранее или подбирать эвристиками.
        \item \textbf{Предположение о форме кластеров:} Ищет \textbf{выпуклые, сферические} кластеры примерно одинакового размера. Плохо работает с вытянутыми, вогнутыми кластерами или кластерами разной плотности.
        \item \textbf{Чувствительность к выбросам:} Выбросы могут сильно смещать центроиды.
    \end{itemize}
\end{alerttextbox}

% --- Раздел 3: DBSCAN ---
\section{Кластеризация DBSCAN (Density-Based)}

\begin{myblock}{Идея: Кластеризация на основе плотности}
    DBSCAN находит \textbf{плотные регионы} точек, разделенные областями с низкой плотностью. Позволяет находить кластеры \textbf{произвольной формы} и автоматически определяет \textbf{выбросы (шум)}. Не требует заранее задавать число кластеров K.
\end{myblock}

\begin{myexampleblock}{Аналогия "Поиск островов"}
     Представь карту с точками-домами. DBSCAN ищет "острова" (кластеры), где дома стоят близко друг к другу (\textit{плотные регионы}), отделенные "водой" (\textit{разреженные области}). Дома, стоящие совсем одиноко в "воде", считаются шумом.
\end{myexampleblock}

\begin{textbox}{Ключевые Параметры и Понятия DBSCAN}
    Требует два параметра:
    \begin{itemize}[nosep, leftmargin=*]
        \item \texttt{eps} ($ \epsilon $): Радиус окрестности. Максимальное расстояние между двумя точками, чтобы считать их соседями. % Обернули эпсилон в $ $
        \item \texttt{min\_samples}: Минимальное число соседей (включая саму точку) в $ \epsilon $-окрестности, чтобы точка считалась "основной". % Добавили пробел перед дефисом
    \end{itemize}
    \vspace{0.5ex}
    Основные типы точек:
    \begin{itemize}[nosep, leftmargin=*]
        \item \textbf{Основная точка (Core Point):} Точка, у которой в $ \epsilon $-окрестности $ \ge $ \texttt{min\_samples} точек (включая себя). % Обернули эпсилон и >= в $ $, добавили пробелы
        \item \textbf{Граничная точка (Border Point):} Не основная точка, но находится в $ \epsilon $-окрестности некоторой \textit{основной} точки. % Добавили пробел перед дефисом
        \item \textbf{Шум (Noise Point):} Точка, не являющаяся ни основной, ни граничной. Выброс.
    \end{itemize}
\end{textbox}

\begin{myblock}{Принцип Работы Алгоритма DBSCAN}
    \begin{enumerate}[nosep, itemsep=0.5ex]
        \item Выбирается произвольная непосещенная точка.
        \item Если точка \textit{основная}, начинается формирование нового кластера. Все точки, достижимые по плотности от неё (т.е. её $\epsilon$-соседи, и их $\epsilon$-соседи, если они основные, и так далее), добавляются в этот кластер. Граничные точки тоже добавляются, но от них кластер не "растет".
        \item Если точка \textit{не основная} (граничная или шум), она помечается как посещенная (возможно, как шум; если позже окажется в окрестности основной точки, станет граничной и войдет в кластер).
        \item Шаги 1-3 повторяются, пока все точки не будут посещены.
    \end{enumerate}
    Точки, оставшиеся помеченными как шум, не принадлежат ни одному кластеру.
\end{myblock}

\begin{alerttextbox}{Плюсы и Минусы DBSCAN}
    \textbf{Плюсы:}
    \begin{itemize}[nosep, leftmargin=*]
        \item Не нужно заранее задавать количество кластеров K.
        \item Способен находить кластеры сложной, невыпуклой формы.
        \item Устойчив к выбросам и явно их идентифицирует как шум.
    \end{itemize}
    \vspace{0.5ex} % Небольшой разделитель
    \textbf{Минусы:}
    \begin{itemize}[nosep, leftmargin=*]
        \item Результат чувствителен к выбору параметров \texttt{eps} и \texttt{min\_samples}. Их подбор может быть нетривиальным (часто используют k-distance graph для \texttt{eps}).
        \item Плохо справляется с кластерами, имеющими сильно различающуюся плотность, так как параметры \texttt{eps} и \texttt{min\_samples} глобальны.
        \item Может быть вычислительно затратным на очень больших датасетах (сложность около $O(N \log N)$ или $O(N^2)$ без пространственных индексов).
    \end{itemize}
\end{alerttextbox}

% --- Конец контента ---