% >>> Контент для шпаргалки Cheatsheet 7: Основы Unsupervised Learning (PCA & K-Means)

% --- Введение ---
\begin{textbox}{Обучение без учителя (Unsupervised Learning): Введение}
    В отличие от обучения с учителем, здесь у нас \textbf{нет правильных ответов} (меток) для данных. Цель — найти \textbf{скрытые структуры}, закономерности или группы в самих данных. Представь, что тебе дали кучу разных носков, и ты должен рассортировать их по парам, не зная изначально, какие носки составляют пару — ты ищешь схожесть сам.
    \begin{itemize}
        \item \textbf{Основные задачи:} Понижение размерности, кластеризация, поиск аномалий, изучение ассоциативных правил.
        \item \textbf{Ключевые методы для старта:} PCA (понижение размерности) и K-Means (кластеризация).
    \end{itemize}
\end{textbox}

% --- Раздел 1: PCA ---
\section{Метод Главных Компонент (PCA - Principal Component Analysis)}

\begin{myblock}{Идея: Понижение размерности}
    Представь, что у тебя есть сложный объект (многомерные данные), и ты хочешь сделать его "плоскую" фотографию (понизить размерность), но так, чтобы на фото сохранилось как можно больше информации о форме объекта. PCA именно это и делает — находит наиболее информативные "проекции" данных.
    \vspace{0.5ex} % Небольшой отступ
    \textbf{Цель:} Уменьшить количество признаков (столбцов) в данных, сохранив при этом максимальное количество **дисперсии** (информации, вариативности).
\end{myblock}

% Используем itemize вместо enumerate для стабильности
\begin{textbox}{Как работает PCA (Интуиция)}
    PCA ищет новые оси (называемые \textbf{главными компонентами}) в пространстве признаков. Эти оси обладают следующими свойствами:
    \begin{itemize}
        \item \textbf{Максимизация дисперсии:} Первая главная компонента — это направление, вдоль которого данные имеют наибольший разброс (дисперсию). Вторая — следующее направление с максимальной дисперсией, но \textbf{ортогональное} (перпендикулярное) первой, и так далее.
        \item \textbf{Ортогональность:} Главные компоненты не коррелируют друг с другом.
        \item \textbf{Математика (основа):} Эти направления \textit{математически} соответствуют \textbf{собственным векторам} (\textit{eigenvectors}) ковариационной матрицы данных. "Важность" каждого направления (объем объясняемой дисперсии) определяется соответствующим \textbf{собственным значением} (\textit{eigenvalue}). Чем больше собственное значение, тем больше дисперсии объясняет компонента.
    \end{itemize}
    \textbf{Аналогия "Тени":} Представь, что твои данные — это облако точек в 3D. PCA находит такую "стену" (плоскость), что тень (проекция) от облака на эту стену будет максимально "размазанной", то есть сохранит максимум информации об исходной форме облака. Эта стена и задается главными компонентами.
\end{textbox}

\begin{alerttextbox}{Когда применять PCA и Как выбрать компоненты?}
    \begin{itemize}
        \item \textbf{Визуализация:} Уменьшение размерности до 2D или 3D для построения графиков.
        \item \textbf{Уменьшение шума:} Отбрасывание компонент с малой дисперсией может убрать шум.
        \item \textbf{Борьба с мультиколлинеарностью:} Главные компоненты не коррелируют, что полезно для некоторых моделей (например, линейной регрессии).
        \item \textbf{Ускорение обучения:} Меньше признаков -> быстрее работают другие алгоритмы ML.
        \item \textbf{Важно:} PCA чувствителен к \textbf{масштабу данных}. Перед применением PCA признаки обычно нужно \textbf{стандартизировать} (например, с помощью StandardScaler).
        \item \textbf{Выбор количества компонент:} Часто смотрят на \textbf{долю объясненной дисперсии} (explained variance ratio) для каждой компоненты. Выбирают такое количество компонент, которое объясняет достаточный процент общей дисперсии (например, 90-99\%) или ищут "локоть" на графике кумулятивной объясненной дисперсии (когда добавление новой компоненты уже не дает значительного прироста объясненной дисперсии).
    \end{itemize}
\end{alerttextbox}

% --- Раздел 2: K-Means ---
\section{Кластеризация K-Means}

\begin{myblock}{Идея: Кластеризация}
    Разделить все объекты (точки данных) на заранее заданное число ($K$) групп (кластеров) так, чтобы объекты внутри одного кластера были максимально похожи друг на друга, а объекты из разных кластеров — максимально различны.
    \vspace{0.5ex}
    \textbf{Аналогия "Почтовые отделения":} Представь, что тебе нужно открыть $K$ почтовых отделений в городе так, чтобы суммарное расстояние от всех жителей до ближайшего к ним отделения было минимальным. K-Means решает похожую задачу: центры кластеров (\textbf{центроиды}) — это как почтовые отделения, а точки данных — жители.
\end{myblock}

\begin{textbox}{Как работает K-Means (Алгоритм)}
    Это итеративный алгоритм:
    \begin{enumerate}
        \item \textbf{Инициализация:} Выбрать $K$ (количество кластеров). Случайным образом разместить $K$ \textbf{центроидов} (начальных центров кластеров).
        \item \textbf{Шаг присваивания (Assignment Step):} Каждую точку данных отнести к кластеру, чей центроид к ней \textbf{ближе всего} (обычно по евклидову расстоянию).
        \item \textbf{Шаг обновления (Update Step):} Пересчитать положение каждого центроида как \textbf{среднее арифметическое} всех точек, отнесенных к этому кластеру на предыдущем шаге.
        \item \textbf{Повторение:} Повторять шаги 2 и 3 до тех пор, пока центроиды не перестанут значительно смещаться или не будет достигнуто максимальное число итераций.
    \end{enumerate}
    \textbf{Важно:} Результат K-Means зависит от начального положения центроидов. Обычно алгоритм запускают несколько раз с разными инициализациями и выбирают лучший результат (например, с наименьшим WCSS - см. ниже). Также K-Means предполагает, что кластеры имеют примерно сферическую форму и одинаковый размер.
\end{textbox}

\subsection{Выбор количества кластеров (K)}
\begin{myexampleblock}{Как подобрать K?}
    Выбор оптимального K — нетривиальная задача, так как нет единственно "правильного" ответа. Популярные эвристические методы:
    \begin{itemize}
        \item \textbf{Метод Локтя (Elbow Method):} Строится график зависимости суммы квадратов расстояний от точек до центроидов их кластеров (\textbf{WCSS - Within-Cluster Sum of Squares} или инерция) от числа кластеров K. Ищется точка "изгиба" (локтя) на графике, после которой добавление нового кластера уже не дает существенного уменьшения WCSS. \textit{Аналогия: После определенного момента добавление нового почтового отделения уже не сильно сокращает среднее расстояние для жителей.}
        \item \textbf{Метод Силуэта (Silhouette Method):} Запускают K-Means для разных K и выбирают то K, для которого \textbf{средний Силуэт} по всем точкам максимален (ближе к 1). Этот метод часто дает более осмысленные результаты, чем метод локтя.
        \item \textbf{Знание предметной области:} Иногда оптимальное количество кластеров можно определить исходя из понимания данных и бизнес-задачи (например, мы знаем, что у нас 3 типа клиентов).
    \end{itemize}
\end{myexampleblock}

% График для метода локтя (раскомментирован)
\begin{center}
\begin{tikzpicture}
    \begin{axis}[
        cheatsheet plot style, % Используем стиль из шаблона
        title={Метод Локтя (Пример)},
        xlabel={Число кластеров (K)},
        ylabel={WCSS (Инерция)},
        xmin=1, xmax=10,
        ymin=0,
        xtick={1,2,3,4,5,6,7,8,9,10},
        legend pos=north east,
        grid=major,
    ]
    \addplot+[smooth, mark=*, blue] coordinates {
        (1, 300) (2, 180) (3, 100) (4, 70) (5, 50) (6, 40) (7, 35) (8, 32) (9, 30) (10, 29)
    };
    \addlegendentry{WCSS}
    % Опционально: указать "локоть"
    \draw [red, dashed, thick] (axis cs:3,0) -- (axis cs:3,100);
    \node[red, anchor=west] at (axis cs:3.2, 110) {Возможный "локоть"};
    \end{axis}
\end{tikzpicture}
\end{center}

\subsection{Оценка качества: Силуэт (Silhouette Score)}
\begin{myexampleblock}{Интуиция Силуэта}
    Как понять, насколько хорошо точки сгруппировались? Метрика "Силуэт" помогает это оценить для каждой точки данных. Она показывает, насколько точка "хорошо сидит" в своем кластере по сравнению с соседними кластерами.
    \vspace{0.5ex}
    Для каждой точки $i$:
    \begin{itemize}
        \item $a(i)$: Среднее расстояние от точки $i$ до \textbf{всех других точек в её собственном} кластере (насколько она "своя"). Чем меньше $a(i)$, тем лучше.
        \item $b(i)$: Минимальное \textbf{среднее} расстояние от точки $i$ до \textbf{всех точек в каком-либо другом} ("соседнем", ближайшем) кластере (насколько она далека от "чужих"). Чем больше $b(i)$, тем лучше.
    \end{itemize}
    \textbf{Силуэт точки $i$}: \[ s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))} \]
    \textbf{Интерпретация значений $s(i)$:}
    \begin{itemize}
        \item Близко к $+1$: Точка $i$ хорошо соответствует своему кластеру и далека от других (идеальный случай).
        \item Близко к $0$: Точка $i$ находится на границе между кластерами.
        \item Близко к $-1$: Точка $i$, скорее всего, попала не в тот кластер.
    \end{itemize}
    \textbf{Общий Силуэт:} Часто усредняют значения $s(i)$ по всем точкам, чтобы получить общую оценку качества кластеризации для выбранного K. Чем ближе средний силуэт к 1, тем лучше разделение на кластеры.
    \vspace{0.5ex}
    \textbf{Аналогия "Районы города":} Силуэт жителя показывает, насколько он близок к своим соседям по району ($a(i)$) и насколько он далек от жителей ближайшего другого района ($b(i)$). Высокий силуэт — четко очерченные районы, низкий — районы смешаны или житель живет "на границе".
\end{myexampleblock}

\subsection{Плюсы K-Means}
\begin{textbox}{Преимущества}
    \begin{itemize}
        \item \textbf{Простота и скорость:} Легко понять, реализовать и он относительно быстро работает даже на больших наборах данных. Вычислительная сложность линейно зависит от числа точек.
        \item \textbf{Интерпретируемость:} Концепция центроидов как "представителей" кластера интуитивно понятна.
    \end{itemize}
\end{textbox}

\subsection{Минусы K-Means}
\begin{alerttextbox}{Ограничения}
    \begin{itemize}
        \item \textbf{Чувствительность к инициализации:} Разные начальные положения центроидов могут привести к разным результатам кластеризации. Решение: многократные запуски (параметр \texttt{n\_init} в scikit-learn) или "умная" инициализация (K-Means++). % ИСПРАВЛЕНО ЗДЕСЬ
        \item \textbf{Необходимость задавать K:} Нужно заранее знать количество кластеров или использовать эвристики (метод локтя, силуэт) для его подбора, что не всегда просто.
        \item \textbf{Предположение о форме кластеров:} K-Means хорошо работает, когда кластеры имеют примерно сферическую (выпуклую) форму, одинаковый размер и плотность. Он плохо справляется с вытянутыми кластерами, кластерами сложной формы или разного размера. (Алгоритмы вроде DBSCAN или спектральной кластеризации лучше подходят для таких случаев).
        \item \textbf{Чувствительность к выбросам:} Аномальные точки могут сильно смещать положение центроидов, искажая результат кластеризации.
    \end{itemize}
\end{alerttextbox}

% --- Конец контента ---